[2022-06-21 00:28:23,418] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 00:28:23,425] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 00:28:23,426] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 00:28:23,426] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 00:28:23,426] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 00:28:23,619] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 00:28:23,620] {standard_task_runner.py:54} INFO - Started process 110685 to run task
[2022-06-21 00:28:23,658] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '100', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmplqymx8q8']
[2022-06-21 00:28:23,658] {standard_task_runner.py:78} INFO - Job 100: Subtask transform_football
[2022-06-21 00:28:23,872] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 00:28:23,883] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: yarn, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 00:28:23,883] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: spark-submit --master yarn --name football_transformation --queue root.default /home/lucas/pipeline-data/spark/transformation.py
[2022-06-21 00:28:23,887] {taskinstance.py:1150} ERROR - [Errno 2] No such file or directory: 'spark-submit'
Traceback (most recent call last):
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 390, in submit
    self._submit_sp = subprocess.Popen(spark_submit_cmd,
  File "/usr/local/lib/python3.9/subprocess.py", line 947, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/usr/local/lib/python3.9/subprocess.py", line 1819, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'spark-submit'
[2022-06-21 00:28:23,915] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T032823, end_date=20220621T032823
[2022-06-21 00:28:28,262] {local_task_job.py:102} INFO - Task exited with return code 1
[2022-06-21 00:35:35,444] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 00:35:35,451] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 00:35:35,451] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 00:35:35,451] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 00:35:35,451] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 00:35:35,642] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 00:35:35,643] {standard_task_runner.py:54} INFO - Started process 112820 to run task
[2022-06-21 00:35:35,681] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '100', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpn_e9vbt2']
[2022-06-21 00:35:35,681] {standard_task_runner.py:78} INFO - Job 100: Subtask transform_football
[2022-06-21 00:35:35,846] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 00:35:35,856] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 00:35:35,857] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py
[2022-06-21 00:35:36,566] {spark_submit_hook.py:479} INFO - 22/06/21 00:35:36 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 00:35:36,566] {spark_submit_hook.py:479} INFO - 22/06/21 00:35:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 00:35:37,328] {spark_submit_hook.py:479} INFO - Traceback (most recent call last):
[2022-06-21 00:35:37,328] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 9, in <module>
[2022-06-21 00:35:37,328] {spark_submit_hook.py:479} INFO - from helpers import Helpers
[2022-06-21 00:35:37,328] {spark_submit_hook.py:479} INFO - ModuleNotFoundError: No module named 'helpers'
[2022-06-21 00:35:37,349] {spark_submit_hook.py:479} INFO - log4j:WARN No appenders could be found for logger (org.apache.spark.util.ShutdownHookManager).
[2022-06-21 00:35:37,349] {spark_submit_hook.py:479} INFO - log4j:WARN Please initialize the log4j system properly.
[2022-06-21 00:35:37,349] {spark_submit_hook.py:479} INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
[2022-06-21 00:35:37,374] {taskinstance.py:1150} ERROR - Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py. Error code is: 1.
Traceback (most recent call last):
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 403, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py. Error code is: 1.
[2022-06-21 00:35:37,375] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T033535, end_date=20220621T033537
[2022-06-21 00:35:40,282] {local_task_job.py:102} INFO - Task exited with return code 1
[2022-06-21 09:48:03,198] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 09:48:03,206] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 09:48:03,206] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 09:48:03,206] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 09:48:03,206] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 09:48:03,478] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 09:48:03,480] {standard_task_runner.py:54} INFO - Started process 9885 to run task
[2022-06-21 09:48:03,517] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '112', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpevjbxcol']
[2022-06-21 09:48:03,518] {standard_task_runner.py:78} INFO - Job 112: Subtask transform_football
[2022-06-21 09:48:03,830] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 09:48:03,840] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 09:48:03,841] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py
[2022-06-21 09:48:04,597] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:04 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 09:48:04,597] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 09:48:05,391] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 09:48:05,396] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 09:48:05,465] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 09:48:05,530] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO ResourceUtils: ==============================================================
[2022-06-21 09:48:05,531] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 09:48:05,531] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO ResourceUtils: ==============================================================
[2022-06-21 09:48:05,531] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 09:48:05,546] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 09:48:05,555] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 09:48:05,556] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 09:48:05,589] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 09:48:05,589] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 09:48:05,590] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 09:48:05,590] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 09:48:05,590] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 09:48:05,750] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO Utils: Successfully started service 'sparkDriver' on port 44051.
[2022-06-21 09:48:05,766] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 09:48:05,787] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 09:48:05,800] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 09:48:05,801] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 09:48:05,803] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 09:48:05,816] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bb53d0b9-50ef-418e-b944-d78e9d6713b9
[2022-06-21 09:48:05,832] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 09:48:05,843] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 09:48:05,989] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 09:48:06,024] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 09:48:06,139] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 09:48:06,155] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42275.
[2022-06-21 09:48:06,155] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO NettyBlockTransferService: Server created on 192.168.0.13:42275
[2022-06-21 09:48:06,156] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 09:48:06,160] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 42275, None)
[2022-06-21 09:48:06,163] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:42275 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 42275, None)
[2022-06-21 09:48:06,165] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 42275, None)
[2022-06-21 09:48:06,165] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 42275, None)
[2022-06-21 09:48:06,375] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655815686374
[2022-06-21 09:48:06,375] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-e935eff4-6bcc-4f4f-a7ca-b386809b980b/userFiles-bca965ad-8d1d-4db9-b774-059e6f062efe/helpers.py
[2022-06-21 09:48:06,467] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 09:48:06,469] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 09:48:06,935] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:06 INFO InMemoryFileIndex: It took 20 ms to list leaf files for 1 paths.
[2022-06-21 09:48:07,059] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:07 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 09:48:08,333] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 09:48:08,334] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 09:48:08,337] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 09:48:08,520] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 09:48:08,554] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 09:48:08,555] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:42275 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 09:48:08,558] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:48:08,563] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 09:48:08,679] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:48:08,689] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 09:48:08,689] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 09:48:08,690] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 09:48:08,690] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO DAGScheduler: Missing parents: List()
[2022-06-21 09:48:08,693] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 09:48:08,749] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 09:48:08,750] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 09:48:08,751] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:42275 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 09:48:08,752] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 09:48:08,760] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 09:48:08,760] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 09:48:08,791] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-21 09:48:08,800] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 09:48:08,801] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655815686374
[2022-06-21 09:48:08,813] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:08 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-e935eff4-6bcc-4f4f-a7ca-b386809b980b/userFiles-bca965ad-8d1d-4db9-b774-059e6f062efe/helpers.py
[2022-06-21 09:48:09,017] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 09:48:09,159] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO CodeGenerator: Code generated in 118.056579 ms
[2022-06-21 09:48:09,193] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 09:48:09,201] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 416 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 09:48:09,202] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 09:48:09,206] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,503 s
[2022-06-21 09:48:09,208] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 09:48:09,208] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 09:48:09,209] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,530267 s
[2022-06-21 09:48:09,228] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 09:48:09,232] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 09:48:09,273] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 09:48:09,273] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 09:48:09,273] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 09:48:09,278] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 09:48:09,285] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 09:48:09,286] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:42275 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,287] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:48:09,287] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197676 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 09:48:09,294] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:48:09,295] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 09:48:09,295] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 09:48:09,295] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 09:48:09,295] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Missing parents: List()
[2022-06-21 09:48:09,296] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 09:48:09,299] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 09:48:09,300] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 09:48:09,301] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:42275 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,301] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 09:48:09,302] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 09:48:09,302] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 09:48:09,303] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-21 09:48:09,303] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 09:48:09,309] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-10/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 09:48:09,321] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4079 bytes result sent to driver
[2022-06-21 09:48:09,323] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 21 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 09:48:09,323] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 09:48:09,324] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,027 s
[2022-06-21 09:48:09,324] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 09:48:09,324] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 09:48:09,324] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,029817 s
[2022-06-21 09:48:09,424] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 09:48:09,428] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 09:48:09,446] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 09:48:09,446] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 09:48:09,446] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 09:48:09,450] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 09:48:09,456] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 09:48:09,457] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:42275 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,458] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:48:09,458] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215261 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 09:48:09,464] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:48:09,465] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 09:48:09,465] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 09:48:09,465] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 09:48:09,465] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Missing parents: List()
[2022-06-21 09:48:09,466] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 09:48:09,468] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 09:48:09,470] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 09:48:09,470] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:42275 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,470] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 09:48:09,471] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 09:48:09,471] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 09:48:09,472] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-21 09:48:09,472] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 09:48:09,477] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-10/WhoScored_20220410.json, range: 0-20957, partition values: [empty row]
[2022-06-21 09:48:09,488] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2172 bytes result sent to driver
[2022-06-21 09:48:09,489] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 17 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 09:48:09,489] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 09:48:09,490] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,024 s
[2022-06-21 09:48:09,490] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 09:48:09,490] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 09:48:09,490] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,026217 s
[2022-06-21 09:48:09,564] {spark_submit_hook.py:479} INFO - root
[2022-06-21 09:48:09,564] {spark_submit_hook.py:479} INFO - |-- home_team: string (nullable = true)
[2022-06-21 09:48:09,564] {spark_submit_hook.py:479} INFO - |-- away_team: string (nullable = true)
[2022-06-21 09:48:09,564] {spark_submit_hook.py:479} INFO - |-- odd_home: string (nullable = true)
[2022-06-21 09:48:09,564] {spark_submit_hook.py:479} INFO - |-- odd_draw: string (nullable = true)
[2022-06-21 09:48:09,564] {spark_submit_hook.py:479} INFO - |-- odd_away: string (nullable = true)
[2022-06-21 09:48:09,564] {spark_submit_hook.py:479} INFO - 
[2022-06-21 09:48:09,620] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DataSourceStrategy: Pruning directories with:
[2022-06-21 09:48:09,621] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 09:48:09,621] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 09:48:09,621] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 09:48:09,686] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO CodeGenerator: Code generated in 15.566831 ms
[2022-06-21 09:48:09,702] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO CodeGenerator: Code generated in 11.332559 ms
[2022-06-21 09:48:09,705] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 337.9 KiB, free 364.8 MiB)
[2022-06-21 09:48:09,715] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 364.8 MiB)
[2022-06-21 09:48:09,715] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:42275 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,716] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0
[2022-06-21 09:48:09,718] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 09:48:09,747] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:42275 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,750] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:42275 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,752] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:42275 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,754] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:42275 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,768] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2022-06-21 09:48:09,769] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 09:48:09,769] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
[2022-06-21 09:48:09,769] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 09:48:09,769] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Missing parents: List()
[2022-06-21 09:48:09,769] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 09:48:09,774] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 27.7 KiB, free 365.5 MiB)
[2022-06-21 09:48:09,775] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 365.5 MiB)
[2022-06-21 09:48:09,775] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:42275 (size: 12.2 KiB, free: 366.2 MiB)
[2022-06-21 09:48:09,776] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
[2022-06-21 09:48:09,776] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 09:48:09,776] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-21 09:48:09,777] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()
[2022-06-21 09:48:09,778] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-21 09:48:09,816] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:09 INFO CodeGenerator: Code generated in 7.159755 ms
[2022-06-21 09:48:10,208] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [19092]
[2022-06-21 09:48:10,222] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO CodeGenerator: Code generated in 10.777771 ms
[2022-06-21 09:48:10,225] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO CodeGenerator: Code generated in 13.162874 ms
[2022-06-21 09:48:10,245] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO PythonUDFRunner: Times: total = 416, boot = 373, init = 43, finish = 0
[2022-06-21 09:48:10,247] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2485 bytes result sent to driver
[2022-06-21 09:48:10,248] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 471 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 09:48:10,248] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-21 09:48:10,249] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39099
[2022-06-21 09:48:10,250] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0,480 s
[2022-06-21 09:48:10,250] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 09:48:10,250] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-21 09:48:10,251] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0,482684 s
[2022-06-21 09:48:10,271] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO CodeGenerator: Code generated in 11.125141 ms
[2022-06-21 09:48:10,275] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - |  home_team|  away_team|odd_home|odd_draw|odd_away|
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - |Atletico-GO|Flamengo RJ|    4.06|    3.40|    1.97|
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - | Fluminense|     Santos|    1.77|    3.44|    5.29|
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - 
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - Traceback (most recent call last):
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 54, in <module>
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - export_json(odds_portal_df, '/home/lucas/pipeline-data/datalake/silver')
[2022-06-21 09:48:10,276] {spark_submit_hook.py:479} INFO - NameError: name 'export_json' is not defined
[2022-06-21 09:48:10,302] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 09:48:10,308] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 09:48:10,314] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 09:48:10,321] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO MemoryStore: MemoryStore cleared
[2022-06-21 09:48:10,321] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO BlockManager: BlockManager stopped
[2022-06-21 09:48:10,324] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 09:48:10,326] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 09:48:10,328] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 09:48:10,328] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 09:48:10,328] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-31690c4c-de81-4835-a0fe-f41849a9162d
[2022-06-21 09:48:10,330] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-e935eff4-6bcc-4f4f-a7ca-b386809b980b
[2022-06-21 09:48:10,331] {spark_submit_hook.py:479} INFO - 22/06/21 09:48:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-e935eff4-6bcc-4f4f-a7ca-b386809b980b/pyspark-7f11ee31-184e-442c-b909-d9c18d880933
[2022-06-21 09:48:10,371] {taskinstance.py:1150} ERROR - Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py. Error code is: 1.
Traceback (most recent call last):
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 403, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py. Error code is: 1.
[2022-06-21 09:48:10,372] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T124803, end_date=20220621T124810
[2022-06-21 09:48:12,972] {local_task_job.py:102} INFO - Task exited with return code 1
[2022-06-21 09:57:03,615] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 09:57:03,622] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 09:57:03,622] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 09:57:03,622] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 09:57:03,622] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 09:57:03,793] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 09:57:03,794] {standard_task_runner.py:54} INFO - Started process 12948 to run task
[2022-06-21 09:57:03,831] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '120', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmprphib1zx']
[2022-06-21 09:57:03,831] {standard_task_runner.py:78} INFO - Job 120: Subtask transform_football
[2022-06-21 09:57:03,990] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 09:57:04,001] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 09:57:04,001] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py
[2022-06-21 09:57:04,680] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:04 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 09:57:04,680] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 09:57:05,468] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 09:57:05,473] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 09:57:05,534] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 09:57:05,585] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO ResourceUtils: ==============================================================
[2022-06-21 09:57:05,585] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 09:57:05,585] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO ResourceUtils: ==============================================================
[2022-06-21 09:57:05,585] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 09:57:05,600] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 09:57:05,609] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 09:57:05,609] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 09:57:05,642] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 09:57:05,643] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 09:57:05,643] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 09:57:05,643] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 09:57:05,643] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 09:57:05,794] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO Utils: Successfully started service 'sparkDriver' on port 37929.
[2022-06-21 09:57:05,810] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 09:57:05,830] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 09:57:05,842] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 09:57:05,842] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 09:57:05,844] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 09:57:05,856] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3eba2132-0c15-4337-93dd-b55e91521dd1
[2022-06-21 09:57:05,872] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 09:57:05,882] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 09:57:06,018] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 09:57:06,052] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 09:57:06,161] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 09:57:06,176] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38689.
[2022-06-21 09:57:06,176] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO NettyBlockTransferService: Server created on 192.168.0.13:38689
[2022-06-21 09:57:06,177] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 09:57:06,181] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 38689, None)
[2022-06-21 09:57:06,183] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:38689 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 38689, None)
[2022-06-21 09:57:06,185] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 38689, None)
[2022-06-21 09:57:06,186] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 38689, None)
[2022-06-21 09:57:06,376] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655816226376
[2022-06-21 09:57:06,377] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-99942006-b86b-42f0-92c8-fc0dcb676820/userFiles-542ad94c-0d50-4e06-a454-c9179fa78ed9/helpers.py
[2022-06-21 09:57:06,463] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 09:57:06,464] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 09:57:06,940] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:06 INFO InMemoryFileIndex: It took 22 ms to list leaf files for 1 paths.
[2022-06-21 09:57:07,076] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:07 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 09:57:08,355] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 09:57:08,356] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 09:57:08,358] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 09:57:08,531] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 09:57:08,563] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 09:57:08,565] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:38689 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 09:57:08,568] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:57:08,573] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 09:57:08,678] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:57:08,688] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 09:57:08,688] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 09:57:08,688] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 09:57:08,689] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO DAGScheduler: Missing parents: List()
[2022-06-21 09:57:08,691] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 09:57:08,742] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 09:57:08,744] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 09:57:08,744] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:38689 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 09:57:08,745] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 09:57:08,753] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 09:57:08,754] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 09:57:08,784] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-21 09:57:08,793] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 09:57:08,794] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655816226376
[2022-06-21 09:57:08,805] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:08 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-99942006-b86b-42f0-92c8-fc0dcb676820/userFiles-542ad94c-0d50-4e06-a454-c9179fa78ed9/helpers.py
[2022-06-21 09:57:09,011] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 09:57:09,146] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO CodeGenerator: Code generated in 112.296077 ms
[2022-06-21 09:57:09,179] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 09:57:09,184] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 09:57:09,185] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 09:57:09,189] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,489 s
[2022-06-21 09:57:09,191] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 09:57:09,191] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 09:57:09,192] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,513701 s
[2022-06-21 09:57:09,210] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 09:57:09,214] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 09:57:09,256] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 09:57:09,257] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 09:57:09,257] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 09:57:09,262] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 09:57:09,269] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 09:57:09,270] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:38689 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,271] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:57:09,271] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197676 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 09:57:09,279] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:57:09,280] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 09:57:09,280] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 09:57:09,280] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 09:57:09,280] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Missing parents: List()
[2022-06-21 09:57:09,280] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 09:57:09,283] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 09:57:09,285] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 09:57:09,285] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:38689 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,286] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 09:57:09,286] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 09:57:09,286] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 09:57:09,288] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-21 09:57:09,288] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 09:57:09,294] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-10/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 09:57:09,305] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4079 bytes result sent to driver
[2022-06-21 09:57:09,306] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 19 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 09:57:09,307] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 09:57:09,307] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,026 s
[2022-06-21 09:57:09,307] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 09:57:09,307] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 09:57:09,308] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,028645 s
[2022-06-21 09:57:09,412] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 09:57:09,416] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 09:57:09,436] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 09:57:09,436] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 09:57:09,437] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 09:57:09,441] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 09:57:09,447] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 09:57:09,448] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:38689 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,448] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:57:09,449] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215261 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 09:57:09,456] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 09:57:09,457] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 09:57:09,457] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 09:57:09,457] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 09:57:09,457] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Missing parents: List()
[2022-06-21 09:57:09,458] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 09:57:09,460] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 09:57:09,461] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 09:57:09,462] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:38689 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,462] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 09:57:09,463] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 09:57:09,463] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 09:57:09,464] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-21 09:57:09,464] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 09:57:09,469] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-10/WhoScored_20220410.json, range: 0-20957, partition values: [empty row]
[2022-06-21 09:57:09,478] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2172 bytes result sent to driver
[2022-06-21 09:57:09,479] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 15 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 09:57:09,479] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 09:57:09,480] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,022 s
[2022-06-21 09:57:09,480] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 09:57:09,480] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 09:57:09,480] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,024196 s
[2022-06-21 09:57:09,558] {spark_submit_hook.py:479} INFO - root
[2022-06-21 09:57:09,558] {spark_submit_hook.py:479} INFO - |-- home_team: string (nullable = true)
[2022-06-21 09:57:09,558] {spark_submit_hook.py:479} INFO - |-- away_team: string (nullable = true)
[2022-06-21 09:57:09,558] {spark_submit_hook.py:479} INFO - |-- odd_home: string (nullable = true)
[2022-06-21 09:57:09,558] {spark_submit_hook.py:479} INFO - |-- odd_draw: string (nullable = true)
[2022-06-21 09:57:09,558] {spark_submit_hook.py:479} INFO - |-- odd_away: string (nullable = true)
[2022-06-21 09:57:09,558] {spark_submit_hook.py:479} INFO - 
[2022-06-21 09:57:09,620] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:38689 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,623] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:38689 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,625] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:38689 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,625] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DataSourceStrategy: Pruning directories with:
[2022-06-21 09:57:09,626] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 09:57:09,626] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 09:57:09,626] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 09:57:09,626] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:38689 in memory (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 09:57:09,696] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO CodeGenerator: Code generated in 16.812585 ms
[2022-06-21 09:57:09,711] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO CodeGenerator: Code generated in 10.28774 ms
[2022-06-21 09:57:09,716] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 337.9 KiB, free 365.6 MiB)
[2022-06-21 09:57:09,725] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 09:57:09,725] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:38689 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,726] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0
[2022-06-21 09:57:09,728] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 09:57:09,769] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2022-06-21 09:57:09,770] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 09:57:09,770] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
[2022-06-21 09:57:09,770] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 09:57:09,770] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Missing parents: List()
[2022-06-21 09:57:09,770] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 09:57:09,775] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 27.7 KiB, free 365.5 MiB)
[2022-06-21 09:57:09,777] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 365.5 MiB)
[2022-06-21 09:57:09,777] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:38689 (size: 12.2 KiB, free: 366.2 MiB)
[2022-06-21 09:57:09,778] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
[2022-06-21 09:57:09,778] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 09:57:09,778] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-21 09:57:09,779] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()
[2022-06-21 09:57:09,779] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-21 09:57:09,814] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:09 INFO CodeGenerator: Code generated in 7.364342 ms
[2022-06-21 09:57:10,201] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [19092]
[2022-06-21 09:57:10,216] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO CodeGenerator: Code generated in 10.636878 ms
[2022-06-21 09:57:10,219] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO CodeGenerator: Code generated in 13.210627 ms
[2022-06-21 09:57:10,236] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO PythonUDFRunner: Times: total = 411, boot = 370, init = 40, finish = 1
[2022-06-21 09:57:10,238] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2485 bytes result sent to driver
[2022-06-21 09:57:10,239] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 460 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 09:57:10,239] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-21 09:57:10,240] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33421
[2022-06-21 09:57:10,241] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0,470 s
[2022-06-21 09:57:10,242] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 09:57:10,242] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-21 09:57:10,242] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0,472722 s
[2022-06-21 09:57:10,266] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO CodeGenerator: Code generated in 14.774402 ms
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - |  home_team|  away_team|odd_home|odd_draw|odd_away|
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - |Atletico-GO|Flamengo RJ|    4.06|    3.40|    1.97|
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - | Fluminense|     Santos|    1.77|    3.44|    5.29|
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - 
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - Traceback (most recent call last):
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 54, in <module>
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - export_json(odds_portal_df, '/home/lucas/pipeline-data/datalake/silver')
[2022-06-21 09:57:10,274] {spark_submit_hook.py:479} INFO - NameError: name 'export_json' is not defined
[2022-06-21 09:57:10,304] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 09:57:10,310] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 09:57:10,316] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 09:57:10,322] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO MemoryStore: MemoryStore cleared
[2022-06-21 09:57:10,322] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO BlockManager: BlockManager stopped
[2022-06-21 09:57:10,325] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 09:57:10,326] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 09:57:10,329] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 09:57:10,329] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 09:57:10,329] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-99942006-b86b-42f0-92c8-fc0dcb676820/pyspark-b077976f-991d-4768-92a2-92c471687843
[2022-06-21 09:57:10,330] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-99942006-b86b-42f0-92c8-fc0dcb676820
[2022-06-21 09:57:10,331] {spark_submit_hook.py:479} INFO - 22/06/21 09:57:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-58575fb4-e2d9-40ce-8912-72fa712c3a0a
[2022-06-21 09:57:10,374] {taskinstance.py:1150} ERROR - Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py. Error code is: 1.
Traceback (most recent call last):
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 403, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py. Error code is: 1.
[2022-06-21 09:57:10,375] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T125703, end_date=20220621T125710
[2022-06-21 09:57:13,473] {local_task_job.py:102} INFO - Task exited with return code 1
[2022-06-21 10:01:06,813] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 10:01:06,821] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 10:01:06,821] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:01:06,821] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 10:01:06,821] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:01:07,215] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 10:01:07,217] {standard_task_runner.py:54} INFO - Started process 14011 to run task
[2022-06-21 10:01:07,255] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '121', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpm5nmvt_m']
[2022-06-21 10:01:07,256] {standard_task_runner.py:78} INFO - Job 121: Subtask transform_football
[2022-06-21 10:01:07,400] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 10:01:07,410] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 10:01:07,411] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py
[2022-06-21 10:01:08,092] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:08 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 10:01:08,093] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 10:01:08,915] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 10:01:08,920] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:08 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 10:01:08,980] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 10:01:09,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO ResourceUtils: ==============================================================
[2022-06-21 10:01:09,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 10:01:09,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO ResourceUtils: ==============================================================
[2022-06-21 10:01:09,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 10:01:09,045] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 10:01:09,055] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 10:01:09,055] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 10:01:09,088] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 10:01:09,089] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 10:01:09,089] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 10:01:09,089] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 10:01:09,089] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 10:01:09,240] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO Utils: Successfully started service 'sparkDriver' on port 45627.
[2022-06-21 10:01:09,257] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 10:01:09,278] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 10:01:09,292] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 10:01:09,292] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 10:01:09,294] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 10:01:09,308] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ccc7090a-3227-431c-a580-e4e779e5a0c4
[2022-06-21 10:01:09,324] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 10:01:09,335] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 10:01:09,483] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 10:01:09,517] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 10:01:09,626] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 10:01:09,642] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42715.
[2022-06-21 10:01:09,642] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO NettyBlockTransferService: Server created on 192.168.0.13:42715
[2022-06-21 10:01:09,643] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 10:01:09,647] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 42715, None)
[2022-06-21 10:01:09,649] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:42715 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 42715, None)
[2022-06-21 10:01:09,650] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 42715, None)
[2022-06-21 10:01:09,651] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 42715, None)
[2022-06-21 10:01:09,841] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655816469840
[2022-06-21 10:01:09,841] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-98e597ac-d2b1-4091-8487-82b6a08a143b/userFiles-899b4947-02bf-4091-ba1f-4404ec53add8/helpers.py
[2022-06-21 10:01:09,924] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 10:01:09,926] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:09 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 10:01:10,412] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:10 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.
[2022-06-21 10:01:10,530] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:10 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:01:11,712] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:11 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:01:11,712] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:11 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:01:11,714] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:11 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:01:11,895] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 10:01:11,929] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 10:01:11,931] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:42715 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 10:01:11,933] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:11 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:11,938] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:01:12,048] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:12,071] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:01:12,072] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:01:12,072] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:01:12,072] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:01:12,075] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:01:12,124] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 10:01:12,126] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 10:01:12,126] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:42715 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 10:01:12,127] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:01:12,135] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:01:12,135] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 10:01:12,163] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-21 10:01:12,171] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 10:01:12,172] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655816469840
[2022-06-21 10:01:12,182] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-98e597ac-d2b1-4091-8487-82b6a08a143b/userFiles-899b4947-02bf-4091-ba1f-4404ec53add8/helpers.py
[2022-06-21 10:01:12,373] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 10:01:12,503] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO CodeGenerator: Code generated in 109.129827 ms
[2022-06-21 10:01:12,535] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 10:01:12,540] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 382 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:01:12,541] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 10:01:12,545] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,461 s
[2022-06-21 10:01:12,546] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:01:12,547] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 10:01:12,548] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,500012 s
[2022-06-21 10:01:12,565] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:01:12,569] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 10:01:12,607] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:01:12,608] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:01:12,608] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:01:12,612] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 10:01:12,619] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 10:01:12,619] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:42715 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:12,620] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:12,620] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197676 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:01:12,628] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:12,629] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:01:12,629] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:01:12,629] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:01:12,629] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:01:12,630] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:01:12,633] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 10:01:12,635] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 10:01:12,635] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:42715 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:12,636] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:01:12,636] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:01:12,636] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 10:01:12,638] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-21 10:01:12,638] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 10:01:12,643] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-10/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 10:01:12,652] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4079 bytes result sent to driver
[2022-06-21 10:01:12,654] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 17 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:01:12,654] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 10:01:12,654] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,023 s
[2022-06-21 10:01:12,655] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:01:12,655] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 10:01:12,655] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,026984 s
[2022-06-21 10:01:12,749] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:01:12,753] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 10:01:12,769] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:01:12,769] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:01:12,769] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:01:12,772] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 10:01:12,778] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 10:01:12,779] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:42715 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:12,779] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:12,779] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215261 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:01:12,786] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:12,786] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:01:12,786] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:01:12,786] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:01:12,786] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:01:12,787] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:01:12,789] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 10:01:12,790] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 10:01:12,790] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:42715 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:12,791] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:01:12,791] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:01:12,791] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 10:01:12,792] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-21 10:01:12,792] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 10:01:12,795] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-10/WhoScored_20220410.json, range: 0-20957, partition values: [empty row]
[2022-06-21 10:01:12,806] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2172 bytes result sent to driver
[2022-06-21 10:01:12,807] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 15 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:01:12,807] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 10:01:12,808] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,020 s
[2022-06-21 10:01:12,808] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:01:12,808] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 10:01:12,808] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,022352 s
[2022-06-21 10:01:12,876] {spark_submit_hook.py:479} INFO - root
[2022-06-21 10:01:12,876] {spark_submit_hook.py:479} INFO - |-- home_team: string (nullable = true)
[2022-06-21 10:01:12,876] {spark_submit_hook.py:479} INFO - |-- away_team: string (nullable = true)
[2022-06-21 10:01:12,876] {spark_submit_hook.py:479} INFO - |-- odd_home: string (nullable = true)
[2022-06-21 10:01:12,876] {spark_submit_hook.py:479} INFO - |-- odd_draw: string (nullable = true)
[2022-06-21 10:01:12,876] {spark_submit_hook.py:479} INFO - |-- odd_away: string (nullable = true)
[2022-06-21 10:01:12,876] {spark_submit_hook.py:479} INFO - 
[2022-06-21 10:01:12,927] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO DataSourceStrategy: Pruning directories with:
[2022-06-21 10:01:12,927] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:01:12,927] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:01:12,927] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 10:01:12,987] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO CodeGenerator: Code generated in 14.011223 ms
[2022-06-21 10:01:13,000] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:12 INFO CodeGenerator: Code generated in 9.220234 ms
[2022-06-21 10:01:13,003] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 337.9 KiB, free 364.8 MiB)
[2022-06-21 10:01:13,008] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 364.8 MiB)
[2022-06-21 10:01:13,008] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:42715 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:13,009] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:13,011] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:01:13,047] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:13,047] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:01:13,047] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:01:13,047] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:01:13,047] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:01:13,048] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:01:13,052] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 27.7 KiB, free 364.8 MiB)
[2022-06-21 10:01:13,053] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 364.8 MiB)
[2022-06-21 10:01:13,054] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:42715 (size: 12.2 KiB, free: 366.1 MiB)
[2022-06-21 10:01:13,054] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:01:13,054] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:01:13,054] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-21 10:01:13,055] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4944 bytes) taskResourceAssignments Map()
[2022-06-21 10:01:13,055] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-21 10:01:13,087] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO CodeGenerator: Code generated in 7.072531 ms
[2022-06-21 10:01:13,475] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [19092]
[2022-06-21 10:01:13,489] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO CodeGenerator: Code generated in 9.969372 ms
[2022-06-21 10:01:13,493] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO CodeGenerator: Code generated in 13.921209 ms
[2022-06-21 10:01:13,510] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO PythonUDFRunner: Times: total = 411, boot = 371, init = 40, finish = 0
[2022-06-21 10:01:13,512] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2485 bytes result sent to driver
[2022-06-21 10:01:13,513] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 457 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:01:13,513] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-21 10:01:13,513] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48223
[2022-06-21 10:01:13,514] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0,465 s
[2022-06-21 10:01:13,514] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:01:13,514] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-21 10:01:13,515] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0,467916 s
[2022-06-21 10:01:13,534] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO CodeGenerator: Code generated in 11.262496 ms
[2022-06-21 10:01:13,540] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 10:01:13,540] {spark_submit_hook.py:479} INFO - |  home_team|  away_team|odd_home|odd_draw|odd_away|
[2022-06-21 10:01:13,540] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 10:01:13,540] {spark_submit_hook.py:479} INFO - |Atletico-GO|Flamengo RJ|    4.06|    3.40|    1.97|
[2022-06-21 10:01:13,540] {spark_submit_hook.py:479} INFO - | Fluminense|     Santos|    1.77|    3.44|    5.29|
[2022-06-21 10:01:13,540] {spark_submit_hook.py:479} INFO - +-----------+-----------+--------+--------+--------+
[2022-06-21 10:01:13,540] {spark_submit_hook.py:479} INFO - 
[2022-06-21 10:01:13,566] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DataSourceStrategy: Pruning directories with:
[2022-06-21 10:01:13,567] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:01:13,567] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:01:13,567] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 10:01:13,591] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 10:01:13,592] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 10:01:13,592] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 10:01:13,662] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 337.9 KiB, free 364.4 MiB)
[2022-06-21 10:01:13,668] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 364.4 MiB)
[2022-06-21 10:01:13,668] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:42715 (size: 32.5 KiB, free: 366.1 MiB)
[2022-06-21 10:01:13,669] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkContext: Created broadcast 8 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:13,669] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:01:13,704] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.0.13:42715 in memory (size: 32.5 KiB, free: 366.1 MiB)
[2022-06-21 10:01:13,707] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:42715 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:13,708] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:01:13,709] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Got job 4 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:01:13,709] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Final stage: ResultStage 4 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:01:13,709] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:01:13,709] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:01:13,710] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.13:42715 in memory (size: 12.2 KiB, free: 366.2 MiB)
[2022-06-21 10:01:13,710] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Submitting ResultStage 4 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:01:13,712] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:42715 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:13,714] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:42715 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:13,715] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:42715 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:01:13,724] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 220.8 KiB, free 365.3 MiB)
[2022-06-21 10:01:13,725] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 78.9 KiB, free 365.3 MiB)
[2022-06-21 10:01:13,725] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:42715 (size: 78.9 KiB, free: 366.2 MiB)
[2022-06-21 10:01:13,726] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:01:13,726] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:01:13,726] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-21 10:01:13,728] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5173 bytes) taskResourceAssignments Map()
[2022-06-21 10:01:13,729] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-21 10:01:13,753] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 10:01:13,753] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 10:01:13,753] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 10:01:13,805] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [19092]
[2022-06-21 10:01:13,814] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO PythonUDFRunner: Times: total = 7, boot = -289, init = 296, finish = 0
[2022-06-21 10:01:13,819] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileOutputCommitter: Saved output of task 'attempt_202206211001135733128882780972379_0004_m_000000_4' to file:/home/lucas/pipeline-data/datalake/silver/_temporary/0/task_202206211001135733128882780972379_0004_m_000000
[2022-06-21 10:01:13,819] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkHadoopMapRedUtil: attempt_202206211001135733128882780972379_0004_m_000000_4: Committed
[2022-06-21 10:01:13,824] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3456 bytes result sent to driver
[2022-06-21 10:01:13,825] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 98 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:01:13,825] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-21 10:01:13,825] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: ResultStage 4 (json at NativeMethodAccessorImpl.java:0) finished in 0,114 s
[2022-06-21 10:01:13,826] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:01:13,826] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-21 10:01:13,826] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO DAGScheduler: Job 4 finished: json at NativeMethodAccessorImpl.java:0, took 0,118280 s
[2022-06-21 10:01:13,827] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileFormatWriter: Start to commit write Job 637e53de-fa17-409a-9f76-0499b540f28e.
[2022-06-21 10:01:13,834] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileFormatWriter: Write Job 637e53de-fa17-409a-9f76-0499b540f28e committed. Elapsed time: 6 ms.
[2022-06-21 10:01:13,836] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO FileFormatWriter: Finished processing stats for write job 637e53de-fa17-409a-9f76-0499b540f28e.
[2022-06-21 10:01:13,867] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 10:01:13,872] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 10:01:13,878] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 10:01:13,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO MemoryStore: MemoryStore cleared
[2022-06-21 10:01:13,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManager: BlockManager stopped
[2022-06-21 10:01:13,888] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 10:01:13,890] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 10:01:13,893] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 10:01:13,893] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 10:01:13,893] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-98e597ac-d2b1-4091-8487-82b6a08a143b
[2022-06-21 10:01:13,895] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-f1de84d4-1097-4871-85d2-07ee669acf04
[2022-06-21 10:01:13,896] {spark_submit_hook.py:479} INFO - 22/06/21 10:01:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-98e597ac-d2b1-4091-8487-82b6a08a143b/pyspark-ca8ffe37-a221-42bf-9cd2-687fc9dd3c56
[2022-06-21 10:01:13,945] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T130106, end_date=20220621T130113
[2022-06-21 10:01:16,660] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-06-21 10:37:49,880] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 10:37:49,887] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 10:37:49,888] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:37:49,888] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 10:37:49,888] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:37:50,093] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 10:37:50,095] {standard_task_runner.py:54} INFO - Started process 20116 to run task
[2022-06-21 10:37:50,131] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '122', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpo8nbhmoe']
[2022-06-21 10:37:50,131] {standard_task_runner.py:78} INFO - Job 122: Subtask transform_football
[2022-06-21 10:37:50,257] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 10:37:50,270] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 10:37:50,271] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/ --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-21 10:37:50,997] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:50 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 10:37:50,997] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 10:37:51,791] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 10:37:51,797] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 10:37:51,860] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 10:37:51,919] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO ResourceUtils: ==============================================================
[2022-06-21 10:37:51,919] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 10:37:51,919] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO ResourceUtils: ==============================================================
[2022-06-21 10:37:51,919] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 10:37:51,933] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 10:37:51,942] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 10:37:51,943] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 10:37:51,976] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 10:37:51,977] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 10:37:51,977] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 10:37:51,977] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 10:37:51,978] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 10:37:52,140] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO Utils: Successfully started service 'sparkDriver' on port 46411.
[2022-06-21 10:37:52,156] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 10:37:52,176] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 10:37:52,188] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 10:37:52,188] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 10:37:52,190] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 10:37:52,203] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d30f9431-9c02-47df-93af-8d9d24c14934
[2022-06-21 10:37:52,218] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 10:37:52,229] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 10:37:52,377] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 10:37:52,418] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 10:37:52,540] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 10:37:52,557] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42611.
[2022-06-21 10:37:52,557] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO NettyBlockTransferService: Server created on 192.168.0.13:42611
[2022-06-21 10:37:52,558] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 10:37:52,562] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 42611, None)
[2022-06-21 10:37:52,564] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:42611 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 42611, None)
[2022-06-21 10:37:52,566] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 42611, None)
[2022-06-21 10:37:52,567] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 42611, None)
[2022-06-21 10:37:52,774] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655818672773
[2022-06-21 10:37:52,774] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-5c139e2f-19d7-4df0-b5d7-fb63227b44f4/userFiles-9987411d-654f-4d7e-8808-5bb22f15e1c0/helpers.py
[2022-06-21 10:37:52,858] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 10:37:52,859] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:52 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 10:37:53,327] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:53 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.
[2022-06-21 10:37:53,452] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:37:54,647] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:37:54,647] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:37:54,649] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:37:54,822] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 10:37:54,854] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 10:37:54,856] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:42611 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 10:37:54,859] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:37:54,864] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:37:54,974] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:37:54,984] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:37:54,984] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:37:54,984] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:37:54,985] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:37:54,988] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:37:55,038] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 10:37:55,039] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 10:37:55,040] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:42611 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 10:37:55,040] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:37:55,049] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:37:55,050] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 10:37:55,080] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-21 10:37:55,088] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 10:37:55,090] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655818672773
[2022-06-21 10:37:55,100] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-5c139e2f-19d7-4df0-b5d7-fb63227b44f4/userFiles-9987411d-654f-4d7e-8808-5bb22f15e1c0/helpers.py
[2022-06-21 10:37:55,310] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 10:37:55,453] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO CodeGenerator: Code generated in 120.321916 ms
[2022-06-21 10:37:55,489] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 10:37:55,494] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 420 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:37:55,495] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 10:37:55,499] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,503 s
[2022-06-21 10:37:55,500] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:37:55,501] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 10:37:55,502] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,527385 s
[2022-06-21 10:37:55,519] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:37:55,523] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 10:37:55,564] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:37:55,564] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:37:55,564] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:37:55,569] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 10:37:55,577] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 10:37:55,578] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:42611 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:37:55,578] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:37:55,579] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197676 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:37:55,587] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:37:55,588] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:37:55,588] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:37:55,588] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:37:55,588] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:37:55,589] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:37:55,592] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 10:37:55,593] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 10:37:55,593] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:42611 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:37:55,594] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:37:55,594] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:37:55,594] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 10:37:55,595] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-21 10:37:55,596] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 10:37:55,600] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-10/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 10:37:55,610] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4079 bytes result sent to driver
[2022-06-21 10:37:55,611] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 16 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:37:55,611] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 10:37:55,612] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,022 s
[2022-06-21 10:37:55,612] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:37:55,612] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 10:37:55,613] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,025369 s
[2022-06-21 10:37:55,714] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:37:55,718] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 10:37:55,734] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:37:55,735] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:37:55,735] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:37:55,738] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 10:37:55,764] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 10:37:55,764] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:42611 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:37:55,765] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:37:55,765] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215261 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:37:55,771] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:37:55,772] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:37:55,772] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:37:55,772] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:37:55,772] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:37:55,773] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:37:55,776] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 10:37:55,777] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 10:37:55,777] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:42611 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:37:55,777] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:37:55,778] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:37:55,778] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 10:37:55,779] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-21 10:37:55,779] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 10:37:55,783] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-10/WhoScored_20220410.json, range: 0-20957, partition values: [empty row]
[2022-06-21 10:37:55,792] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2172 bytes result sent to driver
[2022-06-21 10:37:55,793] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 14 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:37:55,793] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 10:37:55,793] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,019 s
[2022-06-21 10:37:55,794] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:37:55,794] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 10:37:55,794] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,022302 s
[2022-06-21 10:37:55,854] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:42611 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:37:55,856] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:42611 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:37:55,858] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:42611 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:37:55,859] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:42611 in memory (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 10:37:55,871] {spark_submit_hook.py:479} INFO - Traceback (most recent call last):
[2022-06-21 10:37:55,871] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 58, in <module>
[2022-06-21 10:37:55,872] {spark_submit_hook.py:479} INFO - football_transform(spark, args.src, args.dest, args.process_date)
[2022-06-21 10:37:55,872] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 39, in football_transform
[2022-06-21 10:37:55,872] {spark_submit_hook.py:479} INFO - export_json(odds_portal_df, table_dest.format(table_name="odds_portal"))
[2022-06-21 10:37:55,872] {spark_submit_hook.py:479} INFO - NameError: name 'odds_portal_df' is not defined
[2022-06-21 10:37:55,891] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 10:37:55,896] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 10:37:55,902] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 10:37:55,908] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO MemoryStore: MemoryStore cleared
[2022-06-21 10:37:55,908] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManager: BlockManager stopped
[2022-06-21 10:37:55,910] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 10:37:55,911] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 10:37:55,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 10:37:55,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 10:37:55,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-a52cb933-6d49-4d34-ae78-0a6543c058bf
[2022-06-21 10:37:55,914] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c139e2f-19d7-4df0-b5d7-fb63227b44f4/pyspark-fb3ab61e-944d-40e1-b8d1-511f5771046c
[2022-06-21 10:37:55,915] {spark_submit_hook.py:479} INFO - 22/06/21 10:37:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c139e2f-19d7-4df0-b5d7-fb63227b44f4
[2022-06-21 10:37:55,946] {taskinstance.py:1150} ERROR - Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/ --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10. Error code is: 1.
Traceback (most recent call last):
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 403, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/ --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10. Error code is: 1.
[2022-06-21 10:37:55,947] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T133749, end_date=20220621T133755
[2022-06-21 10:37:59,709] {local_task_job.py:102} INFO - Task exited with return code 1
[2022-06-21 10:44:20,856] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 10:44:20,864] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 10:44:20,864] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:44:20,864] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 10:44:20,864] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:44:21,047] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 10:44:21,049] {standard_task_runner.py:54} INFO - Started process 21456 to run task
[2022-06-21 10:44:21,088] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '122', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmp3_n3lkjd']
[2022-06-21 10:44:21,089] {standard_task_runner.py:78} INFO - Job 122: Subtask transform_football
[2022-06-21 10:44:21,268] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 10:44:21,281] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 10:44:21,281] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/ --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-21 10:44:22,033] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 10:44:22,034] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 10:44:22,857] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 10:44:22,862] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 10:44:22,927] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 10:44:22,980] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 INFO ResourceUtils: ==============================================================
[2022-06-21 10:44:22,980] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 10:44:22,980] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 INFO ResourceUtils: ==============================================================
[2022-06-21 10:44:22,980] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 10:44:22,995] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 10:44:23,006] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 10:44:23,006] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 10:44:23,040] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 10:44:23,040] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 10:44:23,041] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 10:44:23,041] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 10:44:23,041] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 10:44:23,195] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO Utils: Successfully started service 'sparkDriver' on port 39153.
[2022-06-21 10:44:23,212] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 10:44:23,233] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 10:44:23,246] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 10:44:23,246] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 10:44:23,248] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 10:44:23,261] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dc538425-d8fb-4cf3-858a-aadf7fd0591e
[2022-06-21 10:44:23,277] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 10:44:23,288] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 10:44:23,438] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 10:44:23,474] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 10:44:23,588] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 10:44:23,603] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41981.
[2022-06-21 10:44:23,603] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO NettyBlockTransferService: Server created on 192.168.0.13:41981
[2022-06-21 10:44:23,604] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 10:44:23,609] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 41981, None)
[2022-06-21 10:44:23,611] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:41981 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 41981, None)
[2022-06-21 10:44:23,612] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 41981, None)
[2022-06-21 10:44:23,613] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 41981, None)
[2022-06-21 10:44:23,819] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655819063819
[2022-06-21 10:44:23,820] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-4ae88551-8d56-4dc7-8a46-432ca9fbe209/userFiles-75f2c556-5788-415d-8538-eb88a7273e32/helpers.py
[2022-06-21 10:44:23,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 10:44:23,914] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:23 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 10:44:24,405] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:24 INFO InMemoryFileIndex: It took 20 ms to list leaf files for 1 paths.
[2022-06-21 10:44:24,539] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:24 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:44:25,824] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:25 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:44:25,825] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:25 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:44:25,827] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:25 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:44:26,017] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 10:44:26,050] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 10:44:26,051] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:41981 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 10:44:26,054] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:44:26,059] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:44:26,166] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:44:26,175] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:44:26,176] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:44:26,176] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:44:26,177] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:44:26,179] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:44:26,232] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 10:44:26,234] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 10:44:26,234] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:41981 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 10:44:26,234] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:44:26,243] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:44:26,243] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 10:44:26,272] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-21 10:44:26,280] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 10:44:26,281] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655819063819
[2022-06-21 10:44:26,292] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-4ae88551-8d56-4dc7-8a46-432ca9fbe209/userFiles-75f2c556-5788-415d-8538-eb88a7273e32/helpers.py
[2022-06-21 10:44:26,501] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 10:44:26,634] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO CodeGenerator: Code generated in 110.620221 ms
[2022-06-21 10:44:26,666] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 10:44:26,671] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:44:26,673] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 10:44:26,676] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,488 s
[2022-06-21 10:44:26,678] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:44:26,678] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 10:44:26,680] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,513636 s
[2022-06-21 10:44:26,698] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:44:26,703] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 10:44:26,746] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:44:26,746] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:44:26,746] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:44:26,751] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 10:44:26,760] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 10:44:26,760] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:41981 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:44:26,761] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:44:26,761] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197676 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:44:26,769] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:44:26,770] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:44:26,770] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:44:26,770] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:44:26,770] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:44:26,771] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:44:26,774] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 10:44:26,776] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 10:44:26,776] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:41981 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:44:26,777] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:44:26,777] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:44:26,777] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 10:44:26,779] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-21 10:44:26,779] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 10:44:26,784] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-10/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 10:44:26,795] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4079 bytes result sent to driver
[2022-06-21 10:44:26,797] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 19 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:44:26,797] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 10:44:26,798] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,026 s
[2022-06-21 10:44:26,798] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:44:26,798] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 10:44:26,798] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,029674 s
[2022-06-21 10:44:26,900] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:44:26,904] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 10:44:26,922] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:44:26,922] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:44:26,922] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:44:26,926] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 10:44:26,935] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 10:44:26,935] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:41981 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:44:26,936] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:44:26,936] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215261 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:44:26,942] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:44:26,943] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:44:26,943] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:44:26,943] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:44:26,943] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:44:26,944] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:44:26,946] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 10:44:26,948] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 10:44:26,948] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:41981 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:44:26,948] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:44:26,949] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:44:26,949] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 10:44:26,950] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-21 10:44:26,950] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 10:44:26,954] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-10/WhoScored_20220410.json, range: 0-20957, partition values: [empty row]
[2022-06-21 10:44:26,962] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2172 bytes result sent to driver
[2022-06-21 10:44:26,963] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 13 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:44:26,963] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 10:44:26,964] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,020 s
[2022-06-21 10:44:26,964] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:44:26,964] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 10:44:26,964] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:26 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,021770 s
[2022-06-21 10:44:27,103] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DataSourceStrategy: Pruning directories with:
[2022-06-21 10:44:27,104] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:44:27,104] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:44:27,104] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 10:44:27,150] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 10:44:27,150] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 10:44:27,150] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 10:44:27,177] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:41981 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:44:27,179] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:41981 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:44:27,181] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:41981 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:44:27,183] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:41981 in memory (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 10:44:27,201] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO CodeGenerator: Code generated in 13.645231 ms
[2022-06-21 10:44:27,215] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO CodeGenerator: Code generated in 9.574826 ms
[2022-06-21 10:44:27,218] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 337.9 KiB, free 365.6 MiB)
[2022-06-21 10:44:27,224] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 10:44:27,224] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:41981 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:44:27,225] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:44:27,228] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:44:27,288] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:44:27,289] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:44:27,289] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:44:27,289] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:44:27,289] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:44:27,290] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[18] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:44:27,309] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 221.3 KiB, free 365.3 MiB)
[2022-06-21 10:44:27,311] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 79.1 KiB, free 365.3 MiB)
[2022-06-21 10:44:27,312] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:41981 (size: 79.1 KiB, free: 366.2 MiB)
[2022-06-21 10:44:27,312] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:44:27,313] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[18] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:44:27,313] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-21 10:44:27,315] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5173 bytes) taskResourceAssignments Map()
[2022-06-21 10:44:27,316] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-21 10:44:27,352] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 10:44:27,352] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 10:44:27,352] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 10:44:27,385] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO CodeGenerator: Code generated in 8.411175 ms
[2022-06-21 10:44:27,798] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [19092]
[2022-06-21 10:44:27,813] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO CodeGenerator: Code generated in 11.110036 ms
[2022-06-21 10:44:27,822] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO CodeGenerator: Code generated in 20.366698 ms
[2022-06-21 10:44:27,870] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO PythonUDFRunner: Times: total = 466, boot = 395, init = 71, finish = 0
[2022-06-21 10:44:27,879] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileOutputCommitter: Saved output of task 'attempt_202206211044273951125865855756623_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-10/_temporary/0/task_202206211044273951125865855756623_0003_m_000000
[2022-06-21 10:44:27,879] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SparkHadoopMapRedUtil: attempt_202206211044273951125865855756623_0003_m_000000_3: Committed
[2022-06-21 10:44:27,884] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3456 bytes result sent to driver
[2022-06-21 10:44:27,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 572 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:44:27,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-21 10:44:27,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33753
[2022-06-21 10:44:27,887] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,596 s
[2022-06-21 10:44:27,887] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:44:27,887] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-21 10:44:27,888] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,600000 s
[2022-06-21 10:44:27,889] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileFormatWriter: Start to commit write Job 3d56fb3f-82b8-4572-80d3-00333ef9a946.
[2022-06-21 10:44:27,897] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileFormatWriter: Write Job 3d56fb3f-82b8-4572-80d3-00333ef9a946 committed. Elapsed time: 6 ms.
[2022-06-21 10:44:27,899] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO FileFormatWriter: Finished processing stats for write job 3d56fb3f-82b8-4572-80d3-00333ef9a946.
[2022-06-21 10:44:27,930] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 10:44:27,935] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 10:44:27,941] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 10:44:27,947] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO MemoryStore: MemoryStore cleared
[2022-06-21 10:44:27,947] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO BlockManager: BlockManager stopped
[2022-06-21 10:44:27,950] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 10:44:27,951] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 10:44:27,954] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 10:44:27,954] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 10:44:27,954] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ae88551-8d56-4dc7-8a46-432ca9fbe209
[2022-06-21 10:44:27,955] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d42894b-602e-49d6-9c97-cdcf14717cdd
[2022-06-21 10:44:27,956] {spark_submit_hook.py:479} INFO - 22/06/21 10:44:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ae88551-8d56-4dc7-8a46-432ca9fbe209/pyspark-e87e2196-b741-4065-a060-7d909bdac202
[2022-06-21 10:44:28,000] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T134420, end_date=20220621T134428
[2022-06-21 10:44:30,498] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-06-21 11:09:14,842] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 11:09:14,849] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 11:09:14,849] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 11:09:14,849] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 11:09:14,849] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 11:09:15,051] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 11:09:15,053] {standard_task_runner.py:54} INFO - Started process 26431 to run task
[2022-06-21 11:09:15,090] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '122', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpjerwh5hx']
[2022-06-21 11:09:15,090] {standard_task_runner.py:78} INFO - Job 122: Subtask transform_football
[2022-06-21 11:09:15,259] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 11:09:15,271] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 11:09:15,272] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-21 11:09:15,941] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:15 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 11:09:15,941] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 11:09:16,741] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 11:09:16,747] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 11:09:16,808] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 11:09:16,860] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO ResourceUtils: ==============================================================
[2022-06-21 11:09:16,860] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 11:09:16,860] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO ResourceUtils: ==============================================================
[2022-06-21 11:09:16,860] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 11:09:16,875] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 11:09:16,885] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 11:09:16,885] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 11:09:16,918] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 11:09:16,919] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 11:09:16,919] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 11:09:16,919] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 11:09:16,920] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 11:09:17,072] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO Utils: Successfully started service 'sparkDriver' on port 34211.
[2022-06-21 11:09:17,088] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 11:09:17,108] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 11:09:17,120] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 11:09:17,120] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 11:09:17,122] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 11:09:17,135] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8d4bccfa-0746-4a83-bf88-927eb6c06e86
[2022-06-21 11:09:17,151] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 11:09:17,161] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 11:09:17,299] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 11:09:17,336] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 11:09:17,452] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 11:09:17,468] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42411.
[2022-06-21 11:09:17,468] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO NettyBlockTransferService: Server created on 192.168.0.13:42411
[2022-06-21 11:09:17,469] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 11:09:17,473] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 42411, None)
[2022-06-21 11:09:17,474] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:42411 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 42411, None)
[2022-06-21 11:09:17,476] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 42411, None)
[2022-06-21 11:09:17,476] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 42411, None)
[2022-06-21 11:09:17,680] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655820557680
[2022-06-21 11:09:17,680] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-3ef9b9aa-c4ff-4872-b870-94038d4e140f/userFiles-35eed91e-9c54-41ea-bf09-4878f5976696/helpers.py
[2022-06-21 11:09:17,774] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 11:09:17,775] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:17 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 11:09:18,260] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:18 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.
[2022-06-21 11:09:18,360] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:18 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 11:09:19,564] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 11:09:19,565] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 11:09:19,567] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 11:09:19,741] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 11:09:19,772] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 11:09:19,774] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:42411 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 11:09:19,777] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 11:09:19,782] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 11:09:19,885] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 11:09:19,894] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 11:09:19,894] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 11:09:19,894] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 11:09:19,895] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO DAGScheduler: Missing parents: List()
[2022-06-21 11:09:19,898] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 11:09:19,947] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 11:09:19,949] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 11:09:19,949] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:42411 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 11:09:19,949] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 11:09:19,958] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 11:09:19,958] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 11:09:19,988] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-21 11:09:19,996] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 11:09:19,997] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:19 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655820557680
[2022-06-21 11:09:20,008] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-3ef9b9aa-c4ff-4872-b870-94038d4e140f/userFiles-35eed91e-9c54-41ea-bf09-4878f5976696/helpers.py
[2022-06-21 11:09:20,209] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 11:09:20,342] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO CodeGenerator: Code generated in 111.639682 ms
[2022-06-21 11:09:20,375] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 11:09:20,380] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 399 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 11:09:20,382] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 11:09:20,385] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,479 s
[2022-06-21 11:09:20,387] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 11:09:20,387] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 11:09:20,389] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,503530 s
[2022-06-21 11:09:20,404] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 11:09:20,408] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 11:09:20,446] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 11:09:20,446] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 11:09:20,446] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 11:09:20,451] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 11:09:20,458] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 11:09:20,458] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:42411 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,459] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 11:09:20,459] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197676 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 11:09:20,467] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 11:09:20,467] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 11:09:20,467] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 11:09:20,467] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 11:09:20,468] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Missing parents: List()
[2022-06-21 11:09:20,468] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 11:09:20,471] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 11:09:20,472] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 11:09:20,473] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:42411 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,473] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 11:09:20,474] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 11:09:20,474] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 11:09:20,475] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-21 11:09:20,475] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 11:09:20,480] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 11:09:20,489] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4079 bytes result sent to driver
[2022-06-21 11:09:20,491] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 16 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 11:09:20,491] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 11:09:20,491] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,022 s
[2022-06-21 11:09:20,491] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 11:09:20,491] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 11:09:20,492] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,024824 s
[2022-06-21 11:09:20,588] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 11:09:20,592] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 11:09:20,610] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 11:09:20,610] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 11:09:20,610] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 11:09:20,614] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 11:09:20,643] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 11:09:20,643] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:42411 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,644] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 11:09:20,644] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215261 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 11:09:20,650] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 11:09:20,651] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 11:09:20,651] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 11:09:20,651] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 11:09:20,651] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Missing parents: List()
[2022-06-21 11:09:20,652] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 11:09:20,654] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 11:09:20,655] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 11:09:20,656] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:42411 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,656] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 11:09:20,656] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 11:09:20,656] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 11:09:20,657] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-21 11:09:20,657] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 11:09:20,661] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-20957, partition values: [empty row]
[2022-06-21 11:09:20,670] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2172 bytes result sent to driver
[2022-06-21 11:09:20,671] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 13 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 11:09:20,671] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 11:09:20,671] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,018 s
[2022-06-21 11:09:20,671] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 11:09:20,671] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 11:09:20,672] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,021057 s
[2022-06-21 11:09:20,792] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:42411 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,795] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:42411 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,797] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:42411 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,798] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:42411 in memory (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 11:09:20,811] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 11:09:20,811] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 11:09:20,812] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 11:09:20,852] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 11:09:20,852] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 11:09:20,853] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 11:09:20,920] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO CodeGenerator: Code generated in 13.753082 ms
[2022-06-21 11:09:20,923] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 337.9 KiB, free 365.6 MiB)
[2022-06-21 11:09:20,929] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 11:09:20,930] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:42411 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,930] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 11:09:20,932] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 11:09:20,979] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 11:09:20,980] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 11:09:20,980] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 11:09:20,980] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 11:09:20,980] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Missing parents: List()
[2022-06-21 11:09:20,981] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 11:09:20,997] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 217.5 KiB, free 365.3 MiB)
[2022-06-21 11:09:20,998] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 365.3 MiB)
[2022-06-21 11:09:20,999] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:42411 (size: 78.6 KiB, free: 366.2 MiB)
[2022-06-21 11:09:20,999] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
[2022-06-21 11:09:21,000] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 11:09:21,000] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-21 11:09:21,002] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes) taskResourceAssignments Map()
[2022-06-21 11:09:21,002] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-21 11:09:21,037] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 11:09:21,037] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 11:09:21,037] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 11:09:21,069] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO CodeGenerator: Code generated in 8.530243 ms
[2022-06-21 11:09:21,457] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 11:09:21,473] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO CodeGenerator: Code generated in 12.522024 ms
[2022-06-21 11:09:21,473] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO CodeGenerator: Code generated in 13.887908 ms
[2022-06-21 11:09:21,498] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO PythonUDFRunner: Times: total = 414, boot = 369, init = 45, finish = 0
[2022-06-21 11:09:21,503] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO FileOutputCommitter: Saved output of task 'attempt_202206211109202433731425658091674_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-10/_temporary/0/task_202206211109202433731425658091674_0003_m_000000
[2022-06-21 11:09:21,504] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO SparkHadoopMapRedUtil: attempt_202206211109202433731425658091674_0003_m_000000_3: Committed
[2022-06-21 11:09:21,508] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3225 bytes result sent to driver
[2022-06-21 11:09:21,509] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 509 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 11:09:21,509] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-21 11:09:21,510] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 43145
[2022-06-21 11:09:21,511] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,529 s
[2022-06-21 11:09:21,511] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 11:09:21,511] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-21 11:09:21,512] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,532507 s
[2022-06-21 11:09:21,513] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO FileFormatWriter: Start to commit write Job 0880bcae-871c-4465-be21-5f18a11b6a1d.
[2022-06-21 11:09:21,519] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO FileFormatWriter: Write Job 0880bcae-871c-4465-be21-5f18a11b6a1d committed. Elapsed time: 5 ms.
[2022-06-21 11:09:21,521] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO FileFormatWriter: Finished processing stats for write job 0880bcae-871c-4465-be21-5f18a11b6a1d.
[2022-06-21 11:09:21,546] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 11:09:21,551] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 11:09:21,556] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 11:09:21,563] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO MemoryStore: MemoryStore cleared
[2022-06-21 11:09:21,563] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO BlockManager: BlockManager stopped
[2022-06-21 11:09:21,566] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 11:09:21,568] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 11:09:21,570] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 11:09:21,570] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 11:09:21,571] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-3ef9b9aa-c4ff-4872-b870-94038d4e140f
[2022-06-21 11:09:21,572] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-739a3da9-4937-4974-a240-3bf6ba73900e
[2022-06-21 11:09:21,573] {spark_submit_hook.py:479} INFO - 22/06/21 11:09:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-3ef9b9aa-c4ff-4872-b870-94038d4e140f/pyspark-0042a4c6-9d89-41ba-b7d2-ea778784fabf
[2022-06-21 11:09:21,615] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T140914, end_date=20220621T140921
[2022-06-21 11:09:24,675] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-06-21 12:07:41,138] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 12:07:41,145] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 12:07:41,145] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 12:07:41,145] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 12:07:41,145] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 12:07:41,315] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 12:07:41,317] {standard_task_runner.py:54} INFO - Started process 37253 to run task
[2022-06-21 12:07:41,354] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '122', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmp1war7hjf']
[2022-06-21 12:07:41,355] {standard_task_runner.py:78} INFO - Job 122: Subtask transform_football
[2022-06-21 12:07:41,483] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 12:07:41,495] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 12:07:41,496] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-21 12:07:42,223] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:42 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 12:07:42,223] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 12:07:43,026] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 12:07:43,032] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 12:07:43,097] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 12:07:43,155] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO ResourceUtils: ==============================================================
[2022-06-21 12:07:43,155] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 12:07:43,156] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO ResourceUtils: ==============================================================
[2022-06-21 12:07:43,156] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 12:07:43,170] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 12:07:43,179] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 12:07:43,180] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 12:07:43,214] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 12:07:43,215] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 12:07:43,215] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 12:07:43,215] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 12:07:43,215] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 12:07:43,384] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO Utils: Successfully started service 'sparkDriver' on port 37551.
[2022-06-21 12:07:43,400] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 12:07:43,420] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 12:07:43,432] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 12:07:43,433] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 12:07:43,435] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 12:07:43,447] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-444d0503-063b-40ff-9a53-fb90873bcb13
[2022-06-21 12:07:43,463] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 12:07:43,473] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 12:07:43,616] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 12:07:43,650] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 12:07:43,763] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 12:07:43,779] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46751.
[2022-06-21 12:07:43,779] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO NettyBlockTransferService: Server created on 192.168.0.13:46751
[2022-06-21 12:07:43,780] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 12:07:43,784] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 46751, None)
[2022-06-21 12:07:43,786] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:46751 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 46751, None)
[2022-06-21 12:07:43,788] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 46751, None)
[2022-06-21 12:07:43,789] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 46751, None)
[2022-06-21 12:07:43,988] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655824063988
[2022-06-21 12:07:43,989] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:43 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-ee6941dd-0c1a-46b9-b713-d4e592e5bf85/userFiles-bce2f0a9-5f5c-47c4-a936-f1fb1e655db0/helpers.py
[2022-06-21 12:07:44,072] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 12:07:44,073] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 12:07:44,528] {spark_submit_hook.py:479} INFO - Traceback (most recent call last):
[2022-06-21 12:07:44,528] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 82, in <module>
[2022-06-21 12:07:44,529] {spark_submit_hook.py:479} INFO - football_transform(spark, "/home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-11", "", "")
[2022-06-21 12:07:44,529] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 44, in football_transform
[2022-06-21 12:07:44,529] {spark_submit_hook.py:479} INFO - op_raw_df = spark.read.json(src + '/odds_portal')
[2022-06-21 12:07:44,529] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 229, in json
[2022-06-21 12:07:44,529] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2022-06-21 12:07:44,529] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
[2022-06-21 12:07:44,531] {spark_submit_hook.py:479} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-11/odds_portal
[2022-06-21 12:07:44,551] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 12:07:44,557] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 12:07:44,563] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 12:07:44,568] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO MemoryStore: MemoryStore cleared
[2022-06-21 12:07:44,569] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO BlockManager: BlockManager stopped
[2022-06-21 12:07:44,581] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 12:07:44,583] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 12:07:44,585] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 12:07:44,638] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 12:07:44,639] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-55c59097-7757-4ad8-933b-04590dbf7d5b
[2022-06-21 12:07:44,640] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee6941dd-0c1a-46b9-b713-d4e592e5bf85
[2022-06-21 12:07:44,641] {spark_submit_hook.py:479} INFO - 22/06/21 12:07:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee6941dd-0c1a-46b9-b713-d4e592e5bf85/pyspark-ff4a1b34-ae42-4014-a70e-4d21fce93875
[2022-06-21 12:07:44,666] {taskinstance.py:1150} ERROR - Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10. Error code is: 1.
Traceback (most recent call last):
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 403, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10. Error code is: 1.
[2022-06-21 12:07:44,667] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T150741, end_date=20220621T150744
[2022-06-21 12:07:46,003] {local_task_job.py:102} INFO - Task exited with return code 1
[2022-06-21 12:11:54,362] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 12:11:54,369] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 12:11:54,369] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 12:11:54,369] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 12:11:54,369] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 12:11:54,583] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 12:11:54,585] {standard_task_runner.py:54} INFO - Started process 38190 to run task
[2022-06-21 12:11:54,630] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '122', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpw4k1osyh']
[2022-06-21 12:11:54,631] {standard_task_runner.py:78} INFO - Job 122: Subtask transform_football
[2022-06-21 12:11:55,157] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [None]> on host lucas-AB350M-DS3H-V2
[2022-06-21 12:11:55,170] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 12:11:55,171] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-21 12:11:55,953] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:55 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 12:11:55,954] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 12:11:56,793] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 12:11:56,799] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 12:11:56,866] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 12:11:56,926] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO ResourceUtils: ==============================================================
[2022-06-21 12:11:56,926] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 12:11:56,926] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO ResourceUtils: ==============================================================
[2022-06-21 12:11:56,927] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 12:11:56,942] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 12:11:56,951] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 12:11:56,951] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 12:11:56,987] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 12:11:56,987] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 12:11:56,988] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 12:11:56,988] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 12:11:56,988] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 12:11:57,159] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO Utils: Successfully started service 'sparkDriver' on port 40375.
[2022-06-21 12:11:57,175] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 12:11:57,196] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 12:11:57,208] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 12:11:57,209] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 12:11:57,211] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 12:11:57,227] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9f9bdf1a-8172-42d5-9b7b-5d89a7084ebe
[2022-06-21 12:11:57,248] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 12:11:57,263] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 12:11:57,424] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 12:11:57,460] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 12:11:57,582] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 12:11:57,599] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40145.
[2022-06-21 12:11:57,599] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO NettyBlockTransferService: Server created on 192.168.0.13:40145
[2022-06-21 12:11:57,600] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 12:11:57,605] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 40145, None)
[2022-06-21 12:11:57,607] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:40145 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 40145, None)
[2022-06-21 12:11:57,609] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 40145, None)
[2022-06-21 12:11:57,610] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 40145, None)
[2022-06-21 12:11:57,819] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655824317818
[2022-06-21 12:11:57,820] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-1fc0ac01-8220-4287-a27f-591ce3e9fc28/userFiles-933c7a73-1402-42b1-9280-f2763f80e50b/helpers.py
[2022-06-21 12:11:57,910] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 12:11:57,912] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:57 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 12:11:58,405] {spark_submit_hook.py:479} INFO - Traceback (most recent call last):
[2022-06-21 12:11:58,405] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 82, in <module>
[2022-06-21 12:11:58,405] {spark_submit_hook.py:479} INFO - football_transform(spark, "/home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-11", "", "")
[2022-06-21 12:11:58,405] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 44, in football_transform
[2022-06-21 12:11:58,405] {spark_submit_hook.py:479} INFO - op_raw_df = spark.read.json(src + '/odds_portal')
[2022-06-21 12:11:58,405] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 229, in json
[2022-06-21 12:11:58,405] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2022-06-21 12:11:58,405] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
[2022-06-21 12:11:58,408] {spark_submit_hook.py:479} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-11/odds_portal
[2022-06-21 12:11:58,429] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 12:11:58,434] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 12:11:58,441] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 12:11:58,446] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO MemoryStore: MemoryStore cleared
[2022-06-21 12:11:58,446] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO BlockManager: BlockManager stopped
[2022-06-21 12:11:58,452] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 12:11:58,454] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 12:11:58,455] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 12:11:58,512] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 12:11:58,514] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-1fc0ac01-8220-4287-a27f-591ce3e9fc28
[2022-06-21 12:11:58,515] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-1fc0ac01-8220-4287-a27f-591ce3e9fc28/pyspark-22195916-0ccd-42a4-972a-01d2624065fd
[2022-06-21 12:11:58,515] {spark_submit_hook.py:479} INFO - 22/06/21 12:11:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-816efa33-69e8-4381-9ff2-8474e4dfd9a2
[2022-06-21 12:11:58,548] {taskinstance.py:1150} ERROR - Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10. Error code is: 1.
Traceback (most recent call last):
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 403, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10. Error code is: 1.
[2022-06-21 12:11:58,548] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=, end_date=20220621T151158
[2022-06-21 12:11:59,378] {local_task_job.py:102} INFO - Task exited with return code 1
[2022-06-21 12:22:10,695] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 12:22:10,702] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 12:22:10,702] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 12:22:10,702] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 12:22:10,702] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 12:22:11,031] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 12:22:11,033] {standard_task_runner.py:54} INFO - Started process 40255 to run task
[2022-06-21 12:22:11,072] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '122', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmprdu818t5']
[2022-06-21 12:22:11,072] {standard_task_runner.py:78} INFO - Job 122: Subtask transform_football
[2022-06-21 12:22:11,211] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 12:22:11,223] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 12:22:11,224] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-21 12:22:11,986] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:11 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 12:22:11,987] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 12:22:12,784] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 12:22:12,789] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 12:22:12,850] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 12:22:12,900] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO ResourceUtils: ==============================================================
[2022-06-21 12:22:12,900] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 12:22:12,900] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO ResourceUtils: ==============================================================
[2022-06-21 12:22:12,900] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 12:22:12,915] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 12:22:12,923] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 12:22:12,924] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 12:22:12,958] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 12:22:12,958] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 12:22:12,958] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 12:22:12,958] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 12:22:12,959] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 12:22:13,111] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO Utils: Successfully started service 'sparkDriver' on port 40957.
[2022-06-21 12:22:13,128] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 12:22:13,148] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 12:22:13,160] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 12:22:13,160] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 12:22:13,162] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 12:22:13,175] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d12e5206-4558-440e-90d3-d04f4603f25b
[2022-06-21 12:22:13,190] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 12:22:13,200] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 12:22:13,340] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 12:22:13,374] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 12:22:13,484] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 12:22:13,498] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45805.
[2022-06-21 12:22:13,498] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO NettyBlockTransferService: Server created on 192.168.0.13:45805
[2022-06-21 12:22:13,499] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 12:22:13,503] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 45805, None)
[2022-06-21 12:22:13,506] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:45805 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 45805, None)
[2022-06-21 12:22:13,507] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 45805, None)
[2022-06-21 12:22:13,508] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 45805, None)
[2022-06-21 12:22:13,702] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655824933701
[2022-06-21 12:22:13,702] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-8d1b0dfa-b5c3-4eef-981c-71628a31ba5e/userFiles-98dc027e-9217-4c5c-aa5e-0eed5bdeb6b8/helpers.py
[2022-06-21 12:22:13,793] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 12:22:13,794] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:13 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 12:22:14,249] {spark_submit_hook.py:479} INFO - Traceback (most recent call last):
[2022-06-21 12:22:14,249] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 82, in <module>
[2022-06-21 12:22:14,249] {spark_submit_hook.py:479} INFO - football_transform(spark, "/home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-11", "", "")
[2022-06-21 12:22:14,249] {spark_submit_hook.py:479} INFO - File "/home/lucas/pipeline-data/spark/transformation.py", line 44, in football_transform
[2022-06-21 12:22:14,249] {spark_submit_hook.py:479} INFO - op_raw_df = spark.read.json(src + '/odds_portal')
[2022-06-21 12:22:14,249] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 229, in json
[2022-06-21 12:22:14,249] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2022-06-21 12:22:14,249] {spark_submit_hook.py:479} INFO - File "/home/lucas/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
[2022-06-21 12:22:14,252] {spark_submit_hook.py:479} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-11/odds_portal
[2022-06-21 12:22:14,271] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 12:22:14,277] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 12:22:14,284] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 12:22:14,290] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO MemoryStore: MemoryStore cleared
[2022-06-21 12:22:14,290] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO BlockManager: BlockManager stopped
[2022-06-21 12:22:14,296] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 12:22:14,298] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 12:22:14,300] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 12:22:14,359] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 12:22:14,359] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-a3f715ed-66d7-4779-b2b1-2edbce877077
[2022-06-21 12:22:14,360] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d1b0dfa-b5c3-4eef-981c-71628a31ba5e
[2022-06-21 12:22:14,361] {spark_submit_hook.py:479} INFO - 22/06/21 12:22:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d1b0dfa-b5c3-4eef-981c-71628a31ba5e/pyspark-9188e8c7-f428-4557-bd1c-a37fb638c34a
[2022-06-21 12:22:14,389] {taskinstance.py:1150} ERROR - Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10. Error code is: 1.
Traceback (most recent call last):
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/lucas/pipeline-data/.env/lib/python3.9/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 403, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10. Error code is: 1.
[2022-06-21 12:22:14,389] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T152210, end_date=20220621T152214
[2022-06-21 12:22:15,542] {local_task_job.py:102} INFO - Task exited with return code 1
[2022-06-21 13:10:41,838] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 13:10:41,845] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-21 13:10:41,845] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 13:10:41,845] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 13:10:41,845] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 13:10:42,028] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-21 13:10:42,029] {standard_task_runner.py:54} INFO - Started process 50421 to run task
[2022-06-21 13:10:42,066] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '122', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpp7aru8hk']
[2022-06-21 13:10:42,066] {standard_task_runner.py:78} INFO - Job 122: Subtask transform_football
[2022-06-21 13:10:42,209] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 13:10:42,221] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 13:10:42,222] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-21 13:10:42,926] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:42 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 13:10:42,926] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 13:10:43,701] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 13:10:43,706] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 13:10:43,769] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 13:10:43,826] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO ResourceUtils: ==============================================================
[2022-06-21 13:10:43,826] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 13:10:43,826] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO ResourceUtils: ==============================================================
[2022-06-21 13:10:43,826] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 13:10:43,842] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 13:10:43,852] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 13:10:43,852] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 13:10:43,887] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 13:10:43,888] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 13:10:43,888] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 13:10:43,888] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 13:10:43,889] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 13:10:44,052] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO Utils: Successfully started service 'sparkDriver' on port 38635.
[2022-06-21 13:10:44,070] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 13:10:44,090] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 13:10:44,102] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 13:10:44,102] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 13:10:44,105] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 13:10:44,117] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c8000e74-21ac-47ef-aec7-5a5630a0148f
[2022-06-21 13:10:44,132] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 13:10:44,143] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 13:10:44,289] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 13:10:44,329] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 13:10:44,445] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 13:10:44,461] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34675.
[2022-06-21 13:10:44,461] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO NettyBlockTransferService: Server created on 192.168.0.13:34675
[2022-06-21 13:10:44,462] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 13:10:44,467] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 34675, None)
[2022-06-21 13:10:44,469] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:34675 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 34675, None)
[2022-06-21 13:10:44,471] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 34675, None)
[2022-06-21 13:10:44,472] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 34675, None)
[2022-06-21 13:10:44,680] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655827844679
[2022-06-21 13:10:44,680] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-efda3e45-44aa-41ab-98a4-921fb09b2251/userFiles-d1d9e641-9bdd-4199-9856-71f8cf090ab4/helpers.py
[2022-06-21 13:10:44,767] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 13:10:44,768] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:44 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 13:10:45,233] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:45 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.
[2022-06-21 13:10:45,334] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:45 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 13:10:46,557] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 13:10:46,558] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 13:10:46,560] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 13:10:46,733] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 13:10:46,767] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 13:10:46,769] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:34675 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 13:10:46,772] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:46,777] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 13:10:46,890] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:46,900] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 13:10:46,900] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 13:10:46,900] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 13:10:46,901] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO DAGScheduler: Missing parents: List()
[2022-06-21 13:10:46,903] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 13:10:46,954] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 13:10:46,956] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 13:10:46,956] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:34675 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 13:10:46,957] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 13:10:46,965] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 13:10:46,966] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 13:10:46,996] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-21 13:10:47,004] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 13:10:47,006] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655827844679
[2022-06-21 13:10:47,017] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-efda3e45-44aa-41ab-98a4-921fb09b2251/userFiles-d1d9e641-9bdd-4199-9856-71f8cf090ab4/helpers.py
[2022-06-21 13:10:47,218] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 13:10:47,351] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO CodeGenerator: Code generated in 112.015965 ms
[2022-06-21 13:10:47,384] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 13:10:47,390] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 400 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 13:10:47,391] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 13:10:47,395] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,483 s
[2022-06-21 13:10:47,396] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 13:10:47,397] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 13:10:47,398] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,507787 s
[2022-06-21 13:10:47,414] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 13:10:47,417] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 13:10:47,456] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 13:10:47,456] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 13:10:47,456] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 13:10:47,461] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 13:10:47,468] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 13:10:47,468] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:34675 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 13:10:47,469] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:47,469] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 13:10:47,477] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:47,478] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 13:10:47,478] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 13:10:47,478] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 13:10:47,478] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Missing parents: List()
[2022-06-21 13:10:47,478] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 13:10:47,481] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 13:10:47,482] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 13:10:47,483] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:34675 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 13:10:47,483] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 13:10:47,484] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 13:10:47,484] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 13:10:47,485] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-21 13:10:47,485] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 13:10:47,489] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-21 13:10:47,503] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2322 bytes result sent to driver
[2022-06-21 13:10:47,504] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 20 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 13:10:47,504] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 13:10:47,504] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,025 s
[2022-06-21 13:10:47,505] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 13:10:47,505] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 13:10:47,505] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,027902 s
[2022-06-21 13:10:47,581] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 13:10:47,584] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-21 13:10:47,602] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 13:10:47,602] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 13:10:47,602] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 13:10:47,606] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 13:10:47,612] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 13:10:47,613] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:34675 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 13:10:47,613] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:47,614] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197676 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 13:10:47,620] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:47,621] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 13:10:47,621] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 13:10:47,621] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 13:10:47,621] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Missing parents: List()
[2022-06-21 13:10:47,621] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 13:10:47,624] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 13:10:47,625] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 13:10:47,626] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:34675 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 13:10:47,626] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 13:10:47,626] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 13:10:47,627] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 13:10:47,628] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-21 13:10:47,628] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 13:10:47,632] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 13:10:47,639] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4079 bytes result sent to driver
[2022-06-21 13:10:47,640] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 13 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 13:10:47,640] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 13:10:47,641] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,018 s
[2022-06-21 13:10:47,641] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 13:10:47,641] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 13:10:47,641] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,020824 s
[2022-06-21 13:10:47,767] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:34675 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 13:10:47,769] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:34675 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 13:10:47,771] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:34675 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 13:10:47,772] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:34675 in memory (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 13:10:47,907] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 13:10:47,908] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 13:10:47,908] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 13:10:47,949] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 13:10:47,949] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 13:10:47,949] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 13:10:47,989] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO CodeGenerator: Code generated in 13.484846 ms
[2022-06-21 13:10:47,992] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 337.9 KiB, free 365.6 MiB)
[2022-06-21 13:10:47,998] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 13:10:47,998] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:34675 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 13:10:47,999] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:47 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:48,001] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 13:10:48,050] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:48,050] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 13:10:48,050] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 13:10:48,050] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 13:10:48,050] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Missing parents: List()
[2022-06-21 13:10:48,051] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 13:10:48,064] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 217.5 KiB, free 365.3 MiB)
[2022-06-21 13:10:48,065] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 365.3 MiB)
[2022-06-21 13:10:48,066] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:34675 (size: 78.6 KiB, free: 366.2 MiB)
[2022-06-21 13:10:48,066] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
[2022-06-21 13:10:48,066] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 13:10:48,066] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-21 13:10:48,069] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes) taskResourceAssignments Map()
[2022-06-21 13:10:48,069] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-21 13:10:48,101] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 13:10:48,101] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 13:10:48,101] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 13:10:48,131] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO CodeGenerator: Code generated in 8.155103 ms
[2022-06-21 13:10:48,516] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 13:10:48,527] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO CodeGenerator: Code generated in 8.922727 ms
[2022-06-21 13:10:48,536] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO CodeGenerator: Code generated in 13.719532 ms
[2022-06-21 13:10:48,549] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO PythonUDFRunner: Times: total = 405, boot = 368, init = 37, finish = 0
[2022-06-21 13:10:48,554] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileOutputCommitter: Saved output of task 'attempt_202206211310483809904152232739264_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-10/_temporary/0/task_202206211310483809904152232739264_0003_m_000000
[2022-06-21 13:10:48,555] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkHadoopMapRedUtil: attempt_202206211310483809904152232739264_0003_m_000000_3: Committed
[2022-06-21 13:10:48,558] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3225 bytes result sent to driver
[2022-06-21 13:10:48,559] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 492 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 13:10:48,560] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-21 13:10:48,560] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37567
[2022-06-21 13:10:48,561] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,509 s
[2022-06-21 13:10:48,561] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 13:10:48,561] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-21 13:10:48,561] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,511967 s
[2022-06-21 13:10:48,562] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileFormatWriter: Start to commit write Job 9eb9c3a1-cd97-4b5a-ad56-96b5c7bf13d3.
[2022-06-21 13:10:48,569] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileFormatWriter: Write Job 9eb9c3a1-cd97-4b5a-ad56-96b5c7bf13d3 committed. Elapsed time: 6 ms.
[2022-06-21 13:10:48,572] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileFormatWriter: Finished processing stats for write job 9eb9c3a1-cd97-4b5a-ad56-96b5c7bf13d3.
[2022-06-21 13:10:48,664] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(matches)
[2022-06-21 13:10:48,664] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Post-Scan Filters: (size(matches#24, true) > 0),isnotnull(matches#24)
[2022-06-21 13:10:48,665] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Output Data Schema: struct<matches: array<struct<comments:array<struct<minute:string,represent_min:string,second:string,team:string,text:string>>,match_id:string>>>
[2022-06-21 13:10:48,667] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(teams)
[2022-06-21 13:10:48,667] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Post-Scan Filters: (size(teams#94, true) > 0),isnotnull(teams#94)
[2022-06-21 13:10:48,667] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Output Data Schema: struct<teams: array<struct<id:string,name:string>>>
[2022-06-21 13:10:48,696] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 13:10:48,696] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 13:10:48,696] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 13:10:48,753] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO CodeGenerator: Code generated in 13.633458 ms
[2022-06-21 13:10:48,755] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 337.9 KiB, free 364.9 MiB)
[2022-06-21 13:10:48,761] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 364.9 MiB)
[2022-06-21 13:10:48,761] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:34675 (size: 32.5 KiB, free: 366.1 MiB)
[2022-06-21 13:10:48,761] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-21 13:10:48,762] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 13:10:48,778] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-21 13:10:48,779] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
[2022-06-21 13:10:48,779] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
[2022-06-21 13:10:48,779] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 13:10:48,779] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Missing parents: List()
[2022-06-21 13:10:48,779] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
[2022-06-21 13:10:48,781] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 16.4 KiB, free 364.9 MiB)
[2022-06-21 13:10:48,782] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 364.9 MiB)
[2022-06-21 13:10:48,782] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:34675 (size: 7.4 KiB, free: 366.1 MiB)
[2022-06-21 13:10:48,782] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1478
[2022-06-21 13:10:48,783] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
[2022-06-21 13:10:48,783] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-21 13:10:48,783] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-21 13:10:48,784] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-21 13:10:48,791] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-21 13:10:48,802] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO CodeGenerator: Code generated in 9.32472 ms
[2022-06-21 13:10:48,812] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO CodeGenerator: Code generated in 3.685789 ms
[2022-06-21 13:10:48,821] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1776 bytes result sent to driver
[2022-06-21 13:10:48,822] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 39 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 13:10:48,822] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-21 13:10:48,823] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,043 s
[2022-06-21 13:10:48,823] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 13:10:48,823] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-21 13:10:48,823] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,044907 s
[2022-06-21 13:10:48,834] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.0 MiB, free 348.9 MiB)
[2022-06-21 13:10:48,836] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 308.0 B, free 348.9 MiB)
[2022-06-21 13:10:48,836] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.13:34675 (size: 308.0 B, free: 366.1 MiB)
[2022-06-21 13:10:48,837] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-21 13:10:48,845] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Pushed Filters: IsNotNull(matches)
[2022-06-21 13:10:48,845] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Post-Scan Filters: (size(matches#24, true) > 0),isnotnull(matches#24)
[2022-06-21 13:10:48,846] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceStrategy: Output Data Schema: struct<matches: array<struct<comments:array<struct<minute:string,represent_min:string,second:string,team:string,text:string>>,match_id:string>>>
[2022-06-21 13:10:48,916] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO CodeGenerator: Code generated in 41.154757 ms
[2022-06-21 13:10:48,918] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 337.9 KiB, free 348.6 MiB)
[2022-06-21 13:10:48,924] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 348.5 MiB)
[2022-06-21 13:10:48,924] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.13:34675 (size: 32.5 KiB, free: 366.1 MiB)
[2022-06-21 13:10:48,925] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Created broadcast 11 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:48,925] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 13:10:48,941] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 13:10:48,942] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Got job 5 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 13:10:48,942] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 13:10:48,942] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 13:10:48,942] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Missing parents: List()
[2022-06-21 13:10:48,943] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Submitting ResultStage 5 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 13:10:48,958] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 229.3 KiB, free 348.3 MiB)
[2022-06-21 13:10:48,971] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 80.0 KiB, free 348.4 MiB)
[2022-06-21 13:10:48,972] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.13:34675 in memory (size: 78.6 KiB, free: 366.2 MiB)
[2022-06-21 13:10:48,972] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.13:34675 (size: 80.0 KiB, free: 366.1 MiB)
[2022-06-21 13:10:48,972] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1478
[2022-06-21 13:10:48,972] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 13:10:48,972] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2022-06-21 13:10:48,973] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5161 bytes) taskResourceAssignments Map()
[2022-06-21 13:10:48,973] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.0.13:34675 in memory (size: 32.5 KiB, free: 366.1 MiB)
[2022-06-21 13:10:48,974] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2022-06-21 13:10:48,975] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.0.13:34675 in memory (size: 7.4 KiB, free: 366.1 MiB)
[2022-06-21 13:10:48,984] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 13:10:48,984] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 13:10:48,984] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 13:10:48,993] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-21 13:10:49,008] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO CodeGenerator: Code generated in 12.926979 ms
[2022-06-21 13:10:49,033] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO FileOutputCommitter: Saved output of task 'attempt_202206211310486832537558621956714_0005_m_000000_5' to file:/home/lucas/pipeline-data/datalake/silver/who_scored/process_date=2022-04-10/_temporary/0/task_202206211310486832537558621956714_0005_m_000000
[2022-06-21 13:10:49,034] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO SparkHadoopMapRedUtil: attempt_202206211310486832537558621956714_0005_m_000000_5: Committed
[2022-06-21 13:10:49,034] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2782 bytes result sent to driver
[2022-06-21 13:10:49,035] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 62 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 13:10:49,035] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-06-21 13:10:49,036] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 0,092 s
[2022-06-21 13:10:49,036] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 13:10:49,036] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2022-06-21 13:10:49,036] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO DAGScheduler: Job 5 finished: json at NativeMethodAccessorImpl.java:0, took 0,094888 s
[2022-06-21 13:10:49,037] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO FileFormatWriter: Start to commit write Job 6e9cc3a0-f43e-443e-9165-a71d7dcb5996.
[2022-06-21 13:10:49,041] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO FileFormatWriter: Write Job 6e9cc3a0-f43e-443e-9165-a71d7dcb5996 committed. Elapsed time: 4 ms.
[2022-06-21 13:10:49,042] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO FileFormatWriter: Finished processing stats for write job 6e9cc3a0-f43e-443e-9165-a71d7dcb5996.
[2022-06-21 13:10:49,063] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 13:10:49,068] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 13:10:49,073] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 13:10:49,080] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO MemoryStore: MemoryStore cleared
[2022-06-21 13:10:49,080] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO BlockManager: BlockManager stopped
[2022-06-21 13:10:49,083] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 13:10:49,084] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 13:10:49,086] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 13:10:49,086] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 13:10:49,087] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-bfe2025c-0c5d-409d-b443-e0afc551617b
[2022-06-21 13:10:49,088] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-efda3e45-44aa-41ab-98a4-921fb09b2251
[2022-06-21 13:10:49,089] {spark_submit_hook.py:479} INFO - 22/06/21 13:10:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-efda3e45-44aa-41ab-98a4-921fb09b2251/pyspark-a536cf65-8a7d-4a72-955c-7dfdac6c88a5
[2022-06-21 13:10:49,122] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220621T161041, end_date=20220621T161049
[2022-06-21 13:10:51,676] {local_task_job.py:102} INFO - Task exited with return code 0
