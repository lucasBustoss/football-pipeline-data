[2022-06-22 21:55:11,910] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-22 21:55:11,915] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-22 21:55:11,915] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-22 21:55:11,915] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-22 21:55:11,915] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-22 21:55:11,919] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-22 21:55:11,921] {standard_task_runner.py:54} INFO - Started process 22901 to run task
[2022-06-22 21:55:11,953] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '128', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpwfsbsq4t']
[2022-06-22 21:55:11,953] {standard_task_runner.py:78} INFO - Job 128: Subtask transform_football
[2022-06-22 21:55:11,964] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-22 21:55:11,976] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-22 21:55:11,977] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-22 21:55:12,669] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:12 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-22 21:55:12,670] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: An illegal reflective access operation has occurred
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/spark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-06-22 21:55:13,098] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-22 21:55:13,545] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-22 21:55:13,550] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkContext: Running Spark version 3.1.3
[2022-06-22 21:55:13,573] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceUtils: ==============================================================
[2022-06-22 21:55:13,573] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-22 21:55:13,573] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceUtils: ==============================================================
[2022-06-22 21:55:13,573] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkContext: Submitted application: football_transformation
[2022-06-22 21:55:13,590] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-22 21:55:13,599] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceProfile: Limiting resource is cpu
[2022-06-22 21:55:13,599] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-22 21:55:13,625] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: Changing view acls to: lucas
[2022-06-22 21:55:13,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-22 21:55:13,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: Changing view acls groups to:
[2022-06-22 21:55:13,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: Changing modify acls groups to:
[2022-06-22 21:55:13,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-22 21:55:13,743] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO Utils: Successfully started service 'sparkDriver' on port 34035.
[2022-06-22 21:55:13,757] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkEnv: Registering MapOutputTracker
[2022-06-22 21:55:13,774] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-22 21:55:13,786] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-22 21:55:13,786] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-22 21:55:13,788] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-22 21:55:13,796] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6c0e4312-983b-46c2-9277-b598b399855f
[2022-06-22 21:55:13,810] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-06-22 21:55:13,820] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-22 21:55:13,938] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-22 21:55:13,973] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-22 21:55:14,103] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-22 21:55:14,119] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38779.
[2022-06-22 21:55:14,119] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO NettyBlockTransferService: Server created on 192.168.0.13:38779
[2022-06-22 21:55:14,120] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-22 21:55:14,125] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 38779, None)
[2022-06-22 21:55:14,128] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:38779 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.13, 38779, None)
[2022-06-22 21:55:14,129] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 38779, None)
[2022-06-22 21:55:14,130] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 38779, None)
[2022-06-22 21:55:14,341] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655945714341
[2022-06-22 21:55:14,342] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-2f01a28c-e8a4-449d-ac45-c6c747ae1117/userFiles-4c695fe3-4c02-406f-adb7-f30ec8b0adaa/helpers.py
[2022-06-22 21:55:14,420] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/pipeline-data/spark-warehouse').
[2022-06-22 21:55:14,420] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-22 21:55:14,848] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO InMemoryFileIndex: It took 22 ms to list leaf files for 1 paths.
[2022-06-22 21:55:14,888] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-22 21:55:15,820] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:15 INFO FileSourceStrategy: Pushed Filters:
[2022-06-22 21:55:15,821] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:15 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-22 21:55:15,822] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-22 21:55:15,980] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 171.5 KiB, free 434.2 MiB)
[2022-06-22 21:55:16,012] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.2 MiB)
[2022-06-22 21:55:16,014] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.4 MiB)
[2022-06-22 21:55:16,017] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,021] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:16,115] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,123] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:16,123] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:16,124] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:16,124] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:16,127] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:16,175] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)
[2022-06-22 21:55:16,177] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2022-06-22 21:55:16,177] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:38779 (size: 6.3 KiB, free: 434.4 MiB)
[2022-06-22 21:55:16,178] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:16,185] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:16,186] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-22 21:55:16,212] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:16,219] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-22 21:55:16,221] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655945714341
[2022-06-22 21:55:16,237] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-2f01a28c-e8a4-449d-ac45-c6c747ae1117/userFiles-4c695fe3-4c02-406f-adb7-f30ec8b0adaa/helpers.py
[2022-06-22 21:55:16,301] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-22 21:55:16,458] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO CodeGenerator: Code generated in 94.214036 ms
[2022-06-22 21:55:16,482] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2016 bytes result sent to driver
[2022-06-22 21:55:16,488] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 280 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:16,489] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-22 21:55:16,492] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,358 s
[2022-06-22 21:55:16,495] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:16,495] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-22 21:55:16,496] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,381038 s
[2022-06-22 21:55:16,517] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-22 21:55:16,523] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-22 21:55:16,552] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Pushed Filters:
[2022-06-22 21:55:16,552] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-22 21:55:16,552] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-22 21:55:16,556] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 171.5 KiB, free 434.0 MiB)
[2022-06-22 21:55:16,562] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.0 MiB)
[2022-06-22 21:55:16,563] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,564] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,564] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:16,574] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,575] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:16,575] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:16,575] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:16,576] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:16,576] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:16,580] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.0 KiB, free 434.0 MiB)
[2022-06-22 21:55:16,582] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.0 MiB)
[2022-06-22 21:55:16,582] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:38779 (size: 6.3 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,583] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:16,583] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:16,583] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-22 21:55:16,585] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:16,585] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-22 21:55:16,590] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-22 21:55:16,605] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2322 bytes result sent to driver
[2022-06-22 21:55:16,606] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 22 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:16,607] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-22 21:55:16,607] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,030 s
[2022-06-22 21:55:16,607] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:16,607] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-22 21:55:16,608] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,033210 s
[2022-06-22 21:55:16,680] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-22 21:55:16,683] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-22 21:55:16,700] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Pushed Filters:
[2022-06-22 21:55:16,700] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-22 21:55:16,700] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-22 21:55:16,704] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 171.5 KiB, free 433.8 MiB)
[2022-06-22 21:55:16,710] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 433.8 MiB)
[2022-06-22 21:55:16,710] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,711] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,711] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197686 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:16,718] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,719] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:16,719] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:16,719] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:16,719] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:16,720] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:16,722] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.0 KiB, free 433.8 MiB)
[2022-06-22 21:55:16,724] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 433.8 MiB)
[2022-06-22 21:55:16,724] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:38779 (size: 6.3 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,724] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:16,725] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:16,725] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-22 21:55:16,726] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:16,726] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-22 21:55:16,731] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3382, partition values: [empty row]
[2022-06-22 21:55:16,737] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4079 bytes result sent to driver
[2022-06-22 21:55:16,738] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 12 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:16,738] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-22 21:55:16,739] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,019 s
[2022-06-22 21:55:16,739] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:16,739] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-22 21:55:16,739] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,021158 s
[2022-06-22 21:55:16,958] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.13:38779 in memory (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,961] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:38779 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,963] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.13:38779 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,965] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:38779 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2022-06-22 21:55:16,966] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:38779 in memory (size: 24.0 KiB, free: 434.4 MiB)
[2022-06-22 21:55:16,968] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:38779 in memory (size: 24.0 KiB, free: 434.4 MiB)
[2022-06-22 21:55:17,109] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Pushed Filters:
[2022-06-22 21:55:17,109] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-22 21:55:17,109] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-22 21:55:17,145] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-22 21:55:17,146] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-22 21:55:17,176] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 13.705802 ms
[2022-06-22 21:55:17,179] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 171.4 KiB, free 434.2 MiB)
[2022-06-22 21:55:17,184] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.2 MiB)
[2022-06-22 21:55:17,185] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.4 MiB)
[2022-06-22 21:55:17,186] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:17,188] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:17,231] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:17,245] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 158.4 KiB, free 434.1 MiB)
[2022-06-22 21:55:17,247] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 57.2 KiB, free 434.0 MiB)
[2022-06-22 21:55:17,247] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:38779 (size: 57.2 KiB, free: 434.3 MiB)
[2022-06-22 21:55:17,247] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:17,248] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:17,248] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-22 21:55:17,251] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:17,251] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-22 21:55:17,285] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-22 21:55:17,286] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-22 21:55:17,316] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 8.779301 ms
[2022-06-22 21:55:17,609] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-22 21:55:17,624] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 12.314446 ms
[2022-06-22 21:55:17,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 14.177641 ms
[2022-06-22 21:55:17,649] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO PythonUDFRunner: Times: total = 322, boot = 278, init = 44, finish = 0
[2022-06-22 21:55:17,655] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileOutputCommitter: Saved output of task 'attempt_202206222155177761076671649883344_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-10/_temporary/0/task_202206222155177761076671649883344_0003_m_000000
[2022-06-22 21:55:17,656] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkHadoopMapRedUtil: attempt_202206222155177761076671649883344_0003_m_000000_3: Committed
[2022-06-22 21:55:17,658] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3149 bytes result sent to driver
[2022-06-22 21:55:17,660] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 411 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:17,660] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-22 21:55:17,660] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55475
[2022-06-22 21:55:17,661] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,428 s
[2022-06-22 21:55:17,662] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:17,662] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-22 21:55:17,662] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,431377 s
[2022-06-22 21:55:17,670] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileFormatWriter: Write Job 2e1f9e9a-dbf7-4ae9-967d-5dcb2ef9ea82 committed.
[2022-06-22 21:55:17,673] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileFormatWriter: Finished processing stats for write job 2e1f9e9a-dbf7-4ae9-967d-5dcb2ef9ea82.
[2022-06-22 21:55:17,776] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(matches)
[2022-06-22 21:55:17,776] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Post-Scan Filters: (size(matches#24, true) > 0),isnotnull(matches#24)
[2022-06-22 21:55:17,776] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Output Data Schema: struct<matches: array<struct<comments:array<struct<minute:string,represent_min:string,second:string,team:string,text:string>>,match_id:string>>>
[2022-06-22 21:55:17,779] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(teams)
[2022-06-22 21:55:17,779] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Post-Scan Filters: (size(teams#25, true) > 0),isnotnull(teams#25)
[2022-06-22 21:55:17,779] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Output Data Schema: struct<teams: array<struct<id:string,name:string>>>
[2022-06-22 21:55:17,800] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-22 21:55:17,800] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-22 21:55:17,815] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 9.658133 ms
[2022-06-22 21:55:17,829] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 9.891506 ms
[2022-06-22 21:55:17,832] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 171.4 KiB, free 433.8 MiB)
[2022-06-22 21:55:17,837] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 433.8 MiB)
[2022-06-22 21:55:17,838] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:17,838] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2022-06-22 21:55:17,839] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:17,866] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2022-06-22 21:55:17,867] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2022-06-22 21:55:17,867] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2022-06-22 21:55:17,867] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:17,867] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:17,868] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2022-06-22 21:55:17,870] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.8 KiB, free 433.8 MiB)
[2022-06-22 21:55:17,872] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 433.8 MiB)
[2022-06-22 21:55:17,873] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:38779 (size: 9.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:17,873] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:17,874] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:17,874] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-22 21:55:17,875] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:17,875] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-22 21:55:17,896] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 6.937558 ms
[2022-06-22 21:55:17,898] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-22 21:55:17,907] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 8.023273 ms
[2022-06-22 21:55:17,915] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 2.946726 ms
[2022-06-22 21:55:17,923] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2089 bytes result sent to driver
[2022-06-22 21:55:17,924] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 49 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:17,924] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-22 21:55:17,925] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,055 s
[2022-06-22 21:55:17,925] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:17,925] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-22 21:55:17,925] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,059064 s
[2022-06-22 21:55:17,934] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.0 MiB, free 417.8 MiB)
[2022-06-22 21:55:17,937] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 308.0 B, free 417.8 MiB)
[2022-06-22 21:55:17,938] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.13:38779 (size: 308.0 B, free: 434.3 MiB)
[2022-06-22 21:55:17,938] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2022-06-22 21:55:17,954] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 11.011246 ms
[2022-06-22 21:55:17,969] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 11.53853 ms
[2022-06-22 21:55:17,972] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 171.4 KiB, free 417.6 MiB)
[2022-06-22 21:55:17,978] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 417.6 MiB)
[2022-06-22 21:55:17,978] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:17,979] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 11 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:17,979] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:17,993] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:17,994] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Got job 5 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:17,994] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:17,994] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:17,994] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:17,995] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:18,007] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.4 KiB, free 417.4 MiB)
[2022-06-22 21:55:18,009] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 60.1 KiB, free 417.4 MiB)
[2022-06-22 21:55:18,009] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.13:38779 (size: 60.1 KiB, free: 434.2 MiB)
[2022-06-22 21:55:18,010] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:18,010] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:18,010] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2022-06-22 21:55:18,011] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5161 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:18,011] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2022-06-22 21:55:18,021] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-22 21:55:18,021] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-22 21:55:18,038] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO CodeGenerator: Code generated in 9.831363 ms
[2022-06-22 21:55:18,052] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO CodeGenerator: Code generated in 11.216769 ms
[2022-06-22 21:55:18,054] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-22 21:55:18,065] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO CodeGenerator: Code generated in 9.514871 ms
[2022-06-22 21:55:18,088] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_202206222155176511356843478564514_0005_m_000000_5' to file:/home/lucas/pipeline-data/datalake/silver/who_scored/process_date=2022-04-10/_temporary/0/task_202206222155176511356843478564514_0005_m_000000
[2022-06-22 21:55:18,088] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkHadoopMapRedUtil: attempt_202206222155176511356843478564514_0005_m_000000_5: Committed
[2022-06-22 21:55:18,089] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3025 bytes result sent to driver
[2022-06-22 21:55:18,089] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 78 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:18,089] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-06-22 21:55:18,090] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 0,095 s
[2022-06-22 21:55:18,090] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:18,090] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2022-06-22 21:55:18,091] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO DAGScheduler: Job 5 finished: json at NativeMethodAccessorImpl.java:0, took 0,097296 s
[2022-06-22 21:55:18,096] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileFormatWriter: Write Job 55df455e-f401-409f-b740-f358def8659c committed.
[2022-06-22 21:55:18,097] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileFormatWriter: Finished processing stats for write job 55df455e-f401-409f-b740-f358def8659c.
[2022-06-22 21:55:18,131] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-22 21:55:18,132] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.0.13:38779 in memory (size: 9.0 KiB, free: 434.2 MiB)
[2022-06-22 21:55:18,134] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.0.13:38779 in memory (size: 24.0 KiB, free: 434.2 MiB)
[2022-06-22 21:55:18,136] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.0.13:38779 in memory (size: 60.1 KiB, free: 434.3 MiB)
[2022-06-22 21:55:18,137] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-22 21:55:18,138] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.13:38779 in memory (size: 57.2 KiB, free: 434.4 MiB)
[2022-06-22 21:55:18,145] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-22 21:55:18,151] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO MemoryStore: MemoryStore cleared
[2022-06-22 21:55:18,151] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManager: BlockManager stopped
[2022-06-22 21:55:18,153] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-22 21:55:18,155] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-22 21:55:18,157] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkContext: Successfully stopped SparkContext
[2022-06-22 21:55:18,157] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO ShutdownHookManager: Shutdown hook called
[2022-06-22 21:55:18,158] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-82e2a93f-f0cd-48f9-8631-d82865ae4a00
[2022-06-22 21:55:18,159] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f01a28c-e8a4-449d-ac45-c6c747ae1117
[2022-06-22 21:55:18,160] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f01a28c-e8a4-449d-ac45-c6c747ae1117/pyspark-ab07dc76-fe85-4151-838f-fddb74829766
[2022-06-22 21:55:18,228] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220623T005511, end_date=20220623T005518
[2022-06-22 21:55:21,914] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-06-23 10:53:13,927] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-23 10:53:13,933] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-23 10:53:13,933] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-23 10:53:13,933] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-23 10:53:13,933] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-23 10:53:13,937] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-23 10:53:13,939] {standard_task_runner.py:54} INFO - Started process 7914 to run task
[2022-06-23 10:53:13,972] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '135', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpf667c_44']
[2022-06-23 10:53:13,973] {standard_task_runner.py:78} INFO - Job 135: Subtask transform_football
[2022-06-23 10:53:13,985] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-23 10:53:13,996] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-23 10:53:13,997] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-23 10:53:14,948] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:14 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-23 10:53:14,948] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-23 10:53:15,403] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-23 10:53:15,887] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-23 10:53:15,893] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO SparkContext: Running Spark version 3.1.3
[2022-06-23 10:53:15,920] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO ResourceUtils: ==============================================================
[2022-06-23 10:53:15,920] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-23 10:53:15,920] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO ResourceUtils: ==============================================================
[2022-06-23 10:53:15,921] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO SparkContext: Submitted application: football_transformation
[2022-06-23 10:53:15,937] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-23 10:53:15,950] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO ResourceProfile: Limiting resource is cpu
[2022-06-23 10:53:15,950] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-23 10:53:15,985] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO SecurityManager: Changing view acls to: lucas
[2022-06-23 10:53:15,985] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-23 10:53:15,985] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO SecurityManager: Changing view acls groups to:
[2022-06-23 10:53:15,985] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO SecurityManager: Changing modify acls groups to:
[2022-06-23 10:53:15,985] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-23 10:53:16,137] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO Utils: Successfully started service 'sparkDriver' on port 35619.
[2022-06-23 10:53:16,156] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO SparkEnv: Registering MapOutputTracker
[2022-06-23 10:53:16,177] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-23 10:53:16,193] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-23 10:53:16,194] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-23 10:53:16,196] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-23 10:53:16,205] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0b2ffab0-498d-4dd1-8daa-f83755a9cf18
[2022-06-23 10:53:16,221] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-23 10:53:16,232] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-23 10:53:16,384] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-23 10:53:16,418] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-23 10:53:16,574] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-23 10:53:16,590] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34739.
[2022-06-23 10:53:16,590] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO NettyBlockTransferService: Server created on 192.168.0.13:34739
[2022-06-23 10:53:16,591] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-23 10:53:16,596] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 34739, None)
[2022-06-23 10:53:16,598] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:34739 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 34739, None)
[2022-06-23 10:53:16,600] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 34739, None)
[2022-06-23 10:53:16,600] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 34739, None)
[2022-06-23 10:53:16,861] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655992396860
[2022-06-23 10:53:16,861] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-ab3a0ba2-34b4-4c8b-b7e1-4b2aea4f01f1/userFiles-ed1903e6-4499-423d-80e0-4115ae90cad0/helpers.py
[2022-06-23 10:53:16,970] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/pipeline-data/spark-warehouse').
[2022-06-23 10:53:16,970] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:16 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-23 10:53:17,479] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:17 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 1 paths.
[2022-06-23 10:53:17,580] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:17 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-23 10:53:18,631] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 10:53:18,632] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 10:53:18,633] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 10:53:18,825] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.5 KiB, free 366.0 MiB)
[2022-06-23 10:53:18,859] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.0 MiB)
[2022-06-23 10:53:18,861] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:34739 (size: 24.0 KiB, free: 366.3 MiB)
[2022-06-23 10:53:18,864] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:18,868] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 10:53:18,969] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:18,978] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 10:53:18,979] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 10:53:18,979] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 10:53:18,980] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO DAGScheduler: Missing parents: List()
[2022-06-23 10:53:18,983] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 10:53:19,040] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2022-06-23 10:53:19,042] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2022-06-23 10:53:19,042] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:34739 (size: 6.3 KiB, free: 366.3 MiB)
[2022-06-23 10:53:19,043] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2022-06-23 10:53:19,052] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 10:53:19,052] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-23 10:53:19,083] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-23 10:53:19,092] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-23 10:53:19,094] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655992396860
[2022-06-23 10:53:19,113] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-ab3a0ba2-34b4-4c8b-b7e1-4b2aea4f01f1/userFiles-ed1903e6-4499-423d-80e0-4115ae90cad0/helpers.py
[2022-06-23 10:53:19,263] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-23 10:53:19,490] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO CodeGenerator: Code generated in 158.83095 ms
[2022-06-23 10:53:19,516] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-23 10:53:19,522] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 444 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 10:53:19,523] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-23 10:53:19,526] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,533 s
[2022-06-23 10:53:19,528] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 10:53:19,528] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-23 10:53:19,530] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,560836 s
[2022-06-23 10:53:19,547] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 10:53:19,550] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 10:53:19,574] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 10:53:19,574] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 10:53:19,574] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 10:53:19,579] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.5 KiB, free 365.7 MiB)
[2022-06-23 10:53:19,585] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.7 MiB)
[2022-06-23 10:53:19,586] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:34739 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 10:53:19,586] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:19,587] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 10:53:19,596] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:19,597] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 10:53:19,597] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 10:53:19,597] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 10:53:19,597] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Missing parents: List()
[2022-06-23 10:53:19,598] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 10:53:19,601] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.0 KiB, free 365.7 MiB)
[2022-06-23 10:53:19,602] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.7 MiB)
[2022-06-23 10:53:19,603] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:34739 (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 10:53:19,603] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2022-06-23 10:53:19,604] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 10:53:19,604] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-23 10:53:19,605] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-23 10:53:19,605] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-23 10:53:19,610] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 10:53:19,624] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2322 bytes result sent to driver
[2022-06-23 10:53:19,625] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 21 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 10:53:19,625] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-23 10:53:19,626] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,027 s
[2022-06-23 10:53:19,626] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 10:53:19,626] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-23 10:53:19,626] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,030127 s
[2022-06-23 10:53:19,703] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-23 10:53:19,707] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 10:53:19,730] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 10:53:19,730] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 10:53:19,730] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 10:53:19,734] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 285.5 KiB, free 365.4 MiB)
[2022-06-23 10:53:19,740] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.4 MiB)
[2022-06-23 10:53:19,740] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:34739 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 10:53:19,741] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:19,741] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197686 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 10:53:19,747] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:19,748] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 10:53:19,748] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 10:53:19,748] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 10:53:19,748] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Missing parents: List()
[2022-06-23 10:53:19,749] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 10:53:19,752] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.0 KiB, free 365.3 MiB)
[2022-06-23 10:53:19,753] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.3 MiB)
[2022-06-23 10:53:19,753] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:34739 (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 10:53:19,754] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2022-06-23 10:53:19,754] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 10:53:19,754] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-23 10:53:19,755] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-23 10:53:19,755] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-23 10:53:19,759] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3382, partition values: [empty row]
[2022-06-23 10:53:19,765] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4079 bytes result sent to driver
[2022-06-23 10:53:19,766] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 11 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 10:53:19,766] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-23 10:53:19,766] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,016 s
[2022-06-23 10:53:19,767] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 10:53:19,767] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-23 10:53:19,767] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:19 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,019648 s
[2022-06-23 10:53:20,129] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 10:53:20,129] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 10:53:20,129] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-23 10:53:20,169] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 10:53:20,169] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 10:53:20,200] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 14.028932 ms
[2022-06-23 10:53:20,204] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 285.4 KiB, free 365.1 MiB)
[2022-06-23 10:53:20,209] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.0 MiB)
[2022-06-23 10:53:20,210] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:34739 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 10:53:20,210] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:20,212] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 10:53:20,255] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:20,256] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 10:53:20,256] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 10:53:20,256] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 10:53:20,256] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Missing parents: List()
[2022-06-23 10:53:20,256] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 10:53:20,268] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 158.4 KiB, free 364.9 MiB)
[2022-06-23 10:53:20,269] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 57.4 KiB, free 364.8 MiB)
[2022-06-23 10:53:20,270] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:34739 (size: 57.4 KiB, free: 366.1 MiB)
[2022-06-23 10:53:20,270] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1433
[2022-06-23 10:53:20,270] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 10:53:20,270] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-23 10:53:20,272] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes) taskResourceAssignments Map()
[2022-06-23 10:53:20,273] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-23 10:53:20,303] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 10:53:20,303] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 10:53:20,335] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 8.697331 ms
[2022-06-23 10:53:20,637] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-23 10:53:20,649] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 8.977698 ms
[2022-06-23 10:53:20,653] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 11.063685 ms
[2022-06-23 10:53:20,671] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO PythonUDFRunner: Times: total = 322, boot = 286, init = 36, finish = 0
[2022-06-23 10:53:20,678] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileOutputCommitter: Saved output of task 'attempt_202206231053208363884334520464825_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-10/_temporary/0/task_202206231053208363884334520464825_0003_m_000000
[2022-06-23 10:53:20,678] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SparkHadoopMapRedUtil: attempt_202206231053208363884334520464825_0003_m_000000_3: Committed
[2022-06-23 10:53:20,681] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3149 bytes result sent to driver
[2022-06-23 10:53:20,682] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 410 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 10:53:20,682] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-23 10:53:20,682] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39797
[2022-06-23 10:53:20,683] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,426 s
[2022-06-23 10:53:20,683] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 10:53:20,683] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-23 10:53:20,683] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,428604 s
[2022-06-23 10:53:20,690] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileFormatWriter: Write Job 8d9c1571-a4a6-45a3-b52f-ad913ba13d45 committed.
[2022-06-23 10:53:20,692] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileFormatWriter: Finished processing stats for write job 8d9c1571-a4a6-45a3-b52f-ad913ba13d45.
[2022-06-23 10:53:20,811] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(matches)
[2022-06-23 10:53:20,812] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Post-Scan Filters: (size(matches#24, true) > 0),isnotnull(matches#24)
[2022-06-23 10:53:20,812] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Output Data Schema: struct<matches: array<struct<comments:array<struct<minute:string,represent_min:string,second:string,team:string,text:string>>,match_id:string>>>
[2022-06-23 10:53:20,815] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Pushed Filters: IsNotNull(teams)
[2022-06-23 10:53:20,815] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Post-Scan Filters: (size(teams#25, true) > 0),isnotnull(teams#25)
[2022-06-23 10:53:20,815] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceStrategy: Output Data Schema: struct<teams: array<struct<id:string,name:string>>>
[2022-06-23 10:53:20,840] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 10:53:20,840] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 10:53:20,853] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 9.079509 ms
[2022-06-23 10:53:20,869] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 10.285703 ms
[2022-06-23 10:53:20,871] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 285.4 KiB, free 364.5 MiB)
[2022-06-23 10:53:20,876] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 364.5 MiB)
[2022-06-23 10:53:20,877] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:34739 (size: 24.0 KiB, free: 366.1 MiB)
[2022-06-23 10:53:20,877] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 10:53:20,878] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 10:53:20,902] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 10:53:20,903] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
[2022-06-23 10:53:20,903] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
[2022-06-23 10:53:20,903] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 10:53:20,903] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Missing parents: List()
[2022-06-23 10:53:20,904] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
[2022-06-23 10:53:20,905] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.8 KiB, free 364.5 MiB)
[2022-06-23 10:53:20,906] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 364.5 MiB)
[2022-06-23 10:53:20,907] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:34739 (size: 9.0 KiB, free: 366.1 MiB)
[2022-06-23 10:53:20,907] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1433
[2022-06-23 10:53:20,908] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
[2022-06-23 10:53:20,908] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-23 10:53:20,908] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-23 10:53:20,909] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-23 10:53:20,927] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 6.719327 ms
[2022-06-23 10:53:20,928] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 10:53:20,937] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 7.633194 ms
[2022-06-23 10:53:20,946] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 3.081637 ms
[2022-06-23 10:53:20,953] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2089 bytes result sent to driver
[2022-06-23 10:53:20,954] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 46 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 10:53:20,954] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-23 10:53:20,954] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,050 s
[2022-06-23 10:53:20,955] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 10:53:20,955] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-23 10:53:20,955] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,052429 s
[2022-06-23 10:53:20,965] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.0 MiB, free 348.5 MiB)
[2022-06-23 10:53:20,968] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 308.0 B, free 348.5 MiB)
[2022-06-23 10:53:20,968] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.13:34739 (size: 308.0 B, free: 366.1 MiB)
[2022-06-23 10:53:20,968] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 10:53:20,990] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:20 INFO CodeGenerator: Code generated in 14.328856 ms
[2022-06-23 10:53:21,008] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO CodeGenerator: Code generated in 13.145554 ms
[2022-06-23 10:53:21,011] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 285.4 KiB, free 348.2 MiB)
[2022-06-23 10:53:21,017] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 348.2 MiB)
[2022-06-23 10:53:21,018] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.13:34739 (size: 24.0 KiB, free: 366.1 MiB)
[2022-06-23 10:53:21,018] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO SparkContext: Created broadcast 11 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:21,019] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 10:53:21,037] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 10:53:21,038] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: Got job 5 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 10:53:21,038] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 10:53:21,038] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 10:53:21,038] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: Missing parents: List()
[2022-06-23 10:53:21,038] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: Submitting ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 10:53:21,051] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.4 KiB, free 348.0 MiB)
[2022-06-23 10:53:21,052] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 60.1 KiB, free 348.0 MiB)
[2022-06-23 10:53:21,053] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.13:34739 (size: 60.1 KiB, free: 366.0 MiB)
[2022-06-23 10:53:21,053] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1433
[2022-06-23 10:53:21,054] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 10:53:21,054] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2022-06-23 10:53:21,054] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5161 bytes) taskResourceAssignments Map()
[2022-06-23 10:53:21,055] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2022-06-23 10:53:21,081] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 10:53:21,081] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 10:53:21,100] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:34739 in memory (size: 24.0 KiB, free: 366.0 MiB)
[2022-06-23 10:53:21,102] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO CodeGenerator: Code generated in 12.569151 ms
[2022-06-23 10:53:21,103] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:34739 in memory (size: 24.0 KiB, free: 366.1 MiB)
[2022-06-23 10:53:21,105] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.0.13:34739 in memory (size: 24.0 KiB, free: 366.1 MiB)
[2022-06-23 10:53:21,107] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:34739 in memory (size: 6.3 KiB, free: 366.1 MiB)
[2022-06-23 10:53:21,108] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:34739 in memory (size: 6.3 KiB, free: 366.1 MiB)
[2022-06-23 10:53:21,110] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.0.13:34739 in memory (size: 9.0 KiB, free: 366.1 MiB)
[2022-06-23 10:53:21,111] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.13:34739 in memory (size: 57.4 KiB, free: 366.2 MiB)
[2022-06-23 10:53:21,118] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO CodeGenerator: Code generated in 11.932837 ms
[2022-06-23 10:53:21,120] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 10:53:21,129] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO CodeGenerator: Code generated in 8.103064 ms
[2022-06-23 10:53:21,151] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO FileOutputCommitter: Saved output of task 'attempt_202206231053212261540662164161467_0005_m_000000_5' to file:/home/lucas/pipeline-data/datalake/silver/who_scored/process_date=2022-04-10/_temporary/0/task_202206231053212261540662164161467_0005_m_000000
[2022-06-23 10:53:21,151] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO SparkHadoopMapRedUtil: attempt_202206231053212261540662164161467_0005_m_000000_5: Committed
[2022-06-23 10:53:21,152] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3068 bytes result sent to driver
[2022-06-23 10:53:21,152] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 98 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 10:53:21,153] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-06-23 10:53:21,153] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 0,114 s
[2022-06-23 10:53:21,153] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 10:53:21,153] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2022-06-23 10:53:21,153] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO DAGScheduler: Job 5 finished: json at NativeMethodAccessorImpl.java:0, took 0,116417 s
[2022-06-23 10:53:21,160] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO FileFormatWriter: Write Job 611fd30a-3239-43dc-8301-818e8f263224 committed.
[2022-06-23 10:53:21,160] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO FileFormatWriter: Finished processing stats for write job 611fd30a-3239-43dc-8301-818e8f263224.
[2022-06-23 10:53:21,189] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-23 10:53:21,194] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-23 10:53:21,199] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-23 10:53:21,206] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO MemoryStore: MemoryStore cleared
[2022-06-23 10:53:21,206] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManager: BlockManager stopped
[2022-06-23 10:53:21,209] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-23 10:53:21,210] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-23 10:53:21,212] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO SparkContext: Successfully stopped SparkContext
[2022-06-23 10:53:21,212] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO ShutdownHookManager: Shutdown hook called
[2022-06-23 10:53:21,213] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab3a0ba2-34b4-4c8b-b7e1-4b2aea4f01f1/pyspark-26164226-44fa-4dbd-90c3-534de547050e
[2022-06-23 10:53:21,214] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-f9ec8211-3aef-4841-a2ca-7f4dae6786b3
[2022-06-23 10:53:21,215] {spark_submit_hook.py:479} INFO - 22/06/23 10:53:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab3a0ba2-34b4-4c8b-b7e1-4b2aea4f01f1
[2022-06-23 10:53:21,264] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220623T135313, end_date=20220623T135321
[2022-06-23 10:53:23,928] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-06-23 22:26:09,705] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-23 22:26:09,709] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-23 22:26:09,709] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-23 22:26:09,709] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-23 22:26:09,709] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-23 22:26:09,714] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-23 22:26:09,716] {standard_task_runner.py:54} INFO - Started process 68765 to run task
[2022-06-23 22:26:09,749] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '133', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmp0q0vvi1f']
[2022-06-23 22:26:09,750] {standard_task_runner.py:78} INFO - Job 133: Subtask transform_football
[2022-06-23 22:26:09,761] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-23 22:26:09,773] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-23 22:26:09,774] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-23 22:26:10,464] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:10 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-23 22:26:10,464] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-23 22:26:10,918] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-23 22:26:11,448] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-23 22:26:11,456] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SparkContext: Running Spark version 3.1.3
[2022-06-23 22:26:11,488] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO ResourceUtils: ==============================================================
[2022-06-23 22:26:11,489] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-23 22:26:11,489] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO ResourceUtils: ==============================================================
[2022-06-23 22:26:11,489] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SparkContext: Submitted application: football_transformation
[2022-06-23 22:26:11,506] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-23 22:26:11,516] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO ResourceProfile: Limiting resource is cpu
[2022-06-23 22:26:11,516] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-23 22:26:11,549] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SecurityManager: Changing view acls to: lucas
[2022-06-23 22:26:11,549] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-23 22:26:11,550] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SecurityManager: Changing view acls groups to:
[2022-06-23 22:26:11,550] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SecurityManager: Changing modify acls groups to:
[2022-06-23 22:26:11,550] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-23 22:26:11,714] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO Utils: Successfully started service 'sparkDriver' on port 35715.
[2022-06-23 22:26:11,735] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SparkEnv: Registering MapOutputTracker
[2022-06-23 22:26:11,758] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-23 22:26:11,774] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-23 22:26:11,774] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-23 22:26:11,776] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-23 22:26:11,786] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1003aec1-4a49-40f1-a82b-eb744ae6145e
[2022-06-23 22:26:11,802] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-23 22:26:11,814] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-23 22:26:11,990] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-23 22:26:12,032] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-23 22:26:12,194] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-23 22:26:12,210] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42845.
[2022-06-23 22:26:12,211] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO NettyBlockTransferService: Server created on 192.168.0.13:42845
[2022-06-23 22:26:12,212] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-23 22:26:12,217] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 42845, None)
[2022-06-23 22:26:12,219] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:42845 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 42845, None)
[2022-06-23 22:26:12,220] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 42845, None)
[2022-06-23 22:26:12,221] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 42845, None)
[2022-06-23 22:26:12,492] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1656033972492
[2022-06-23 22:26:12,493] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-c890f88c-a721-4352-9ec2-8c9460d98721/userFiles-94105a33-d007-4965-a011-293384ef675a/helpers.py
[2022-06-23 22:26:12,594] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/pipeline-data/spark-warehouse').
[2022-06-23 22:26:12,594] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:12 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-23 22:26:13,103] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:13 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.
[2022-06-23 22:26:13,206] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:13 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-23 22:26:14,327] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 22:26:14,328] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 22:26:14,331] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 22:26:14,523] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.5 KiB, free 366.0 MiB)
[2022-06-23 22:26:14,559] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.0 MiB)
[2022-06-23 22:26:14,561] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:42845 (size: 24.0 KiB, free: 366.3 MiB)
[2022-06-23 22:26:14,564] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:14,569] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 22:26:14,675] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:14,685] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 22:26:14,685] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 22:26:14,685] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 22:26:14,686] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO DAGScheduler: Missing parents: List()
[2022-06-23 22:26:14,689] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 22:26:14,747] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2022-06-23 22:26:14,749] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2022-06-23 22:26:14,749] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:42845 (size: 6.3 KiB, free: 366.3 MiB)
[2022-06-23 22:26:14,750] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2022-06-23 22:26:14,758] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 22:26:14,759] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-23 22:26:14,793] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-23 22:26:14,802] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-23 22:26:14,804] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1656033972492
[2022-06-23 22:26:14,824] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:14 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-c890f88c-a721-4352-9ec2-8c9460d98721/userFiles-94105a33-d007-4965-a011-293384ef675a/helpers.py
[2022-06-23 22:26:15,007] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-23 22:26:15,245] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO CodeGenerator: Code generated in 168.623411 ms
[2022-06-23 22:26:15,271] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-23 22:26:15,277] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 489 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 22:26:15,278] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-23 22:26:15,282] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,584 s
[2022-06-23 22:26:15,284] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 22:26:15,285] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-23 22:26:15,286] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,610825 s
[2022-06-23 22:26:15,305] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 22:26:15,309] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 22:26:15,334] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 22:26:15,334] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 22:26:15,334] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 22:26:15,339] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.5 KiB, free 365.7 MiB)
[2022-06-23 22:26:15,345] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.7 MiB)
[2022-06-23 22:26:15,346] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:42845 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 22:26:15,346] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:15,347] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 22:26:15,357] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:15,358] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 22:26:15,358] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 22:26:15,358] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 22:26:15,358] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Missing parents: List()
[2022-06-23 22:26:15,358] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 22:26:15,361] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.0 KiB, free 365.7 MiB)
[2022-06-23 22:26:15,362] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.7 MiB)
[2022-06-23 22:26:15,363] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:42845 (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 22:26:15,363] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2022-06-23 22:26:15,364] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 22:26:15,364] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-23 22:26:15,364] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-23 22:26:15,365] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-23 22:26:15,369] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 22:26:15,384] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2322 bytes result sent to driver
[2022-06-23 22:26:15,385] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 21 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 22:26:15,385] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-23 22:26:15,386] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,027 s
[2022-06-23 22:26:15,386] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 22:26:15,386] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-23 22:26:15,386] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,029531 s
[2022-06-23 22:26:15,463] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 22:26:15,467] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 22:26:15,486] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 22:26:15,486] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 22:26:15,486] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 22:26:15,490] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 285.5 KiB, free 365.4 MiB)
[2022-06-23 22:26:15,495] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.4 MiB)
[2022-06-23 22:26:15,496] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:42845 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 22:26:15,496] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:15,497] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197686 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 22:26:15,503] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:15,504] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 22:26:15,504] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 22:26:15,504] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 22:26:15,504] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Missing parents: List()
[2022-06-23 22:26:15,505] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 22:26:15,507] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.0 KiB, free 365.3 MiB)
[2022-06-23 22:26:15,508] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.3 MiB)
[2022-06-23 22:26:15,509] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:42845 (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 22:26:15,509] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2022-06-23 22:26:15,509] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 22:26:15,510] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-23 22:26:15,510] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-23 22:26:15,511] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-23 22:26:15,514] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3382, partition values: [empty row]
[2022-06-23 22:26:15,522] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4079 bytes result sent to driver
[2022-06-23 22:26:15,523] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 13 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 22:26:15,523] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-23 22:26:15,524] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,019 s
[2022-06-23 22:26:15,524] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 22:26:15,524] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-23 22:26:15,524] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,021436 s
[2022-06-23 22:26:15,914] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 22:26:15,914] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 22:26:15,915] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-23 22:26:15,957] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 22:26:15,958] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 22:26:15,996] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO CodeGenerator: Code generated in 15.804601 ms
[2022-06-23 22:26:15,999] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 285.4 KiB, free 365.1 MiB)
[2022-06-23 22:26:16,005] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.0 MiB)
[2022-06-23 22:26:16,006] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:42845 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,006] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:16,008] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 22:26:16,060] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:16,061] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 22:26:16,061] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 22:26:16,061] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 22:26:16,061] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Missing parents: List()
[2022-06-23 22:26:16,062] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 22:26:16,073] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 158.4 KiB, free 364.9 MiB)
[2022-06-23 22:26:16,075] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 57.4 KiB, free 364.8 MiB)
[2022-06-23 22:26:16,075] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:42845 (size: 57.4 KiB, free: 366.1 MiB)
[2022-06-23 22:26:16,075] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1433
[2022-06-23 22:26:16,076] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 22:26:16,076] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-23 22:26:16,079] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes) taskResourceAssignments Map()
[2022-06-23 22:26:16,079] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-23 22:26:16,111] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 22:26:16,112] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 22:26:16,145] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 8.688106 ms
[2022-06-23 22:26:16,453] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-23 22:26:16,464] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 9.377108 ms
[2022-06-23 22:26:16,472] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 14.494597 ms
[2022-06-23 22:26:16,494] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO PythonUDFRunner: Times: total = 334, boot = 288, init = 46, finish = 0
[2022-06-23 22:26:16,500] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileOutputCommitter: Saved output of task 'attempt_202206232226163875175384668606916_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-10/_temporary/0/task_202206232226163875175384668606916_0003_m_000000
[2022-06-23 22:26:16,501] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkHadoopMapRedUtil: attempt_202206232226163875175384668606916_0003_m_000000_3: Committed
[2022-06-23 22:26:16,504] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3149 bytes result sent to driver
[2022-06-23 22:26:16,506] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 428 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 22:26:16,506] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-23 22:26:16,507] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 47637
[2022-06-23 22:26:16,508] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,445 s
[2022-06-23 22:26:16,508] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 22:26:16,508] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-23 22:26:16,508] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,448479 s
[2022-06-23 22:26:16,517] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileFormatWriter: Write Job 02cb2766-8646-4136-a50e-ad7167fcd6ab committed.
[2022-06-23 22:26:16,518] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileFormatWriter: Finished processing stats for write job 02cb2766-8646-4136-a50e-ad7167fcd6ab.
[2022-06-23 22:26:16,646] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceStrategy: Pushed Filters: IsNotNull(matches)
[2022-06-23 22:26:16,647] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceStrategy: Post-Scan Filters: (size(matches#24, true) > 0),isnotnull(matches#24)
[2022-06-23 22:26:16,647] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceStrategy: Output Data Schema: struct<matches: array<struct<comments:array<struct<minute:string,represent_min:string,second:string,team:string,text:string>>,match_id:string>>>
[2022-06-23 22:26:16,650] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceStrategy: Pushed Filters: IsNotNull(teams)
[2022-06-23 22:26:16,650] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceStrategy: Post-Scan Filters: (size(teams#25, true) > 0),isnotnull(teams#25)
[2022-06-23 22:26:16,650] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceStrategy: Output Data Schema: struct<teams: array<struct<id:string,name:string>>>
[2022-06-23 22:26:16,655] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:42845 in memory (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,659] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:42845 in memory (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,661] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.13:42845 in memory (size: 57.4 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,663] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:42845 in memory (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,666] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:42845 in memory (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,681] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 22:26:16,681] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 22:26:16,701] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 13.132174 ms
[2022-06-23 22:26:16,720] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 12.716924 ms
[2022-06-23 22:26:16,724] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 285.4 KiB, free 365.4 MiB)
[2022-06-23 22:26:16,731] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.4 MiB)
[2022-06-23 22:26:16,732] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:42845 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,732] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 22:26:16,733] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 22:26:16,761] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 22:26:16,762] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
[2022-06-23 22:26:16,762] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
[2022-06-23 22:26:16,762] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 22:26:16,762] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Missing parents: List()
[2022-06-23 22:26:16,763] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
[2022-06-23 22:26:16,766] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.8 KiB, free 365.4 MiB)
[2022-06-23 22:26:16,767] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.3 MiB)
[2022-06-23 22:26:16,768] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:42845 (size: 9.0 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,768] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1433
[2022-06-23 22:26:16,769] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
[2022-06-23 22:26:16,769] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-23 22:26:16,770] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-23 22:26:16,770] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-23 22:26:16,799] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 13.043748 ms
[2022-06-23 22:26:16,801] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 22:26:16,812] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 9.367199 ms
[2022-06-23 22:26:16,825] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 4.881591 ms
[2022-06-23 22:26:16,836] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2089 bytes result sent to driver
[2022-06-23 22:26:16,839] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 70 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 22:26:16,839] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-23 22:26:16,840] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,076 s
[2022-06-23 22:26:16,840] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 22:26:16,840] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-23 22:26:16,840] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,078952 s
[2022-06-23 22:26:16,853] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.0 MiB, free 349.3 MiB)
[2022-06-23 22:26:16,859] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 308.0 B, free 349.3 MiB)
[2022-06-23 22:26:16,859] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.13:42845 (size: 308.0 B, free: 366.2 MiB)
[2022-06-23 22:26:16,860] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 22:26:16,883] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 13.989617 ms
[2022-06-23 22:26:16,902] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 14.258959 ms
[2022-06-23 22:26:16,905] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 285.4 KiB, free 349.1 MiB)
[2022-06-23 22:26:16,911] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 349.0 MiB)
[2022-06-23 22:26:16,912] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.13:42845 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 22:26:16,913] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Created broadcast 11 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:16,913] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 22:26:16,931] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 22:26:16,932] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Got job 5 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 22:26:16,932] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 22:26:16,932] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 22:26:16,933] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Missing parents: List()
[2022-06-23 22:26:16,935] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Submitting ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 22:26:16,947] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.4 KiB, free 348.9 MiB)
[2022-06-23 22:26:16,950] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 60.1 KiB, free 348.8 MiB)
[2022-06-23 22:26:16,951] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.13:42845 (size: 60.1 KiB, free: 366.1 MiB)
[2022-06-23 22:26:16,952] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1433
[2022-06-23 22:26:16,952] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 22:26:16,952] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2022-06-23 22:26:16,953] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5161 bytes) taskResourceAssignments Map()
[2022-06-23 22:26:16,953] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2022-06-23 22:26:16,964] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 22:26:16,964] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 22:26:16,987] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:16 INFO CodeGenerator: Code generated in 13.136933 ms
[2022-06-23 22:26:17,012] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO CodeGenerator: Code generated in 20.452887 ms
[2022-06-23 22:26:17,015] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 22:26:17,029] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO CodeGenerator: Code generated in 12.644426 ms
[2022-06-23 22:26:17,057] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO FileOutputCommitter: Saved output of task 'attempt_20220623222616889520398638848420_0005_m_000000_5' to file:/home/lucas/pipeline-data/datalake/silver/who_scored/process_date=2022-04-10/_temporary/0/task_20220623222616889520398638848420_0005_m_000000
[2022-06-23 22:26:17,058] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO SparkHadoopMapRedUtil: attempt_20220623222616889520398638848420_0005_m_000000_5: Committed
[2022-06-23 22:26:17,058] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3025 bytes result sent to driver
[2022-06-23 22:26:17,058] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 106 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 22:26:17,058] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-06-23 22:26:17,059] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 0,125 s
[2022-06-23 22:26:17,059] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 22:26:17,059] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2022-06-23 22:26:17,059] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO DAGScheduler: Job 5 finished: json at NativeMethodAccessorImpl.java:0, took 0,127934 s
[2022-06-23 22:26:17,069] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO FileFormatWriter: Write Job 69c06470-19a0-4580-8118-841143b1620a committed.
[2022-06-23 22:26:17,069] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO FileFormatWriter: Finished processing stats for write job 69c06470-19a0-4580-8118-841143b1620a.
[2022-06-23 22:26:17,104] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-23 22:26:17,111] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-23 22:26:17,116] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-23 22:26:17,123] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO MemoryStore: MemoryStore cleared
[2022-06-23 22:26:17,123] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO BlockManager: BlockManager stopped
[2022-06-23 22:26:17,126] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-23 22:26:17,128] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-23 22:26:17,130] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO SparkContext: Successfully stopped SparkContext
[2022-06-23 22:26:17,130] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO ShutdownHookManager: Shutdown hook called
[2022-06-23 22:26:17,130] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-143a3076-f1e3-4cc0-9b41-6488c9aa72e9
[2022-06-23 22:26:17,134] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-c890f88c-a721-4352-9ec2-8c9460d98721/pyspark-97ca1f3e-d17a-4d64-8e88-cb4d2fea7aa5
[2022-06-23 22:26:17,136] {spark_submit_hook.py:479} INFO - 22/06/23 22:26:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-c890f88c-a721-4352-9ec2-8c9460d98721
[2022-06-23 22:26:17,176] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220624T012609, end_date=20220624T012617
[2022-06-23 22:26:19,708] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-06-23 23:06:43,270] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-23 23:06:43,275] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-23 23:06:43,275] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-23 23:06:43,275] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-23 23:06:43,275] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-23 23:06:43,280] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-23 23:06:43,281] {standard_task_runner.py:54} INFO - Started process 84116 to run task
[2022-06-23 23:06:43,314] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '133', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpq2gkc0xk']
[2022-06-23 23:06:43,314] {standard_task_runner.py:78} INFO - Job 133: Subtask transform_football
[2022-06-23 23:06:43,326] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-23 23:06:43,338] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-23 23:06:43,339] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-23 23:06:44,010] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:44 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-23 23:06:44,010] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-23 23:06:44,446] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-23 23:06:44,972] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-23 23:06:44,978] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:44 INFO SparkContext: Running Spark version 3.1.3
[2022-06-23 23:06:45,009] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO ResourceUtils: ==============================================================
[2022-06-23 23:06:45,009] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-23 23:06:45,009] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO ResourceUtils: ==============================================================
[2022-06-23 23:06:45,009] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SparkContext: Submitted application: twitter_transformation
[2022-06-23 23:06:45,025] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-23 23:06:45,036] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO ResourceProfile: Limiting resource is cpu
[2022-06-23 23:06:45,037] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-23 23:06:45,072] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SecurityManager: Changing view acls to: lucas
[2022-06-23 23:06:45,072] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-23 23:06:45,072] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SecurityManager: Changing view acls groups to:
[2022-06-23 23:06:45,072] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SecurityManager: Changing modify acls groups to:
[2022-06-23 23:06:45,072] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-23 23:06:45,199] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO Utils: Successfully started service 'sparkDriver' on port 35417.
[2022-06-23 23:06:45,221] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SparkEnv: Registering MapOutputTracker
[2022-06-23 23:06:45,241] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-23 23:06:45,257] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-23 23:06:45,257] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-23 23:06:45,259] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-23 23:06:45,268] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eedb7cc2-cd1e-47f6-b384-2e18d07d1152
[2022-06-23 23:06:45,283] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-23 23:06:45,293] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-23 23:06:45,432] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-23 23:06:45,480] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-23 23:06:45,620] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-23 23:06:45,635] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34547.
[2022-06-23 23:06:45,635] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO NettyBlockTransferService: Server created on 192.168.0.13:34547
[2022-06-23 23:06:45,636] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-23 23:06:45,641] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 34547, None)
[2022-06-23 23:06:45,643] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:34547 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 34547, None)
[2022-06-23 23:06:45,644] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 34547, None)
[2022-06-23 23:06:45,645] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 34547, None)
[2022-06-23 23:06:46,011] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/pipeline-data/spark-warehouse').
[2022-06-23 23:06:46,011] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:46 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-23 23:06:46,547] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:46 INFO InMemoryFileIndex: It took 19 ms to list leaf files for 1 paths.
[2022-06-23 23:06:46,651] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:46 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-23 23:06:47,739] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:47 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 23:06:47,740] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:47 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 23:06:47,742] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 23:06:47,927] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.5 KiB, free 366.0 MiB)
[2022-06-23 23:06:47,965] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.0 MiB)
[2022-06-23 23:06:47,967] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:34547 (size: 24.0 KiB, free: 366.3 MiB)
[2022-06-23 23:06:47,970] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:47 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:47,975] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:06:48,078] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:48,090] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:06:48,090] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:06:48,090] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:06:48,091] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:06:48,094] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:06:48,158] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2022-06-23 23:06:48,160] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2022-06-23 23:06:48,160] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:34547 (size: 6.3 KiB, free: 366.3 MiB)
[2022-06-23 23:06:48,161] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:06:48,171] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:06:48,172] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-23 23:06:48,211] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-23 23:06:48,221] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-23 23:06:48,394] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-23 23:06:48,656] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO CodeGenerator: Code generated in 178.08555 ms
[2022-06-23 23:06:48,687] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-23 23:06:48,693] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 489 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:06:48,694] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-23 23:06:48,699] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,594 s
[2022-06-23 23:06:48,700] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:06:48,701] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-23 23:06:48,702] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,624640 s
[2022-06-23 23:06:48,719] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 23:06:48,723] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 23:06:48,748] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 23:06:48,748] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 23:06:48,748] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 23:06:48,753] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.5 KiB, free 365.7 MiB)
[2022-06-23 23:06:48,758] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.7 MiB)
[2022-06-23 23:06:48,759] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:34547 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:06:48,760] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:48,760] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:06:48,769] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:48,770] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:06:48,770] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:06:48,770] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:06:48,770] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:06:48,771] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:06:48,775] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.0 KiB, free 365.7 MiB)
[2022-06-23 23:06:48,776] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.7 MiB)
[2022-06-23 23:06:48,777] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:34547 (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 23:06:48,777] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:06:48,778] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:06:48,778] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-23 23:06:48,779] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-23 23:06:48,780] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-23 23:06:48,786] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 23:06:48,802] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2322 bytes result sent to driver
[2022-06-23 23:06:48,804] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 24 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:06:48,804] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-23 23:06:48,804] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,033 s
[2022-06-23 23:06:48,805] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:06:48,805] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-23 23:06:48,805] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,036262 s
[2022-06-23 23:06:48,883] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 23:06:48,887] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 23:06:48,907] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 23:06:48,907] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 23:06:48,907] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 23:06:48,910] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 285.5 KiB, free 365.4 MiB)
[2022-06-23 23:06:48,916] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.4 MiB)
[2022-06-23 23:06:48,917] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:34547 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:06:48,917] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:48,918] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197686 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:06:48,924] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:48,925] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:06:48,925] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:06:48,925] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:06:48,925] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:06:48,926] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:06:48,928] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.0 KiB, free 365.3 MiB)
[2022-06-23 23:06:48,929] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.3 MiB)
[2022-06-23 23:06:48,930] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:34547 (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 23:06:48,930] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:06:48,931] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:06:48,931] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-23 23:06:48,932] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-23 23:06:48,932] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-23 23:06:48,936] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3382, partition values: [empty row]
[2022-06-23 23:06:48,943] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4079 bytes result sent to driver
[2022-06-23 23:06:48,944] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 13 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:06:48,944] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-23 23:06:48,945] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,019 s
[2022-06-23 23:06:48,945] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:06:48,945] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-23 23:06:48,945] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:48 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,021212 s
[2022-06-23 23:06:49,379] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 23:06:49,379] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 23:06:49,379] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-23 23:06:49,422] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:06:49,422] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:06:49,455] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO CodeGenerator: Code generated in 13.498081 ms
[2022-06-23 23:06:49,459] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 285.4 KiB, free 365.1 MiB)
[2022-06-23 23:06:49,466] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.0 MiB)
[2022-06-23 23:06:49,467] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:34547 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:06:49,467] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:49,469] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:06:49,519] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:49,520] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:06:49,520] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:06:49,520] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:06:49,520] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:06:49,521] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:06:49,534] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 158.5 KiB, free 364.9 MiB)
[2022-06-23 23:06:49,535] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 57.4 KiB, free 364.8 MiB)
[2022-06-23 23:06:49,536] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:34547 (size: 57.4 KiB, free: 366.1 MiB)
[2022-06-23 23:06:49,536] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:06:49,537] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:06:49,537] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-23 23:06:49,540] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes) taskResourceAssignments Map()
[2022-06-23 23:06:49,540] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-23 23:06:49,579] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:06:49,580] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:06:49,612] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO CodeGenerator: Code generated in 9.051267 ms
[2022-06-23 23:06:49,912] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-23 23:06:49,924] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO CodeGenerator: Code generated in 9.827553 ms
[2022-06-23 23:06:49,928] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO CodeGenerator: Code generated in 11.942083 ms
[2022-06-23 23:06:49,951] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO PythonUDFRunner: Times: total = 323, boot = 282, init = 41, finish = 0
[2022-06-23 23:06:49,958] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileOutputCommitter: Saved output of task 'attempt_202206232306494395674512381588752_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/extract_date=2022-04-10/odds_portal/process_date=/_temporary/0/task_202206232306494395674512381588752_0003_m_000000
[2022-06-23 23:06:49,958] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO SparkHadoopMapRedUtil: attempt_202206232306494395674512381588752_0003_m_000000_3: Committed
[2022-06-23 23:06:49,962] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3149 bytes result sent to driver
[2022-06-23 23:06:49,963] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 426 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:06:49,963] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-23 23:06:49,964] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 46257
[2022-06-23 23:06:49,965] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,443 s
[2022-06-23 23:06:49,965] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:06:49,965] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-23 23:06:49,966] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,446428 s
[2022-06-23 23:06:49,974] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileFormatWriter: Write Job 6fc43833-84e5-40ff-bb66-2577d5cbb8d6 committed.
[2022-06-23 23:06:49,976] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:49 INFO FileFormatWriter: Finished processing stats for write job 6fc43833-84e5-40ff-bb66-2577d5cbb8d6.
[2022-06-23 23:06:50,035] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:34547 in memory (size: 6.3 KiB, free: 366.1 MiB)
[2022-06-23 23:06:50,039] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:34547 in memory (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:06:50,042] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:34547 in memory (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 23:06:50,045] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:34547 in memory (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:06:50,048] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.13:34547 in memory (size: 57.4 KiB, free: 366.2 MiB)
[2022-06-23 23:06:50,107] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(matches)
[2022-06-23 23:06:50,107] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Post-Scan Filters: (size(matches#24, true) > 0),isnotnull(matches#24)
[2022-06-23 23:06:50,108] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Output Data Schema: struct<matches: array<struct<comments:array<struct<minute:string,represent_min:string,second:string,team:string,text:string>>,match_id:string>>>
[2022-06-23 23:06:50,110] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(teams)
[2022-06-23 23:06:50,110] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Post-Scan Filters: (size(teams#25, true) > 0),isnotnull(teams#25)
[2022-06-23 23:06:50,110] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Output Data Schema: struct<teams: array<struct<id:string,name:string>>>
[2022-06-23 23:06:50,136] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:06:50,137] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:06:50,158] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 14.463347 ms
[2022-06-23 23:06:50,174] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 10.484842 ms
[2022-06-23 23:06:50,177] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 285.4 KiB, free 365.4 MiB)
[2022-06-23 23:06:50,182] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.4 MiB)
[2022-06-23 23:06:50,182] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:34547 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:06:50,183] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 23:06:50,184] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:06:50,210] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 23:06:50,211] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
[2022-06-23 23:06:50,211] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
[2022-06-23 23:06:50,211] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:06:50,211] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:06:50,213] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
[2022-06-23 23:06:50,215] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.8 KiB, free 365.4 MiB)
[2022-06-23 23:06:50,216] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.3 MiB)
[2022-06-23 23:06:50,216] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:34547 (size: 9.0 KiB, free: 366.2 MiB)
[2022-06-23 23:06:50,216] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:06:50,217] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:06:50,217] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-23 23:06:50,218] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-23 23:06:50,218] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-23 23:06:50,245] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 9.919738 ms
[2022-06-23 23:06:50,247] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 23:06:50,259] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 10.106824 ms
[2022-06-23 23:06:50,270] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 3.930873 ms
[2022-06-23 23:06:50,280] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2089 bytes result sent to driver
[2022-06-23 23:06:50,281] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 64 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:06:50,281] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-23 23:06:50,282] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,068 s
[2022-06-23 23:06:50,282] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:06:50,282] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-23 23:06:50,283] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,072253 s
[2022-06-23 23:06:50,297] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.0 MiB, free 349.3 MiB)
[2022-06-23 23:06:50,301] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 308.0 B, free 349.3 MiB)
[2022-06-23 23:06:50,301] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.13:34547 (size: 308.0 B, free: 366.2 MiB)
[2022-06-23 23:06:50,302] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 23:06:50,325] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 15.700058 ms
[2022-06-23 23:06:50,345] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 15.198214 ms
[2022-06-23 23:06:50,349] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 285.4 KiB, free 349.1 MiB)
[2022-06-23 23:06:50,354] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 349.0 MiB)
[2022-06-23 23:06:50,355] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.13:34547 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:06:50,355] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Created broadcast 11 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:50,356] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:06:50,371] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:50,372] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Got job 5 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:06:50,372] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:06:50,372] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:06:50,372] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:06:50,373] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Submitting ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:06:50,386] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.4 KiB, free 348.9 MiB)
[2022-06-23 23:06:50,387] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 60.1 KiB, free 348.8 MiB)
[2022-06-23 23:06:50,388] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.13:34547 (size: 60.1 KiB, free: 366.1 MiB)
[2022-06-23 23:06:50,388] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:06:50,389] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:06:50,389] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2022-06-23 23:06:50,390] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5161 bytes) taskResourceAssignments Map()
[2022-06-23 23:06:50,390] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2022-06-23 23:06:50,401] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:06:50,401] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:06:50,420] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 10.752753 ms
[2022-06-23 23:06:50,436] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 12.541923 ms
[2022-06-23 23:06:50,438] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 23:06:50,451] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 10.590073 ms
[2022-06-23 23:06:50,477] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileOutputCommitter: Saved output of task 'attempt_202206232306507570749106616878381_0005_m_000000_5' to file:/home/lucas/pipeline-data/datalake/silver/extract_date=2022-04-10/who_scored/process_date=/_temporary/0/task_202206232306507570749106616878381_0005_m_000000
[2022-06-23 23:06:50,477] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkHadoopMapRedUtil: attempt_202206232306507570749106616878381_0005_m_000000_5: Committed
[2022-06-23 23:06:50,478] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3025 bytes result sent to driver
[2022-06-23 23:06:50,479] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 90 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:06:50,479] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-06-23 23:06:50,480] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 0,106 s
[2022-06-23 23:06:50,480] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:06:50,480] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2022-06-23 23:06:50,480] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Job 5 finished: json at NativeMethodAccessorImpl.java:0, took 0,108715 s
[2022-06-23 23:06:50,485] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileFormatWriter: Write Job 5f307a5e-0a17-4165-9f50-f399e1c0b583 committed.
[2022-06-23 23:06:50,485] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileFormatWriter: Finished processing stats for write job 5f307a5e-0a17-4165-9f50-f399e1c0b583.
[2022-06-23 23:06:50,510] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(response)
[2022-06-23 23:06:50,510] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Post-Scan Filters: (size(response#39, true) > 0),isnotnull(response#39)
[2022-06-23 23:06:50,511] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceStrategy: Output Data Schema: struct<response: array<struct<fixture:struct<date:string,id:bigint,periods:struct<first:bigint,second:bigint>,referee:string,status:struct<elapsed:bigint,long:string,short:string>,timestamp:bigint,timezone:string,venue:struct<city:string,id:bigint,name:string>>,goals:struct<away:bigint,home:bigint>,league:struct<country:string,flag:string,id:bigint,logo:string,name:string,round:string,season:bigint>,score:struct<extratime:struct<away:string,home:string>,fulltime:struct<away:bigint,home:bigint>,halftime:struct<away:bigint,home:bigint>,penalty:struct<away:string,home:string>>,teams:struct<away:struct<id:bigint,logo:string,name:string,winner:boolean>,home:struct<id:bigint,logo:string,name:string,winner:boolean>>>>>
[2022-06-23 23:06:50,518] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:06:50,518] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:06:50,539] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 12.53931 ms
[2022-06-23 23:06:50,568] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 22.319152 ms
[2022-06-23 23:06:50,571] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 285.4 KiB, free 348.5 MiB)
[2022-06-23 23:06:50,576] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 348.5 MiB)
[2022-06-23 23:06:50,577] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.0.13:34547 (size: 24.0 KiB, free: 366.1 MiB)
[2022-06-23 23:06:50,577] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Created broadcast 13 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:50,578] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197686 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:06:50,591] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:06:50,592] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Got job 6 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:06:50,592] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Final stage: ResultStage 6 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:06:50,592] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:06:50,592] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:06:50,592] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Submitting ResultStage 6 (CoalescedRDD[40] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:06:50,607] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 188.6 KiB, free 348.3 MiB)
[2022-06-23 23:06:50,609] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 64.8 KiB, free 348.3 MiB)
[2022-06-23 23:06:50,609] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.0.13:34547 (size: 64.8 KiB, free: 366.0 MiB)
[2022-06-23 23:06:50,609] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:06:50,610] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (CoalescedRDD[40] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:06:50,610] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2022-06-23 23:06:50,610] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5165 bytes) taskResourceAssignments Map()
[2022-06-23 23:06:50,611] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2022-06-23 23:06:50,622] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:06:50,622] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:06:50,654] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 19.61592 ms
[2022-06-23 23:06:50,655] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3382, partition values: [empty row]
[2022-06-23 23:06:50,678] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO CodeGenerator: Code generated in 17.892795 ms
[2022-06-23 23:06:50,688] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileOutputCommitter: Saved output of task 'attempt_202206232306502748427378691043526_0006_m_000000_6' to file:/home/lucas/pipeline-data/datalake/silver/extract_date=2022-04-10/api_football/process_date=/_temporary/0/task_202206232306502748427378691043526_0006_m_000000
[2022-06-23 23:06:50,688] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkHadoopMapRedUtil: attempt_202206232306502748427378691043526_0006_m_000000_6: Committed
[2022-06-23 23:06:50,689] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2858 bytes result sent to driver
[2022-06-23 23:06:50,693] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 80 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:06:50,693] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2022-06-23 23:06:50,693] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: ResultStage 6 (json at NativeMethodAccessorImpl.java:0) finished in 0,098 s
[2022-06-23 23:06:50,693] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:06:50,693] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2022-06-23 23:06:50,693] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO DAGScheduler: Job 6 finished: json at NativeMethodAccessorImpl.java:0, took 0,101769 s
[2022-06-23 23:06:50,699] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileFormatWriter: Write Job ac0acaa9-1470-4833-8a7f-64027b62b22f committed.
[2022-06-23 23:06:50,700] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO FileFormatWriter: Finished processing stats for write job ac0acaa9-1470-4833-8a7f-64027b62b22f.
[2022-06-23 23:06:50,728] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-23 23:06:50,735] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-23 23:06:50,748] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-23 23:06:50,756] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO MemoryStore: MemoryStore cleared
[2022-06-23 23:06:50,756] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManager: BlockManager stopped
[2022-06-23 23:06:50,760] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-23 23:06:50,762] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-23 23:06:50,765] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO SparkContext: Successfully stopped SparkContext
[2022-06-23 23:06:50,766] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO ShutdownHookManager: Shutdown hook called
[2022-06-23 23:06:50,766] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-82407fa4-254a-4004-8c80-22a4a03398f0/pyspark-6a6af3ab-d91c-401f-9f1a-aef94bb21bb4
[2022-06-23 23:06:50,767] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-82407fa4-254a-4004-8c80-22a4a03398f0
[2022-06-23 23:06:50,769] {spark_submit_hook.py:479} INFO - 22/06/23 23:06:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-6816b275-3373-4e99-af7b-c64e37824018
[2022-06-23 23:06:50,814] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220624T020643, end_date=20220624T020650
[2022-06-23 23:06:53,272] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-06-23 23:11:43,022] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-23 23:11:43,027] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-23 23:11:43,027] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-23 23:11:43,027] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-23 23:11:43,027] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-23 23:11:43,031] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-23 23:11:43,033] {standard_task_runner.py:54} INFO - Started process 86205 to run task
[2022-06-23 23:11:43,078] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '133', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpaxmh5b5i']
[2022-06-23 23:11:43,078] {standard_task_runner.py:78} INFO - Job 133: Subtask transform_football
[2022-06-23 23:11:43,093] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-23 23:11:43,109] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-23 23:11:43,110] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-23 23:11:43,805] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:43 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-23 23:11:43,805] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-23 23:11:44,278] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-23 23:11:44,801] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-23 23:11:44,806] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO SparkContext: Running Spark version 3.1.3
[2022-06-23 23:11:44,833] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO ResourceUtils: ==============================================================
[2022-06-23 23:11:44,833] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-23 23:11:44,834] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO ResourceUtils: ==============================================================
[2022-06-23 23:11:44,834] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO SparkContext: Submitted application: football_transformation
[2022-06-23 23:11:44,848] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-23 23:11:44,857] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO ResourceProfile: Limiting resource is cpu
[2022-06-23 23:11:44,857] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-23 23:11:44,889] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO SecurityManager: Changing view acls to: lucas
[2022-06-23 23:11:44,889] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-23 23:11:44,889] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO SecurityManager: Changing view acls groups to:
[2022-06-23 23:11:44,889] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO SecurityManager: Changing modify acls groups to:
[2022-06-23 23:11:44,890] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-23 23:11:45,024] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO Utils: Successfully started service 'sparkDriver' on port 45337.
[2022-06-23 23:11:45,047] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO SparkEnv: Registering MapOutputTracker
[2022-06-23 23:11:45,068] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-23 23:11:45,083] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-23 23:11:45,084] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-23 23:11:45,086] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-23 23:11:45,095] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b37263ac-a4ee-49bb-a17d-bf561c8c5bfc
[2022-06-23 23:11:45,110] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-23 23:11:45,122] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-23 23:11:45,271] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-23 23:11:45,307] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-23 23:11:45,433] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-23 23:11:45,448] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36399.
[2022-06-23 23:11:45,449] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO NettyBlockTransferService: Server created on 192.168.0.13:36399
[2022-06-23 23:11:45,450] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-23 23:11:45,455] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 36399, None)
[2022-06-23 23:11:45,458] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:36399 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 36399, None)
[2022-06-23 23:11:45,460] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 36399, None)
[2022-06-23 23:11:45,461] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 36399, None)
[2022-06-23 23:11:45,709] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1656036705709
[2022-06-23 23:11:45,710] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-16d34d20-1869-44de-be70-f2b851e20bfe/userFiles-ed35933f-1725-4c6c-8819-ee152b6b4f86/helpers.py
[2022-06-23 23:11:45,816] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/pipeline-data/spark-warehouse').
[2022-06-23 23:11:45,816] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:45 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-23 23:11:46,316] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:46 INFO InMemoryFileIndex: It took 20 ms to list leaf files for 1 paths.
[2022-06-23 23:11:46,423] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:46 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-23 23:11:47,584] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 23:11:47,585] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 23:11:47,588] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 23:11:47,770] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.5 KiB, free 366.0 MiB)
[2022-06-23 23:11:47,806] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 366.0 MiB)
[2022-06-23 23:11:47,808] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:36399 (size: 24.0 KiB, free: 366.3 MiB)
[2022-06-23 23:11:47,811] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:47,817] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:11:47,940] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:47,952] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:11:47,952] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:11:47,952] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:11:47,954] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:11:47,957] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:11:48,024] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 366.0 MiB)
[2022-06-23 23:11:48,027] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)
[2022-06-23 23:11:48,027] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:36399 (size: 6.3 KiB, free: 366.3 MiB)
[2022-06-23 23:11:48,028] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:11:48,037] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:11:48,051] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-23 23:11:48,092] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-23 23:11:48,102] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-23 23:11:48,103] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1656036705709
[2022-06-23 23:11:48,123] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-16d34d20-1869-44de-be70-f2b851e20bfe/userFiles-ed35933f-1725-4c6c-8819-ee152b6b4f86/helpers.py
[2022-06-23 23:11:48,297] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-23 23:11:48,534] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO CodeGenerator: Code generated in 168.28745 ms
[2022-06-23 23:11:48,560] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-23 23:11:48,566] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 481 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:11:48,567] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-23 23:11:48,572] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,603 s
[2022-06-23 23:11:48,574] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:11:48,574] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-23 23:11:48,575] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,634644 s
[2022-06-23 23:11:48,593] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-23 23:11:48,597] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 23:11:48,624] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 23:11:48,624] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 23:11:48,624] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 23:11:48,629] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 285.5 KiB, free 365.7 MiB)
[2022-06-23 23:11:48,635] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.7 MiB)
[2022-06-23 23:11:48,636] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:36399 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:11:48,637] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:48,637] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:11:48,646] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:48,648] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:11:48,648] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:11:48,648] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:11:48,648] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:11:48,649] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:11:48,652] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.0 KiB, free 365.7 MiB)
[2022-06-23 23:11:48,653] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.7 MiB)
[2022-06-23 23:11:48,653] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:36399 (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 23:11:48,653] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:11:48,654] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:11:48,654] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-23 23:11:48,655] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-23 23:11:48,655] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-23 23:11:48,662] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 23:11:48,679] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2322 bytes result sent to driver
[2022-06-23 23:11:48,680] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 25 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:11:48,680] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-23 23:11:48,680] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,030 s
[2022-06-23 23:11:48,681] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:11:48,681] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-23 23:11:48,681] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,034180 s
[2022-06-23 23:11:48,759] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-23 23:11:48,764] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-23 23:11:48,784] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 23:11:48,785] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 23:11:48,785] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-23 23:11:48,790] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 285.5 KiB, free 365.4 MiB)
[2022-06-23 23:11:48,798] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.4 MiB)
[2022-06-23 23:11:48,798] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:36399 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:11:48,799] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:48,800] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197686 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:11:48,808] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:48,809] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:11:48,809] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:11:48,809] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:11:48,809] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:11:48,810] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:11:48,813] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.0 KiB, free 365.3 MiB)
[2022-06-23 23:11:48,815] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.3 MiB)
[2022-06-23 23:11:48,815] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:36399 (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 23:11:48,816] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:11:48,817] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:11:48,817] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-23 23:11:48,818] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-23 23:11:48,818] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-23 23:11:48,823] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3382, partition values: [empty row]
[2022-06-23 23:11:48,831] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4079 bytes result sent to driver
[2022-06-23 23:11:48,833] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 16 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:11:48,833] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-23 23:11:48,834] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,023 s
[2022-06-23 23:11:48,834] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:11:48,834] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-23 23:11:48,835] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:48 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,026236 s
[2022-06-23 23:11:49,283] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileSourceStrategy: Pushed Filters:
[2022-06-23 23:11:49,283] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-23 23:11:49,283] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-23 23:11:49,323] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:11:49,323] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:11:49,358] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO CodeGenerator: Code generated in 16.862268 ms
[2022-06-23 23:11:49,362] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 285.4 KiB, free 365.1 MiB)
[2022-06-23 23:11:49,369] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.0 MiB)
[2022-06-23 23:11:49,369] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:36399 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:11:49,370] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:49,372] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:11:49,429] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:49,430] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:11:49,430] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:11:49,430] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:11:49,430] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:11:49,431] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:11:49,447] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 158.4 KiB, free 364.9 MiB)
[2022-06-23 23:11:49,449] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 57.4 KiB, free 364.8 MiB)
[2022-06-23 23:11:49,449] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:36399 (size: 57.4 KiB, free: 366.1 MiB)
[2022-06-23 23:11:49,450] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:11:49,450] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:11:49,450] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-23 23:11:49,453] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes) taskResourceAssignments Map()
[2022-06-23 23:11:49,453] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-23 23:11:49,491] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:11:49,491] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:11:49,528] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO CodeGenerator: Code generated in 12.109133 ms
[2022-06-23 23:11:49,858] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-23 23:11:49,874] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO CodeGenerator: Code generated in 12.717829 ms
[2022-06-23 23:11:49,877] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO CodeGenerator: Code generated in 12.753737 ms
[2022-06-23 23:11:49,915] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO PythonUDFRunner: Times: total = 371, boot = 311, init = 60, finish = 0
[2022-06-23 23:11:49,922] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileOutputCommitter: Saved output of task 'attempt_202206232311492952809044143560709_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-10/_temporary/0/task_202206232311492952809044143560709_0003_m_000000
[2022-06-23 23:11:49,922] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO SparkHadoopMapRedUtil: attempt_202206232311492952809044143560709_0003_m_000000_3: Committed
[2022-06-23 23:11:49,924] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:36399 in memory (size: 6.3 KiB, free: 366.1 MiB)
[2022-06-23 23:11:49,926] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3192 bytes result sent to driver
[2022-06-23 23:11:49,927] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:36399 in memory (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:11:49,927] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 476 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:11:49,927] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-23 23:11:49,928] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 34027
[2022-06-23 23:11:49,929] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,498 s
[2022-06-23 23:11:49,929] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:11:49,929] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-23 23:11:49,930] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,500567 s
[2022-06-23 23:11:49,931] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:36399 in memory (size: 6.3 KiB, free: 366.2 MiB)
[2022-06-23 23:11:49,934] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:36399 in memory (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:11:49,942] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileFormatWriter: Write Job 8d715867-781a-4c47-b1f0-1a131329d52a committed.
[2022-06-23 23:11:49,944] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:49 INFO FileFormatWriter: Finished processing stats for write job 8d715867-781a-4c47-b1f0-1a131329d52a.
[2022-06-23 23:11:50,059] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(matches)
[2022-06-23 23:11:50,060] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Post-Scan Filters: (size(matches#24, true) > 0),isnotnull(matches#24)
[2022-06-23 23:11:50,060] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Output Data Schema: struct<matches: array<struct<comments:array<struct<minute:string,represent_min:string,second:string,team:string,text:string>>,match_id:string>>>
[2022-06-23 23:11:50,063] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(teams)
[2022-06-23 23:11:50,063] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Post-Scan Filters: (size(teams#25, true) > 0),isnotnull(teams#25)
[2022-06-23 23:11:50,063] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Output Data Schema: struct<teams: array<struct<id:string,name:string>>>
[2022-06-23 23:11:50,091] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:11:50,091] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:11:50,110] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 13.014192 ms
[2022-06-23 23:11:50,130] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 13.953659 ms
[2022-06-23 23:11:50,133] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 285.4 KiB, free 365.2 MiB)
[2022-06-23 23:11:50,141] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 365.2 MiB)
[2022-06-23 23:11:50,143] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:36399 (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:11:50,143] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 23:11:50,144] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:11:50,182] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 23:11:50,183] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
[2022-06-23 23:11:50,183] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
[2022-06-23 23:11:50,183] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:11:50,183] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:11:50,184] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
[2022-06-23 23:11:50,186] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.8 KiB, free 365.1 MiB)
[2022-06-23 23:11:50,188] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.1 MiB)
[2022-06-23 23:11:50,188] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:36399 (size: 9.0 KiB, free: 366.2 MiB)
[2022-06-23 23:11:50,189] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:11:50,189] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:11:50,189] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-23 23:11:50,190] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-23 23:11:50,190] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-23 23:11:50,213] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 9.084972 ms
[2022-06-23 23:11:50,215] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 23:11:50,227] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 10.504201 ms
[2022-06-23 23:11:50,240] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 4.231716 ms
[2022-06-23 23:11:50,249] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2089 bytes result sent to driver
[2022-06-23 23:11:50,250] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 59 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:11:50,250] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-23 23:11:50,250] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0,065 s
[2022-06-23 23:11:50,250] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:11:50,250] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-23 23:11:50,251] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0,068358 s
[2022-06-23 23:11:50,263] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.0 MiB, free 349.1 MiB)
[2022-06-23 23:11:50,266] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 308.0 B, free 349.1 MiB)
[2022-06-23 23:11:50,267] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.13:36399 (size: 308.0 B, free: 366.2 MiB)
[2022-06-23 23:11:50,267] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
[2022-06-23 23:11:50,287] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 14.186411 ms
[2022-06-23 23:11:50,305] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 13.837969 ms
[2022-06-23 23:11:50,309] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 285.4 KiB, free 348.9 MiB)
[2022-06-23 23:11:50,316] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 348.8 MiB)
[2022-06-23 23:11:50,317] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.13:36399 (size: 24.0 KiB, free: 366.1 MiB)
[2022-06-23 23:11:50,317] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Created broadcast 11 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:50,318] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:11:50,337] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:50,338] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Got job 5 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:11:50,338] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:11:50,338] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:11:50,338] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:11:50,339] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Submitting ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:11:50,351] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.4 KiB, free 348.7 MiB)
[2022-06-23 23:11:50,353] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 60.1 KiB, free 348.6 MiB)
[2022-06-23 23:11:50,353] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.13:36399 (size: 60.1 KiB, free: 366.1 MiB)
[2022-06-23 23:11:50,354] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:11:50,354] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:11:50,354] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2022-06-23 23:11:50,355] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5161 bytes) taskResourceAssignments Map()
[2022-06-23 23:11:50,355] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2022-06-23 23:11:50,365] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:11:50,365] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:11:50,383] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 9.933265 ms
[2022-06-23 23:11:50,398] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 11.147984 ms
[2022-06-23 23:11:50,399] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-23 23:11:50,411] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 9.929969 ms
[2022-06-23 23:11:50,439] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileOutputCommitter: Saved output of task 'attempt_202206232311502194898199347439194_0005_m_000000_5' to file:/home/lucas/pipeline-data/datalake/silver/who_scored/process_date=2022-04-10/_temporary/0/task_202206232311502194898199347439194_0005_m_000000
[2022-06-23 23:11:50,439] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkHadoopMapRedUtil: attempt_202206232311502194898199347439194_0005_m_000000_5: Committed
[2022-06-23 23:11:50,440] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3025 bytes result sent to driver
[2022-06-23 23:11:50,441] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 86 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:11:50,441] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-06-23 23:11:50,442] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 0,102 s
[2022-06-23 23:11:50,442] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:11:50,442] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2022-06-23 23:11:50,442] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Job 5 finished: json at NativeMethodAccessorImpl.java:0, took 0,104733 s
[2022-06-23 23:11:50,448] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileFormatWriter: Write Job 63b6c11b-78ca-466f-becb-8ce5e83dd914 committed.
[2022-06-23 23:11:50,449] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileFormatWriter: Finished processing stats for write job 63b6c11b-78ca-466f-becb-8ce5e83dd914.
[2022-06-23 23:11:50,477] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(response)
[2022-06-23 23:11:50,477] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Post-Scan Filters: (size(response#39, true) > 0),isnotnull(response#39)
[2022-06-23 23:11:50,477] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceStrategy: Output Data Schema: struct<response: array<struct<fixture:struct<date:string,id:bigint,periods:struct<first:bigint,second:bigint>,referee:string,status:struct<elapsed:bigint,long:string,short:string>,timestamp:bigint,timezone:string,venue:struct<city:string,id:bigint,name:string>>,goals:struct<away:bigint,home:bigint>,league:struct<country:string,flag:string,id:bigint,logo:string,name:string,round:string,season:bigint>,score:struct<extratime:struct<away:string,home:string>,fulltime:struct<away:bigint,home:bigint>,halftime:struct<away:bigint,home:bigint>,penalty:struct<away:string,home:string>>,teams:struct<away:struct<id:bigint,logo:string,name:string,winner:boolean>,home:struct<id:bigint,logo:string,name:string,winner:boolean>>>>>
[2022-06-23 23:11:50,486] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:11:50,486] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:11:50,506] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 10.510192 ms
[2022-06-23 23:11:50,538] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 24.447789 ms
[2022-06-23 23:11:50,541] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 285.4 KiB, free 348.3 MiB)
[2022-06-23 23:11:50,548] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 348.3 MiB)
[2022-06-23 23:11:50,549] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.0.13:36399 (size: 24.0 KiB, free: 366.1 MiB)
[2022-06-23 23:11:50,549] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Created broadcast 13 from json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:50,550] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197686 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-23 23:11:50,567] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-23 23:11:50,568] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Got job 6 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-23 23:11:50,568] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Final stage: ResultStage 6 (json at NativeMethodAccessorImpl.java:0)
[2022-06-23 23:11:50,568] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Parents of final stage: List()
[2022-06-23 23:11:50,568] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Missing parents: List()
[2022-06-23 23:11:50,568] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Submitting ResultStage 6 (CoalescedRDD[40] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-23 23:11:50,581] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 188.6 KiB, free 348.1 MiB)
[2022-06-23 23:11:50,583] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 64.8 KiB, free 348.1 MiB)
[2022-06-23 23:11:50,583] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.0.13:36399 (size: 64.8 KiB, free: 366.0 MiB)
[2022-06-23 23:11:50,583] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1433
[2022-06-23 23:11:50,584] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (CoalescedRDD[40] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-23 23:11:50,584] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2022-06-23 23:11:50,585] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5165 bytes) taskResourceAssignments Map()
[2022-06-23 23:11:50,585] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2022-06-23 23:11:50,597] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-23 23:11:50,597] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-23 23:11:50,625] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 15.203514 ms
[2022-06-23 23:11:50,627] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3382, partition values: [empty row]
[2022-06-23 23:11:50,646] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO CodeGenerator: Code generated in 15.300359 ms
[2022-06-23 23:11:50,656] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileOutputCommitter: Saved output of task 'attempt_202206232311508908859362513149117_0006_m_000000_6' to file:/home/lucas/pipeline-data/datalake/silver/api_football/process_date=2022-04-10/_temporary/0/task_202206232311508908859362513149117_0006_m_000000
[2022-06-23 23:11:50,656] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkHadoopMapRedUtil: attempt_202206232311508908859362513149117_0006_m_000000_6: Committed
[2022-06-23 23:11:50,657] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2858 bytes result sent to driver
[2022-06-23 23:11:50,657] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 72 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-23 23:11:50,657] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2022-06-23 23:11:50,658] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: ResultStage 6 (json at NativeMethodAccessorImpl.java:0) finished in 0,089 s
[2022-06-23 23:11:50,658] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-23 23:11:50,658] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2022-06-23 23:11:50,658] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO DAGScheduler: Job 6 finished: json at NativeMethodAccessorImpl.java:0, took 0,091074 s
[2022-06-23 23:11:50,664] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileFormatWriter: Write Job aa53603a-134c-4f07-a97b-920d25d7e4ee committed.
[2022-06-23 23:11:50,664] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO FileFormatWriter: Finished processing stats for write job aa53603a-134c-4f07-a97b-920d25d7e4ee.
[2022-06-23 23:11:50,711] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-23 23:11:50,712] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.0.13:36399 in memory (size: 9.0 KiB, free: 366.0 MiB)
[2022-06-23 23:11:50,715] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.0.13:36399 in memory (size: 60.1 KiB, free: 366.1 MiB)
[2022-06-23 23:11:50,718] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.13:36399 in memory (size: 57.4 KiB, free: 366.1 MiB)
[2022-06-23 23:11:50,720] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.0.13:36399 in memory (size: 24.0 KiB, free: 366.1 MiB)
[2022-06-23 23:11:50,720] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-23 23:11:50,722] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.0.13:36399 in memory (size: 24.0 KiB, free: 366.2 MiB)
[2022-06-23 23:11:50,728] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-23 23:11:50,735] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO MemoryStore: MemoryStore cleared
[2022-06-23 23:11:50,736] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManager: BlockManager stopped
[2022-06-23 23:11:50,738] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-23 23:11:50,740] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-23 23:11:50,745] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO SparkContext: Successfully stopped SparkContext
[2022-06-23 23:11:50,745] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO ShutdownHookManager: Shutdown hook called
[2022-06-23 23:11:50,745] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-16d34d20-1869-44de-be70-f2b851e20bfe/pyspark-3275c949-b4a7-4dc6-989a-8f2c9110f676
[2022-06-23 23:11:50,747] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d7dd029-a662-44d4-893a-57df74ebdcdc
[2022-06-23 23:11:50,748] {spark_submit_hook.py:479} INFO - 22/06/23 23:11:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-16d34d20-1869-44de-be70-f2b851e20bfe
[2022-06-23 23:11:50,800] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220624T021143, end_date=20220624T021150
[2022-06-23 23:11:53,020] {local_task_job.py:102} INFO - Task exited with return code 0
