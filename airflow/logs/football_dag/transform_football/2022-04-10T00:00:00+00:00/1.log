[2022-06-22 21:55:11,910] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-22 21:55:11,915] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [queued]>
[2022-06-22 21:55:11,915] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-22 21:55:11,915] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-22 21:55:11,915] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-22 21:55:11,919] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-10T00:00:00+00:00
[2022-06-22 21:55:11,921] {standard_task_runner.py:54} INFO - Started process 22901 to run task
[2022-06-22 21:55:11,953] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-10T00:00:00+00:00', '--job_id', '128', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmpwfsbsq4t']
[2022-06-22 21:55:11,953] {standard_task_runner.py:78} INFO - Job 128: Subtask transform_football
[2022-06-22 21:55:11,964] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-10T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-22 21:55:11,976] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-22 21:55:11,977] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10 --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-10
[2022-06-22 21:55:12,669] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:12 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-22 21:55:12,670] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: An illegal reflective access operation has occurred
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/lucas/spark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-06-22 21:55:12,805] {spark_submit_hook.py:479} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-06-22 21:55:13,098] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-22 21:55:13,545] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-22 21:55:13,550] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkContext: Running Spark version 3.1.3
[2022-06-22 21:55:13,573] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceUtils: ==============================================================
[2022-06-22 21:55:13,573] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-22 21:55:13,573] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceUtils: ==============================================================
[2022-06-22 21:55:13,573] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkContext: Submitted application: football_transformation
[2022-06-22 21:55:13,590] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-22 21:55:13,599] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceProfile: Limiting resource is cpu
[2022-06-22 21:55:13,599] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-22 21:55:13,625] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: Changing view acls to: lucas
[2022-06-22 21:55:13,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-22 21:55:13,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: Changing view acls groups to:
[2022-06-22 21:55:13,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: Changing modify acls groups to:
[2022-06-22 21:55:13,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-22 21:55:13,743] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO Utils: Successfully started service 'sparkDriver' on port 34035.
[2022-06-22 21:55:13,757] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkEnv: Registering MapOutputTracker
[2022-06-22 21:55:13,774] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-22 21:55:13,786] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-22 21:55:13,786] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-22 21:55:13,788] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-22 21:55:13,796] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6c0e4312-983b-46c2-9277-b598b399855f
[2022-06-22 21:55:13,810] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-06-22 21:55:13,820] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-22 21:55:13,938] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-22 21:55:13,973] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-22 21:55:14,103] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-22 21:55:14,119] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38779.
[2022-06-22 21:55:14,119] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO NettyBlockTransferService: Server created on 192.168.0.13:38779
[2022-06-22 21:55:14,120] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-22 21:55:14,125] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 38779, None)
[2022-06-22 21:55:14,128] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:38779 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.0.13, 38779, None)
[2022-06-22 21:55:14,129] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 38779, None)
[2022-06-22 21:55:14,130] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 38779, None)
[2022-06-22 21:55:14,341] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655945714341
[2022-06-22 21:55:14,342] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-2f01a28c-e8a4-449d-ac45-c6c747ae1117/userFiles-4c695fe3-4c02-406f-adb7-f30ec8b0adaa/helpers.py
[2022-06-22 21:55:14,420] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/lucas/pipeline-data/spark-warehouse').
[2022-06-22 21:55:14,420] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-22 21:55:14,848] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO InMemoryFileIndex: It took 22 ms to list leaf files for 1 paths.
[2022-06-22 21:55:14,888] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-22 21:55:15,820] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:15 INFO FileSourceStrategy: Pushed Filters:
[2022-06-22 21:55:15,821] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:15 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-22 21:55:15,822] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-22 21:55:15,980] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 171.5 KiB, free 434.2 MiB)
[2022-06-22 21:55:16,012] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.2 MiB)
[2022-06-22 21:55:16,014] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.4 MiB)
[2022-06-22 21:55:16,017] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,021] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:16,115] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,123] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:16,123] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:16,124] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:16,124] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:16,127] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:16,175] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)
[2022-06-22 21:55:16,177] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.2 MiB)
[2022-06-22 21:55:16,177] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:38779 (size: 6.3 KiB, free: 434.4 MiB)
[2022-06-22 21:55:16,178] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:16,185] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:16,186] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-22 21:55:16,212] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4934 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:16,219] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-22 21:55:16,221] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655945714341
[2022-06-22 21:55:16,237] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-2f01a28c-e8a4-449d-ac45-c6c747ae1117/userFiles-4c695fe3-4c02-406f-adb7-f30ec8b0adaa/helpers.py
[2022-06-22 21:55:16,301] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-22 21:55:16,458] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO CodeGenerator: Code generated in 94.214036 ms
[2022-06-22 21:55:16,482] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2016 bytes result sent to driver
[2022-06-22 21:55:16,488] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 280 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:16,489] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-22 21:55:16,492] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,358 s
[2022-06-22 21:55:16,495] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:16,495] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-22 21:55:16,496] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,381038 s
[2022-06-22 21:55:16,517] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-22 21:55:16,523] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-22 21:55:16,552] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Pushed Filters:
[2022-06-22 21:55:16,552] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-22 21:55:16,552] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-22 21:55:16,556] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 171.5 KiB, free 434.0 MiB)
[2022-06-22 21:55:16,562] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.0 MiB)
[2022-06-22 21:55:16,563] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,564] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,564] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:16,574] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,575] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:16,575] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:16,575] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:16,576] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:16,576] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:16,580] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.0 KiB, free 434.0 MiB)
[2022-06-22 21:55:16,582] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.0 MiB)
[2022-06-22 21:55:16,582] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:38779 (size: 6.3 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,583] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:16,583] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:16,583] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-22 21:55:16,585] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:16,585] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-22 21:55:16,590] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-22 21:55:16,605] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2322 bytes result sent to driver
[2022-06-22 21:55:16,606] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 22 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:16,607] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-22 21:55:16,607] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,030 s
[2022-06-22 21:55:16,607] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:16,607] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-22 21:55:16,608] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,033210 s
[2022-06-22 21:55:16,680] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-22 21:55:16,683] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
[2022-06-22 21:55:16,700] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Pushed Filters:
[2022-06-22 21:55:16,700] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-22 21:55:16,700] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-22 21:55:16,704] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 171.5 KiB, free 433.8 MiB)
[2022-06-22 21:55:16,710] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 433.8 MiB)
[2022-06-22 21:55:16,710] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,711] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,711] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197686 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:16,718] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:16,719] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:16,719] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:16,719] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:16,719] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:16,720] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:16,722] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.0 KiB, free 433.8 MiB)
[2022-06-22 21:55:16,724] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 433.8 MiB)
[2022-06-22 21:55:16,724] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:38779 (size: 6.3 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,724] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:16,725] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:16,725] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-22 21:55:16,726] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4936 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:16,726] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-22 21:55:16,731] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/api_football/ApiFootball_20220410.json, range: 0-3382, partition values: [empty row]
[2022-06-22 21:55:16,737] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4079 bytes result sent to driver
[2022-06-22 21:55:16,738] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 12 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:16,738] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-22 21:55:16,739] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,019 s
[2022-06-22 21:55:16,739] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:16,739] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-22 21:55:16,739] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,021158 s
[2022-06-22 21:55:16,958] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.13:38779 in memory (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,961] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:38779 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,963] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.13:38779 in memory (size: 6.3 KiB, free: 434.3 MiB)
[2022-06-22 21:55:16,965] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:38779 in memory (size: 6.3 KiB, free: 434.4 MiB)
[2022-06-22 21:55:16,966] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:38779 in memory (size: 24.0 KiB, free: 434.4 MiB)
[2022-06-22 21:55:16,968] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:38779 in memory (size: 24.0 KiB, free: 434.4 MiB)
[2022-06-22 21:55:17,109] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Pushed Filters:
[2022-06-22 21:55:17,109] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-22 21:55:17,109] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-22 21:55:17,145] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-22 21:55:17,146] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-22 21:55:17,176] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 13.705802 ms
[2022-06-22 21:55:17,179] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 171.4 KiB, free 434.2 MiB)
[2022-06-22 21:55:17,184] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.2 MiB)
[2022-06-22 21:55:17,185] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.4 MiB)
[2022-06-22 21:55:17,186] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:17,188] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194534 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:17,231] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:17,232] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:17,245] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 158.4 KiB, free 434.1 MiB)
[2022-06-22 21:55:17,247] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 57.2 KiB, free 434.0 MiB)
[2022-06-22 21:55:17,247] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:38779 (size: 57.2 KiB, free: 434.3 MiB)
[2022-06-22 21:55:17,247] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:17,248] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[17] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:17,248] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-22 21:55:17,251] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:17,251] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-22 21:55:17,285] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-22 21:55:17,286] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-22 21:55:17,316] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 8.779301 ms
[2022-06-22 21:55:17,609] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/odds_portal/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-22 21:55:17,624] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 12.314446 ms
[2022-06-22 21:55:17,626] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 14.177641 ms
[2022-06-22 21:55:17,649] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO PythonUDFRunner: Times: total = 322, boot = 278, init = 44, finish = 0
[2022-06-22 21:55:17,655] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileOutputCommitter: Saved output of task 'attempt_202206222155177761076671649883344_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-10/_temporary/0/task_202206222155177761076671649883344_0003_m_000000
[2022-06-22 21:55:17,656] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkHadoopMapRedUtil: attempt_202206222155177761076671649883344_0003_m_000000_3: Committed
[2022-06-22 21:55:17,658] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3149 bytes result sent to driver
[2022-06-22 21:55:17,660] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 411 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:17,660] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-22 21:55:17,660] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55475
[2022-06-22 21:55:17,661] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,428 s
[2022-06-22 21:55:17,662] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:17,662] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-22 21:55:17,662] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,431377 s
[2022-06-22 21:55:17,670] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileFormatWriter: Write Job 2e1f9e9a-dbf7-4ae9-967d-5dcb2ef9ea82 committed.
[2022-06-22 21:55:17,673] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileFormatWriter: Finished processing stats for write job 2e1f9e9a-dbf7-4ae9-967d-5dcb2ef9ea82.
[2022-06-22 21:55:17,776] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(matches)
[2022-06-22 21:55:17,776] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Post-Scan Filters: (size(matches#24, true) > 0),isnotnull(matches#24)
[2022-06-22 21:55:17,776] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Output Data Schema: struct<matches: array<struct<comments:array<struct<minute:string,represent_min:string,second:string,team:string,text:string>>,match_id:string>>>
[2022-06-22 21:55:17,779] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(teams)
[2022-06-22 21:55:17,779] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Post-Scan Filters: (size(teams#25, true) > 0),isnotnull(teams#25)
[2022-06-22 21:55:17,779] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceStrategy: Output Data Schema: struct<teams: array<struct<id:string,name:string>>>
[2022-06-22 21:55:17,800] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-22 21:55:17,800] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-22 21:55:17,815] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 9.658133 ms
[2022-06-22 21:55:17,829] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 9.891506 ms
[2022-06-22 21:55:17,832] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 171.4 KiB, free 433.8 MiB)
[2022-06-22 21:55:17,837] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 433.8 MiB)
[2022-06-22 21:55:17,838] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:17,838] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2022-06-22 21:55:17,839] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:17,866] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2022-06-22 21:55:17,867] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2022-06-22 21:55:17,867] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2022-06-22 21:55:17,867] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:17,867] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:17,868] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2022-06-22 21:55:17,870] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.8 KiB, free 433.8 MiB)
[2022-06-22 21:55:17,872] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 433.8 MiB)
[2022-06-22 21:55:17,873] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:38779 (size: 9.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:17,873] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:17,874] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:17,874] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-22 21:55:17,875] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 4932 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:17,875] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-22 21:55:17,896] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 6.937558 ms
[2022-06-22 21:55:17,898] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-22 21:55:17,907] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 8.023273 ms
[2022-06-22 21:55:17,915] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 2.946726 ms
[2022-06-22 21:55:17,923] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2089 bytes result sent to driver
[2022-06-22 21:55:17,924] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 49 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:17,924] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-22 21:55:17,925] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,055 s
[2022-06-22 21:55:17,925] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:17,925] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-22 21:55:17,925] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,059064 s
[2022-06-22 21:55:17,934] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 16.0 MiB, free 417.8 MiB)
[2022-06-22 21:55:17,937] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 308.0 B, free 417.8 MiB)
[2022-06-22 21:55:17,938] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.0.13:38779 (size: 308.0 B, free: 434.3 MiB)
[2022-06-22 21:55:17,938] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2022-06-22 21:55:17,954] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 11.011246 ms
[2022-06-22 21:55:17,969] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO CodeGenerator: Code generated in 11.53853 ms
[2022-06-22 21:55:17,972] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 171.4 KiB, free 417.6 MiB)
[2022-06-22 21:55:17,978] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 417.6 MiB)
[2022-06-22 21:55:17,978] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.0.13:38779 (size: 24.0 KiB, free: 434.3 MiB)
[2022-06-22 21:55:17,979] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Created broadcast 11 from json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:17,979] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4215435 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-22 21:55:17,993] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-22 21:55:17,994] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Got job 5 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-22 21:55:17,994] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Final stage: ResultStage 5 (json at NativeMethodAccessorImpl.java:0)
[2022-06-22 21:55:17,994] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Parents of final stage: List()
[2022-06-22 21:55:17,994] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Missing parents: List()
[2022-06-22 21:55:17,995] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:17 INFO DAGScheduler: Submitting ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-22 21:55:18,007] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 168.4 KiB, free 417.4 MiB)
[2022-06-22 21:55:18,009] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 60.1 KiB, free 417.4 MiB)
[2022-06-22 21:55:18,009] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.0.13:38779 (size: 60.1 KiB, free: 434.2 MiB)
[2022-06-22 21:55:18,010] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1433
[2022-06-22 21:55:18,010] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (CoalescedRDD[32] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-22 21:55:18,010] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2022-06-22 21:55:18,011] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5161 bytes) taskResourceAssignments Map()
[2022-06-22 21:55:18,011] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2022-06-22 21:55:18,021] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-22 21:55:18,021] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-22 21:55:18,038] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO CodeGenerator: Code generated in 9.831363 ms
[2022-06-22 21:55:18,052] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO CodeGenerator: Code generated in 11.216769 ms
[2022-06-22 21:55:18,054] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/extract_date=2022-04-10/who_scored/WhoScored_20220410.json, range: 0-21131, partition values: [empty row]
[2022-06-22 21:55:18,065] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO CodeGenerator: Code generated in 9.514871 ms
[2022-06-22 21:55:18,088] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_202206222155176511356843478564514_0005_m_000000_5' to file:/home/lucas/pipeline-data/datalake/silver/who_scored/process_date=2022-04-10/_temporary/0/task_202206222155176511356843478564514_0005_m_000000
[2022-06-22 21:55:18,088] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkHadoopMapRedUtil: attempt_202206222155176511356843478564514_0005_m_000000_5: Committed
[2022-06-22 21:55:18,089] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3025 bytes result sent to driver
[2022-06-22 21:55:18,089] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 78 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-22 21:55:18,089] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-06-22 21:55:18,090] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO DAGScheduler: ResultStage 5 (json at NativeMethodAccessorImpl.java:0) finished in 0,095 s
[2022-06-22 21:55:18,090] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-22 21:55:18,090] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2022-06-22 21:55:18,091] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO DAGScheduler: Job 5 finished: json at NativeMethodAccessorImpl.java:0, took 0,097296 s
[2022-06-22 21:55:18,096] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileFormatWriter: Write Job 55df455e-f401-409f-b740-f358def8659c committed.
[2022-06-22 21:55:18,097] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO FileFormatWriter: Finished processing stats for write job 55df455e-f401-409f-b740-f358def8659c.
[2022-06-22 21:55:18,131] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-22 21:55:18,132] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.0.13:38779 in memory (size: 9.0 KiB, free: 434.2 MiB)
[2022-06-22 21:55:18,134] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.0.13:38779 in memory (size: 24.0 KiB, free: 434.2 MiB)
[2022-06-22 21:55:18,136] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.0.13:38779 in memory (size: 60.1 KiB, free: 434.3 MiB)
[2022-06-22 21:55:18,137] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-22 21:55:18,138] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.0.13:38779 in memory (size: 57.2 KiB, free: 434.4 MiB)
[2022-06-22 21:55:18,145] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-22 21:55:18,151] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO MemoryStore: MemoryStore cleared
[2022-06-22 21:55:18,151] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManager: BlockManager stopped
[2022-06-22 21:55:18,153] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-22 21:55:18,155] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-22 21:55:18,157] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO SparkContext: Successfully stopped SparkContext
[2022-06-22 21:55:18,157] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO ShutdownHookManager: Shutdown hook called
[2022-06-22 21:55:18,158] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-82e2a93f-f0cd-48f9-8631-d82865ae4a00
[2022-06-22 21:55:18,159] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f01a28c-e8a4-449d-ac45-c6c747ae1117
[2022-06-22 21:55:18,160] {spark_submit_hook.py:479} INFO - 22/06/22 21:55:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f01a28c-e8a4-449d-ac45-c6c747ae1117/pyspark-ab07dc76-fe85-4151-838f-fddb74829766
[2022-06-22 21:55:18,228] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220410T000000, start_date=20220623T005511, end_date=20220623T005518
[2022-06-22 21:55:21,914] {local_task_job.py:102} INFO - Task exited with return code 0
