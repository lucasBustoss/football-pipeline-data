[2022-06-21 10:10:53,476] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-12T00:00:00+00:00 [queued]>
[2022-06-21 10:10:53,484] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-12T00:00:00+00:00 [queued]>
[2022-06-21 10:10:53,484] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:10:53,484] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 10:10:53,484] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:10:53,757] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-12T00:00:00+00:00
[2022-06-21 10:10:53,759] {standard_task_runner.py:54} INFO - Started process 16368 to run task
[2022-06-21 10:10:53,810] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-12T00:00:00+00:00', '--job_id', '129', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmp28whig6h']
[2022-06-21 10:10:53,810] {standard_task_runner.py:78} INFO - Job 129: Subtask transform_football
[2022-06-21 10:10:54,153] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-12T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 10:10:54,164] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 10:10:54,164] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py
[2022-06-21 10:10:54,898] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:54 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 10:10:54,898] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 10:10:55,679] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 10:10:55,684] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 10:10:55,744] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 10:10:55,795] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO ResourceUtils: ==============================================================
[2022-06-21 10:10:55,795] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 10:10:55,795] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO ResourceUtils: ==============================================================
[2022-06-21 10:10:55,796] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 10:10:55,809] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 10:10:55,819] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 10:10:55,819] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 10:10:55,853] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 10:10:55,853] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 10:10:55,853] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 10:10:55,854] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 10:10:55,854] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 10:10:56,012] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO Utils: Successfully started service 'sparkDriver' on port 40087.
[2022-06-21 10:10:56,028] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 10:10:56,047] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 10:10:56,059] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 10:10:56,059] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 10:10:56,061] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 10:10:56,074] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-58175d2d-0ef7-43fb-9b89-bc837b0a282e
[2022-06-21 10:10:56,089] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 10:10:56,099] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 10:10:56,236] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 10:10:56,274] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 10:10:56,386] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 10:10:56,401] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33525.
[2022-06-21 10:10:56,401] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO NettyBlockTransferService: Server created on 192.168.0.13:33525
[2022-06-21 10:10:56,402] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 10:10:56,406] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 33525, None)
[2022-06-21 10:10:56,408] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:33525 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 33525, None)
[2022-06-21 10:10:56,410] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 33525, None)
[2022-06-21 10:10:56,411] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 33525, None)
[2022-06-21 10:10:56,622] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655817056622
[2022-06-21 10:10:56,622] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-b4adf812-9ab3-4a1e-872e-305191626b8b/userFiles-7d42311c-94cd-4bb8-959d-317f3df45bfa/helpers.py
[2022-06-21 10:10:56,710] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 10:10:56,711] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:56 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 10:10:57,185] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:57 INFO InMemoryFileIndex: It took 21 ms to list leaf files for 1 paths.
[2022-06-21 10:10:57,316] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:57 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 3 paths.
[2022-06-21 10:10:58,577] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:10:58,578] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:10:58,580] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:10:58,758] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 10:10:58,792] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 10:10:58,793] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:33525 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 10:10:58,796] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:10:58,802] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12584052 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:10:58,914] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:10:58,925] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:10:58,925] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:10:58,925] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:10:58,926] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:10:58,929] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:10:58,984] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 10:10:58,987] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 10:10:58,988] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:33525 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 10:10:58,988] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:10:58,997] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:10:58,998] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 10:10:59,028] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5212 bytes) taskResourceAssignments Map()
[2022-06-21 10:10:59,036] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 10:10:59,038] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655817056622
[2022-06-21 10:10:59,060] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-b4adf812-9ab3-4a1e-872e-305191626b8b/userFiles-7d42311c-94cd-4bb8-959d-317f3df45bfa/helpers.py
[2022-06-21 10:10:59,273] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-11/OddsPortal_20220411.json, range: 0-795, partition values: [empty row]
[2022-06-21 10:10:59,404] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO CodeGenerator: Code generated in 109.48208 ms
[2022-06-21 10:10:59,428] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 10:10:59,430] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-12/OddsPortal_20220412.json, range: 0-115, partition values: [empty row]
[2022-06-21 10:10:59,440] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 10:10:59,445] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 423 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:10:59,446] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 10:10:59,449] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,511 s
[2022-06-21 10:10:59,451] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:10:59,451] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 10:10:59,453] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,538456 s
[2022-06-21 10:10:59,471] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:10:59,478] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 3 paths.
[2022-06-21 10:10:59,526] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:10:59,526] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:10:59,526] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:10:59,532] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 10:10:59,540] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 10:10:59,540] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:33525 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:10:59,541] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:10:59,542] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12594207 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:10:59,551] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:10:59,552] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:10:59,552] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:10:59,552] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:10:59,552] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:10:59,553] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:10:59,556] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 10:10:59,558] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 10:10:59,558] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:33525 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:10:59,559] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:10:59,559] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:10:59,559] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 10:10:59,560] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2022-06-21 10:10:59,561] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 10:10:59,565] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-11/ApiFootball_20220411.json, range: 0-6626, partition values: [empty row]
[2022-06-21 10:10:59,573] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-10/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 10:10:59,577] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-12/ApiFootball_20220412.json, range: 0-1297, partition values: [empty row]
[2022-06-21 10:10:59,582] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4079 bytes result sent to driver
[2022-06-21 10:10:59,583] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 23 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:10:59,583] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 10:10:59,584] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,030 s
[2022-06-21 10:10:59,584] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:10:59,584] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 10:10:59,584] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,033402 s
[2022-06-21 10:10:59,680] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:10:59,685] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 3 paths.
[2022-06-21 10:10:59,704] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:10:59,704] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:10:59,704] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:10:59,709] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 10:10:59,717] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 10:10:59,717] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:33525 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:10:59,718] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:10:59,719] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12680104 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:10:59,725] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:10:59,726] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:10:59,726] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:10:59,726] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:10:59,726] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:10:59,727] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:10:59,730] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 10:10:59,731] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 10:10:59,731] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:33525 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:10:59,732] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:10:59,732] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:10:59,732] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 10:10:59,733] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5206 bytes) taskResourceAssignments Map()
[2022-06-21 10:10:59,733] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 10:10:59,738] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-11/WhoScored_20220411.json, range: 0-76232, partition values: [empty row]
[2022-06-21 10:10:59,750] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-10/WhoScored_20220410.json, range: 0-20957, partition values: [empty row]
[2022-06-21 10:10:59,753] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-12/WhoScored_20220412.json, range: 0-3, partition values: [empty row]
[2022-06-21 10:10:59,755] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2172 bytes result sent to driver
[2022-06-21 10:10:59,756] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 23 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:10:59,756] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 10:10:59,756] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,028 s
[2022-06-21 10:10:59,756] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:10:59,756] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 10:10:59,757] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,031368 s
[2022-06-21 10:10:59,833] {spark_submit_hook.py:479} INFO - root
[2022-06-21 10:10:59,833] {spark_submit_hook.py:479} INFO - |-- home_team: string (nullable = true)
[2022-06-21 10:10:59,833] {spark_submit_hook.py:479} INFO - |-- away_team: string (nullable = true)
[2022-06-21 10:10:59,833] {spark_submit_hook.py:479} INFO - |-- odd_home: string (nullable = true)
[2022-06-21 10:10:59,833] {spark_submit_hook.py:479} INFO - |-- odd_draw: string (nullable = true)
[2022-06-21 10:10:59,833] {spark_submit_hook.py:479} INFO - |-- odd_away: string (nullable = true)
[2022-06-21 10:10:59,833] {spark_submit_hook.py:479} INFO - 
[2022-06-21 10:10:59,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO DataSourceStrategy: Pruning directories with:
[2022-06-21 10:10:59,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:10:59,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:10:59,886] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 10:10:59,946] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO CodeGenerator: Code generated in 13.657152 ms
[2022-06-21 10:10:59,969] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO CodeGenerator: Code generated in 12.020068 ms
[2022-06-21 10:10:59,969] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:33525 in memory (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:10:59,972] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:33525 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:10:59,973] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 337.9 KiB, free 365.2 MiB)
[2022-06-21 10:10:59,974] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:33525 in memory (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 10:10:59,975] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:33525 in memory (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 10:10:59,982] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 10:10:59,982] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:33525 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:10:59,983] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0
[2022-06-21 10:10:59,985] {spark_submit_hook.py:479} INFO - 22/06/21 10:10:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12584052 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:11:00,030] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2022-06-21 10:11:00,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:11:00,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:11:00,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:11:00,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:11:00,032] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:11:00,037] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 28.0 KiB, free 365.5 MiB)
[2022-06-21 10:11:00,038] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 365.5 MiB)
[2022-06-21 10:11:00,038] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:33525 (size: 12.3 KiB, free: 366.2 MiB)
[2022-06-21 10:11:00,039] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:11:00,039] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:11:00,039] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-21 10:11:00,040] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5264 bytes) taskResourceAssignments Map()
[2022-06-21 10:11:00,040] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-21 10:11:00,080] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO CodeGenerator: Code generated in 7.720417 ms
[2022-06-21 10:11:00,469] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-11/OddsPortal_20220411.json, range: 0-795, partition values: [19093]
[2022-06-21 10:11:00,484] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO CodeGenerator: Code generated in 9.916996 ms
[2022-06-21 10:11:00,488] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO CodeGenerator: Code generated in 13.68769 ms
[2022-06-21 10:11:00,501] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [19092]
[2022-06-21 10:11:00,503] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-12/OddsPortal_20220412.json, range: 0-115, partition values: [19094]
[2022-06-21 10:11:00,511] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO PythonUDFRunner: Times: total = 417, boot = 372, init = 45, finish = 0
[2022-06-21 10:11:00,512] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2926 bytes result sent to driver
[2022-06-21 10:11:00,514] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 473 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:11:00,514] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-21 10:11:00,514] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33275
[2022-06-21 10:11:00,515] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0,482 s
[2022-06-21 10:11:00,515] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:11:00,515] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-21 10:11:00,515] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0,485365 s
[2022-06-21 10:11:00,535] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO CodeGenerator: Code generated in 10.973766 ms
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - +-----------+-------------+--------+--------+--------+
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |  home_team|    away_team|odd_home|odd_draw|odd_away|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - +-----------+-------------+--------+--------+--------+
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |       Avai|   America-MG|    2.93|    2.91|    2.73|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |  São Paulo| Athletico-PR|    1.63|    3.65|    6.19|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |  Fortaleza|       Cuiaba|    1.89|    3.14|    4.94|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |Atletico-MG|Internacional|    1.47|    4.34|    7.43|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |   Botafogo|  Corinthians|    2.61|    3.04|    2.98|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |   Coritiba|        Goias|    1.87|    3.29|    4.70|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |  Palmeiras|        Ceara|    1.43|    4.39|    8.11|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |Atletico-GO|     Flamengo|    4.06|    3.40|    1.97|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - | Fluminense|       Santos|    1.77|    3.44|    5.29|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - |  Juventude|RB Bragantino|    2.39|    3.10|    3.22|
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - +-----------+-------------+--------+--------+--------+
[2022-06-21 10:11:00,541] {spark_submit_hook.py:479} INFO - 
[2022-06-21 10:11:00,566] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DataSourceStrategy: Pruning directories with:
[2022-06-21 10:11:00,566] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:11:00,566] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:11:00,567] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 10:11:00,591] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 10:11:00,591] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 10:11:00,592] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 10:11:00,605] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 337.9 KiB, free 365.2 MiB)
[2022-06-21 10:11:00,611] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 10:11:00,612] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.0.13:33525 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:11:00,612] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkContext: Created broadcast 8 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:11:00,613] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12584052 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:11:00,639] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:11:00,640] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Got job 4 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:11:00,640] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Final stage: ResultStage 4 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:11:00,640] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:11:00,641] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:11:00,641] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Submitting ResultStage 4 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:11:00,659] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 221.2 KiB, free 364.9 MiB)
[2022-06-21 10:11:00,661] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 79.0 KiB, free 364.9 MiB)
[2022-06-21 10:11:00,661] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.0.13:33525 (size: 79.0 KiB, free: 366.1 MiB)
[2022-06-21 10:11:00,662] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:11:00,662] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (CoalescedRDD[25] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:11:00,662] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2022-06-21 10:11:00,664] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5493 bytes) taskResourceAssignments Map()
[2022-06-21 10:11:00,665] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2022-06-21 10:11:00,695] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 10:11:00,695] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 10:11:00,695] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 10:11:00,744] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-11/OddsPortal_20220411.json, range: 0-795, partition values: [19093]
[2022-06-21 10:11:00,749] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [19092]
[2022-06-21 10:11:00,751] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-12/OddsPortal_20220412.json, range: 0-115, partition values: [19094]
[2022-06-21 10:11:00,755] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO PythonUDFRunner: Times: total = 10, boot = -229, init = 238, finish = 1
[2022-06-21 10:11:00,760] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileOutputCommitter: Saved output of task 'attempt_202206211011005396091453441187153_0004_m_000000_4' to file:/home/lucas/pipeline-data/datalake/silver/_temporary/0/task_202206211011005396091453441187153_0004_m_000000
[2022-06-21 10:11:00,760] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkHadoopMapRedUtil: attempt_202206211011005396091453441187153_0004_m_000000_4: Committed
[2022-06-21 10:11:00,764] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3456 bytes result sent to driver
[2022-06-21 10:11:00,765] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 102 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:11:00,765] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-06-21 10:11:00,766] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: ResultStage 4 (json at NativeMethodAccessorImpl.java:0) finished in 0,123 s
[2022-06-21 10:11:00,766] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:11:00,766] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-06-21 10:11:00,766] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO DAGScheduler: Job 4 finished: json at NativeMethodAccessorImpl.java:0, took 0,127302 s
[2022-06-21 10:11:00,767] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileFormatWriter: Start to commit write Job f1db9115-b5e9-46bf-ac1e-b7ca1a95b942.
[2022-06-21 10:11:00,774] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileFormatWriter: Write Job f1db9115-b5e9-46bf-ac1e-b7ca1a95b942 committed. Elapsed time: 5 ms.
[2022-06-21 10:11:00,776] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO FileFormatWriter: Finished processing stats for write job f1db9115-b5e9-46bf-ac1e-b7ca1a95b942.
[2022-06-21 10:11:00,798] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 10:11:00,803] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 10:11:00,808] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 10:11:00,814] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO MemoryStore: MemoryStore cleared
[2022-06-21 10:11:00,814] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO BlockManager: BlockManager stopped
[2022-06-21 10:11:00,816] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 10:11:00,817] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 10:11:00,819] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 10:11:00,819] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 10:11:00,820] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-b4adf812-9ab3-4a1e-872e-305191626b8b/pyspark-c607822c-02f1-4b4e-8125-09d47ab8c58b
[2022-06-21 10:11:00,821] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-c26f158f-cdb0-40fd-b11a-20c185de0294
[2022-06-21 10:11:00,821] {spark_submit_hook.py:479} INFO - 22/06/21 10:11:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-b4adf812-9ab3-4a1e-872e-305191626b8b
[2022-06-21 10:11:00,862] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220412T000000, start_date=20220621T131053, end_date=20220621T131100
[2022-06-21 10:11:03,083] {local_task_job.py:102} INFO - Task exited with return code 0
[2022-06-21 10:53:30,964] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-12T00:00:00+00:00 [queued]>
[2022-06-21 10:53:30,971] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: football_dag.transform_football 2022-04-12T00:00:00+00:00 [queued]>
[2022-06-21 10:53:30,971] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:53:30,971] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2022-06-21 10:53:30,971] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-06-21 10:53:31,163] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_football> on 2022-04-12T00:00:00+00:00
[2022-06-21 10:53:31,164] {standard_task_runner.py:54} INFO - Started process 23602 to run task
[2022-06-21 10:53:31,202] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'football_dag', 'transform_football', '2022-04-12T00:00:00+00:00', '--job_id', '130', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/football_dag.py', '--cfg_path', '/tmp/tmp31bza3h0']
[2022-06-21 10:53:31,202] {standard_task_runner.py:78} INFO - Job 130: Subtask transform_football
[2022-06-21 10:53:31,326] {logging_mixin.py:112} INFO - Running <TaskInstance: football_dag.transform_football 2022-04-12T00:00:00+00:00 [running]> on host lucas-AB350M-DS3H-V2
[2022-06-21 10:53:31,338] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-06-21 10:53:31,339] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/lucas/spark/bin/spark-submit --master local --name football_transformation /home/lucas/pipeline-data/spark/transformation.py --src /home/lucas/pipeline-data/datalake/bronze/ --dest /home/lucas/pipeline-data/datalake/silver// --process-date 2022-04-12
[2022-06-21 10:53:32,008] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 WARN Utils: Your hostname, lucas-AB350M-DS3H-V2 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface enp5s0)
[2022-06-21 10:53:32,008] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-06-21 10:53:32,797] {spark_submit_hook.py:479} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-06-21 10:53:32,802] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO SparkContext: Running Spark version 3.2.1
[2022-06-21 10:53:32,863] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-06-21 10:53:32,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO ResourceUtils: ==============================================================
[2022-06-21 10:53:32,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-06-21 10:53:32,914] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO ResourceUtils: ==============================================================
[2022-06-21 10:53:32,914] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO SparkContext: Submitted application: football_transformation
[2022-06-21 10:53:32,928] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-06-21 10:53:32,938] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO ResourceProfile: Limiting resource is cpu
[2022-06-21 10:53:32,938] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-06-21 10:53:32,971] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO SecurityManager: Changing view acls to: lucas
[2022-06-21 10:53:32,971] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO SecurityManager: Changing modify acls to: lucas
[2022-06-21 10:53:32,972] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO SecurityManager: Changing view acls groups to:
[2022-06-21 10:53:32,972] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO SecurityManager: Changing modify acls groups to:
[2022-06-21 10:53:32,972] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(lucas); groups with view permissions: Set(); users  with modify permissions: Set(lucas); groups with modify permissions: Set()
[2022-06-21 10:53:33,124] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO Utils: Successfully started service 'sparkDriver' on port 33191.
[2022-06-21 10:53:33,140] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO SparkEnv: Registering MapOutputTracker
[2022-06-21 10:53:33,160] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO SparkEnv: Registering BlockManagerMaster
[2022-06-21 10:53:33,172] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-06-21 10:53:33,172] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-06-21 10:53:33,174] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-06-21 10:53:33,187] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-448e70fe-1837-485b-91f6-15b72f114a62
[2022-06-21 10:53:33,202] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2022-06-21 10:53:33,212] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-06-21 10:53:33,348] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-06-21 10:53:33,382] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.13:4040
[2022-06-21 10:53:33,500] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO Executor: Starting executor ID driver on host 192.168.0.13
[2022-06-21 10:53:33,517] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46603.
[2022-06-21 10:53:33,517] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO NettyBlockTransferService: Server created on 192.168.0.13:46603
[2022-06-21 10:53:33,518] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-06-21 10:53:33,522] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.13, 46603, None)
[2022-06-21 10:53:33,524] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.13:46603 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.0.13, 46603, None)
[2022-06-21 10:53:33,526] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.13, 46603, None)
[2022-06-21 10:53:33,526] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.13, 46603, None)
[2022-06-21 10:53:33,735] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO SparkContext: Added file /home/lucas/pipeline-data/helpers/helpers.py at file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655819613735
[2022-06-21 10:53:33,736] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO Utils: Copying /home/lucas/pipeline-data/helpers/helpers.py to /tmp/spark-6dc6a4d0-cff5-43cb-ba15-521edb647f1f/userFiles-37752982-d5c6-4918-aabe-21c01b16e9d0/helpers.py
[2022-06-21 10:53:33,829] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-06-21 10:53:33,830] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:33 INFO SharedState: Warehouse path is 'file:/home/lucas/pipeline-data/spark-warehouse'.
[2022-06-21 10:53:34,315] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:34 INFO InMemoryFileIndex: It took 22 ms to list leaf files for 1 paths.
[2022-06-21 10:53:34,447] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:34 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 3 paths.
[2022-06-21 10:53:35,662] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:53:35,663] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:53:35,665] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:53:35,836] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.1 KiB, free 366.0 MiB)
[2022-06-21 10:53:35,867] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)
[2022-06-21 10:53:35,869] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.13:46603 (size: 32.5 KiB, free: 366.3 MiB)
[2022-06-21 10:53:35,872] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:53:35,877] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12584052 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:53:35,980] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:53:35,991] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:53:35,991] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:53:35,991] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:53:35,992] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:53:35,994] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:53:36,051] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.4 KiB, free 365.9 MiB)
[2022-06-21 10:53:36,053] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.9 MiB)
[2022-06-21 10:53:36,053] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.13:46603 (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 10:53:36,054] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:53:36,061] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:53:36,062] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-06-21 10:53:36,091] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5212 bytes) taskResourceAssignments Map()
[2022-06-21 10:53:36,100] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-06-21 10:53:36,101] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO Executor: Fetching file:/home/lucas/pipeline-data/helpers/helpers.py with timestamp 1655819613735
[2022-06-21 10:53:36,112] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO Utils: /home/lucas/pipeline-data/helpers/helpers.py has been previously copied to /tmp/spark-6dc6a4d0-cff5-43cb-ba15-521edb647f1f/userFiles-37752982-d5c6-4918-aabe-21c01b16e9d0/helpers.py
[2022-06-21 10:53:36,309] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-11/OddsPortal_20220411.json, range: 0-795, partition values: [empty row]
[2022-06-21 10:53:36,443] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO CodeGenerator: Code generated in 112.373494 ms
[2022-06-21 10:53:36,467] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [empty row]
[2022-06-21 10:53:36,470] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-12/OddsPortal_20220412.json, range: 0-115, partition values: [empty row]
[2022-06-21 10:53:36,480] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2059 bytes result sent to driver
[2022-06-21 10:53:36,484] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 399 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:53:36,486] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-06-21 10:53:36,489] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0,484 s
[2022-06-21 10:53:36,491] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:53:36,491] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-06-21 10:53:36,493] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0,511957 s
[2022-06-21 10:53:36,510] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:53:36,516] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 3 paths.
[2022-06-21 10:53:36,554] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:53:36,554] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:53:36,554] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:53:36,559] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.1 KiB, free 365.6 MiB)
[2022-06-21 10:53:36,566] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 10:53:36,566] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.13:46603 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:53:36,567] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:53:36,567] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12594207 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:53:36,575] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:53:36,576] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:53:36,576] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:53:36,576] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:53:36,576] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:53:36,577] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:53:36,580] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.4 KiB, free 365.5 MiB)
[2022-06-21 10:53:36,582] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.5 MiB)
[2022-06-21 10:53:36,582] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.13:46603 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:53:36,583] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:53:36,583] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:53:36,583] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2022-06-21 10:53:36,584] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5218 bytes) taskResourceAssignments Map()
[2022-06-21 10:53:36,585] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-06-21 10:53:36,589] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-11/ApiFootball_20220411.json, range: 0-6626, partition values: [empty row]
[2022-06-21 10:53:36,597] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-10/ApiFootball_20220410.json, range: 0-3372, partition values: [empty row]
[2022-06-21 10:53:36,600] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/api_football/extract_date=2022-04-12/ApiFootball_20220412.json, range: 0-1297, partition values: [empty row]
[2022-06-21 10:53:36,605] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4079 bytes result sent to driver
[2022-06-21 10:53:36,607] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 23 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:53:36,607] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-06-21 10:53:36,607] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0,029 s
[2022-06-21 10:53:36,608] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:53:36,608] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-06-21 10:53:36,608] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0,033139 s
[2022-06-21 10:53:36,706] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
[2022-06-21 10:53:36,711] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 3 paths.
[2022-06-21 10:53:36,727] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:53:36,728] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:53:36,728] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-06-21 10:53:36,731] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 338.1 KiB, free 365.2 MiB)
[2022-06-21 10:53:36,738] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.2 MiB)
[2022-06-21 10:53:36,738] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.13:46603 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:53:36,739] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:53:36,739] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12680104 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:53:36,746] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:53:36,746] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:53:36,747] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:53:36,747] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:53:36,747] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:53:36,747] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:53:36,750] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.4 KiB, free 365.2 MiB)
[2022-06-21 10:53:36,751] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 365.2 MiB)
[2022-06-21 10:53:36,752] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.13:46603 (size: 6.5 KiB, free: 366.2 MiB)
[2022-06-21 10:53:36,752] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:53:36,753] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:53:36,753] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2022-06-21 10:53:36,754] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5206 bytes) taskResourceAssignments Map()
[2022-06-21 10:53:36,754] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2022-06-21 10:53:36,758] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-11/WhoScored_20220411.json, range: 0-76232, partition values: [empty row]
[2022-06-21 10:53:36,769] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-10/WhoScored_20220410.json, range: 0-20957, partition values: [empty row]
[2022-06-21 10:53:36,772] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/who_scored/extract_date=2022-04-12/WhoScored_20220412.json, range: 0-3, partition values: [empty row]
[2022-06-21 10:53:36,774] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2172 bytes result sent to driver
[2022-06-21 10:53:36,775] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 22 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:53:36,775] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-06-21 10:53:36,776] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0,027 s
[2022-06-21 10:53:36,776] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:53:36,776] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-06-21 10:53:36,776] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0,030412 s
[2022-06-21 10:53:36,845] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.0.13:46603 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:53:36,848] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.0.13:46603 in memory (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:53:36,850] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.0.13:46603 in memory (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 10:53:36,852] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.0.13:46603 in memory (size: 6.5 KiB, free: 366.3 MiB)
[2022-06-21 10:53:36,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO DataSourceStrategy: Pruning directories with:
[2022-06-21 10:53:36,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Pushed Filters:
[2022-06-21 10:53:36,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Post-Scan Filters:
[2022-06-21 10:53:36,913] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileSourceStrategy: Output Data Schema: struct<away_team: string, home_team: string, odd_away: string, odd_draw: string, odd_home: string ... 3 more fields>
[2022-06-21 10:53:36,957] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 10:53:36,957] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 10:53:36,958] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 10:53:36,997] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:36 INFO CodeGenerator: Code generated in 14.848756 ms
[2022-06-21 10:53:37,014] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO CodeGenerator: Code generated in 12.256598 ms
[2022-06-21 10:53:37,019] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 337.9 KiB, free 365.6 MiB)
[2022-06-21 10:53:37,027] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.6 MiB)
[2022-06-21 10:53:37,028] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.0.13:46603 (size: 32.5 KiB, free: 366.2 MiB)
[2022-06-21 10:53:37,029] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO SparkContext: Created broadcast 6 from json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:53:37,031] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12584052 bytes, open cost is considered as scanning 4194304 bytes.
[2022-06-21 10:53:37,093] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2022-06-21 10:53:37,094] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: Got job 3 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-06-21 10:53:37,094] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: Final stage: ResultStage 3 (json at NativeMethodAccessorImpl.java:0)
[2022-06-21 10:53:37,094] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: Parents of final stage: List()
[2022-06-21 10:53:37,094] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: Missing parents: List()
[2022-06-21 10:53:37,095] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[18] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-06-21 10:53:37,110] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 221.3 KiB, free 365.3 MiB)
[2022-06-21 10:53:37,111] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 79.0 KiB, free 365.3 MiB)
[2022-06-21 10:53:37,111] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.0.13:46603 (size: 79.0 KiB, free: 366.2 MiB)
[2022-06-21 10:53:37,112] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
[2022-06-21 10:53:37,112] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[18] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-06-21 10:53:37,112] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-06-21 10:53:37,114] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.0.13, executor driver, partition 0, PROCESS_LOCAL, 5493 bytes) taskResourceAssignments Map()
[2022-06-21 10:53:37,115] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2022-06-21 10:53:37,157] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2022-06-21 10:53:37,157] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2022-06-21 10:53:37,157] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2022-06-21 10:53:37,190] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO CodeGenerator: Code generated in 8.282353 ms
[2022-06-21 10:53:37,575] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-11/OddsPortal_20220411.json, range: 0-795, partition values: [19093]
[2022-06-21 10:53:37,592] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO CodeGenerator: Code generated in 11.424873 ms
[2022-06-21 10:53:37,592] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO CodeGenerator: Code generated in 12.131601 ms
[2022-06-21 10:53:37,608] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-10/OddsPortal_20220410.json, range: 0-230, partition values: [19092]
[2022-06-21 10:53:37,610] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileScanRDD: Reading File path: file:///home/lucas/pipeline-data/datalake/bronze/odds_portal/extract_date=2022-04-12/OddsPortal_20220412.json, range: 0-115, partition values: [19094]
[2022-06-21 10:53:37,620] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO PythonUDFRunner: Times: total = 414, boot = 368, init = 46, finish = 0
[2022-06-21 10:53:37,625] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileOutputCommitter: Saved output of task 'attempt_202206211053377508123991287686719_0003_m_000000_3' to file:/home/lucas/pipeline-data/datalake/silver/odds_portal/process_date=2022-04-12/_temporary/0/task_202206211053377508123991287686719_0003_m_000000
[2022-06-21 10:53:37,626] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO SparkHadoopMapRedUtil: attempt_202206211053377508123991287686719_0003_m_000000_3: Committed
[2022-06-21 10:53:37,630] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3456 bytes result sent to driver
[2022-06-21 10:53:37,631] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 518 ms on 192.168.0.13 (executor driver) (1/1)
[2022-06-21 10:53:37,631] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-06-21 10:53:37,632] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 57135
[2022-06-21 10:53:37,632] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: ResultStage 3 (json at NativeMethodAccessorImpl.java:0) finished in 0,537 s
[2022-06-21 10:53:37,633] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-06-21 10:53:37,633] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-06-21 10:53:37,633] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO DAGScheduler: Job 3 finished: json at NativeMethodAccessorImpl.java:0, took 0,540024 s
[2022-06-21 10:53:37,634] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileFormatWriter: Start to commit write Job aba9c716-7288-434b-8137-7c4f47ac6ebf.
[2022-06-21 10:53:37,641] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileFormatWriter: Write Job aba9c716-7288-434b-8137-7c4f47ac6ebf committed. Elapsed time: 5 ms.
[2022-06-21 10:53:37,642] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO FileFormatWriter: Finished processing stats for write job aba9c716-7288-434b-8137-7c4f47ac6ebf.
[2022-06-21 10:53:37,665] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO SparkContext: Invoking stop() from shutdown hook
[2022-06-21 10:53:37,671] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO SparkUI: Stopped Spark web UI at http://192.168.0.13:4040
[2022-06-21 10:53:37,677] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-06-21 10:53:37,682] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO MemoryStore: MemoryStore cleared
[2022-06-21 10:53:37,682] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO BlockManager: BlockManager stopped
[2022-06-21 10:53:37,685] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-06-21 10:53:37,686] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-06-21 10:53:37,688] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO SparkContext: Successfully stopped SparkContext
[2022-06-21 10:53:37,688] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO ShutdownHookManager: Shutdown hook called
[2022-06-21 10:53:37,689] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-4fabe06a-eff4-4eed-8feb-abfe49dab3f5
[2022-06-21 10:53:37,690] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-6dc6a4d0-cff5-43cb-ba15-521edb647f1f
[2022-06-21 10:53:37,691] {spark_submit_hook.py:479} INFO - 22/06/21 10:53:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-6dc6a4d0-cff5-43cb-ba15-521edb647f1f/pyspark-b38797a9-b877-49c3-b206-c81ad9d3be18
[2022-06-21 10:53:37,733] {taskinstance.py:1057} INFO - Marking task as SUCCESS.dag_id=football_dag, task_id=transform_football, execution_date=20220412T000000, start_date=20220621T135330, end_date=20220621T135337
[2022-06-21 10:53:40,745] {local_task_job.py:102} INFO - Task exited with return code 0
